{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9de0d6eb",
   "metadata": {},
   "source": [
    "# Dual-Network Hydrogel Mastercurve\n",
    "\n",
    "Auto TTS on dual-network hydrogels across temperature.\n",
    "\n",
    "**Data:** examples/data/temperature_sweep/hydrogels/*.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adc3f61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T06:03:52.343630Z",
     "iopub.status.busy": "2026-01-30T06:03:52.343475Z",
     "iopub.status.idle": "2026-01-30T06:03:52.349496Z",
     "shell.execute_reply": "2026-01-30T06:03:52.348172Z"
    }
   },
   "outputs": [],
   "source": [
    "# Google Colab compatibility - uncomment if running in Colab\n",
    "# !pip install -q rheojax\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d434e1",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2fee32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T06:03:52.364574Z",
     "iopub.status.busy": "2026-01-30T06:03:52.364373Z",
     "iopub.status.idle": "2026-01-30T06:03:54.793875Z",
     "shell.execute_reply": "2026-01-30T06:03:54.733281Z"
    }
   },
   "outputs": [],
   "source": [
    "# Configure matplotlib for inline plotting in VS Code/Jupyter\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rheojax.models import FractionalMaxwellModel\n",
    "from rheojax.models import GeneralizedMaxwell\n",
    "from rheojax.models import HerschelBulkley\n",
    "\n",
    "from rheojax.core.data import RheoData\n",
    "from rheojax.core.jax_config import safe_import_jax, verify_float64\n",
    "from rheojax.pipeline.base import Pipeline\n",
    "from rheojax.transforms.mastercurve import Mastercurve\n",
    "\n",
    "jax, jnp = safe_import_jax()\n",
    "verify_float64()\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "\n",
    "def r2_complex(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    ss_res = np.sum(np.abs(y_true - y_pred) ** 2)\n",
    "    ss_tot = np.sum(np.abs(y_true - np.mean(y_true)) ** 2)\n",
    "    return float(1 - ss_res / ss_tot)\n",
    "\n",
    "def mpe(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    return float(np.mean(np.abs(y_true - y_pred) / np.maximum(np.abs(y_true), 1e-12)) * 100)\n",
    "import arviz as az\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9860d8e1",
   "metadata": {},
   "source": [
    "## Load multi-temperature sweeps (UTF-16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c8154c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T06:03:54.799953Z",
     "iopub.status.busy": "2026-01-30T06:03:54.799451Z",
     "iopub.status.idle": "2026-01-30T06:04:05.532780Z",
     "shell.execute_reply": "2026-01-30T06:04:05.516441Z"
    }
   },
   "outputs": [],
   "source": [
    "# Find data directory - handle different CWD scenarios (IDE vs nbconvert)\n",
    "import codecs\n",
    "from io import StringIO\n",
    "\n",
    "# Try multiple candidate paths for robustness\n",
    "_data_path_candidates = [\n",
    "    Path.cwd().parent / 'data' / 'temperature_sweep' / 'hydrogels',  # CWD = notebook dir\n",
    "    Path.cwd() / 'examples' / 'verification' / 'data' / 'temperature_sweep' / 'hydrogels',  # CWD = project root\n",
    "    Path.cwd() / 'examples' / 'data' / 'temperature_sweep' / 'hydrogels',  # CWD = project root (alt)\n",
    "]\n",
    "\n",
    "DATA_DIR = None\n",
    "for _candidate in _data_path_candidates:\n",
    "    if _candidate.exists() and len(list(_candidate.glob('*.csv'))) > 0:\n",
    "        DATA_DIR = _candidate\n",
    "        break\n",
    "\n",
    "if DATA_DIR is None:\n",
    "    raise FileNotFoundError(\n",
    "        f\"No CSV files found in any candidate path. CWD={Path.cwd()}\\n\"\n",
    "        f\"Tried: {[str(c) for c in _data_path_candidates]}\"\n",
    "    )\n",
    "\n",
    "files = sorted(DATA_DIR.glob('*.csv'))\n",
    "print(f\"Found {len(files)} CSV files in {DATA_DIR}\")\n",
    "\n",
    "datasets = []\n",
    "for fpath in files:\n",
    "    stem = fpath.stem\n",
    "    temp_token = stem.split('_')[-1]\n",
    "    temp_c = float(temp_token.replace('c','').replace('h',''))\n",
    "    \n",
    "    # Read UTF-16 files using codecs (pandas has issues with UTF-16-LE)\n",
    "    try:\n",
    "        with codecs.open(fpath, 'r', encoding='utf-16') as f:\n",
    "            content = f.read()\n",
    "        # Skip header rows (0-10) + units row, data starts at row 12\n",
    "        # Files have alternating blank lines so we filter them out\n",
    "        df = pd.read_csv(StringIO(content), sep='\\t', skiprows=11, \n",
    "                        names=['omega', 'Gp', 'Gpp'], decimal=',', skip_blank_lines=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not read {fpath.name}: {e}\")\n",
    "        continue\n",
    "        \n",
    "    df = df.dropna()\n",
    "    df['omega'] = pd.to_numeric(df['omega'], errors='coerce')\n",
    "    df['Gp'] = pd.to_numeric(df['Gp'], errors='coerce')\n",
    "    df['Gpp'] = pd.to_numeric(df['Gpp'], errors='coerce')\n",
    "    df = df.dropna()\n",
    "    if df.empty:\n",
    "        continue\n",
    "    omega = df['omega'].to_numpy()\n",
    "    Gp = df['Gp'].to_numpy()\n",
    "    Gpp = df['Gpp'].to_numpy()\n",
    "    datasets.append(RheoData(x=omega, y=Gp + 1j*Gpp, x_units='rad/s', y_units='Pa', domain='oscillation', metadata={'temperature': temp_c + 273.15, 'test_mode': 'oscillation'}))\n",
    "\n",
    "if not datasets:\n",
    "    raise ValueError(\"No valid datasets loaded from CSV files\")\n",
    "    \n",
    "print(f\"Loaded {len(datasets)} datasets\")\n",
    "\n",
    "mc = Mastercurve(reference_temp=298.15, method='wlf', auto_shift=True)\n",
    "try:\n",
    "    master, shifts = mc.create_mastercurve(datasets, return_shifts=True)\n",
    "except Exception as exc:\n",
    "    print(f\"Auto-shift failed: {exc}; falling back to manual identity shifts\")\n",
    "    manual_shifts = {d.metadata['temperature']: 1.0 for d in datasets}\n",
    "    mc = Mastercurve(reference_temp=298.15, method='manual', auto_shift=False)\n",
    "    mc.set_manual_shifts(manual_shifts)\n",
    "    master, shifts = mc.create_mastercurve(datasets, return_shifts=True)\n",
    "\n",
    "print(f\"Temps (C): {[round(d.metadata['temperature']-273.15,1) for d in datasets]}\")\n",
    "print(shifts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a63549f",
   "metadata": {},
   "source": [
    "## Plot raw vs shifted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c13f11b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T06:04:05.562568Z",
     "iopub.status.busy": "2026-01-30T06:04:05.558727Z",
     "iopub.status.idle": "2026-01-30T06:04:06.071430Z",
     "shell.execute_reply": "2026-01-30T06:04:06.062319Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6), sharey=True)\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(datasets)))\n",
    "for c, data in zip(colors, datasets):\n",
    "    temp_c = data.metadata['temperature'] - 273.15\n",
    "    axes[0].loglog(data.x/(2*np.pi), np.real(data.y), 'o', color=c, label=f\"{temp_c:.0f}°C G'\")\n",
    "axes[0].set_title(\"Unshifted G'\")\n",
    "axes[0].set_xlabel('Frequency (Hz)')\n",
    "axes[0].set_ylabel('Modulus (Pa)')\n",
    "axes[0].grid(True, which='both', ls='--', alpha=0.4)\n",
    "axes[0].legend(ncol=2, fontsize=8)\n",
    "\n",
    "axes[1].loglog(master.x/(2*np.pi), np.real(master.y), 'o', label=\"G' master\", alpha=0.7)\n",
    "axes[1].loglog(master.x/(2*np.pi), np.imag(master.y), 's', label='G\" master', alpha=0.7)\n",
    "axes[1].set_title('Mastercurve (auto-shift)')\n",
    "axes[1].grid(True, which='both', ls='--', alpha=0.4)\n",
    "axes[1].legend()\n",
    "plt.tight_layout()\n",
    "display(fig)\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3469dba1",
   "metadata": {},
   "source": [
    "## Fit models on mastercurve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66b2563",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T06:04:06.091358Z",
     "iopub.status.busy": "2026-01-30T06:04:06.085818Z",
     "iopub.status.idle": "2026-01-30T06:04:17.881648Z",
     "shell.execute_reply": "2026-01-30T06:04:17.872498Z"
    }
   },
   "outputs": [],
   "source": [
    "omega_master = master.x\n",
    "G_master = master.y\n",
    "\n",
    "gm = GeneralizedMaxwell(n_modes=5, modulus_type='tensile')\n",
    "gm.fit(omega_master, G_master, test_mode='oscillation', use_log_residuals=True)\n",
    "gm_pred_components = gm.predict(omega_master)\n",
    "gm_pred = gm_pred_components[:,0] + 1j*gm_pred_components[:,1]\n",
    "gm_r2 = r2_complex(G_master, gm_pred)\n",
    "\n",
    "fm_pred = np.full_like(G_master, np.nan)\n",
    "fm_r2 = np.nan\n",
    "try:\n",
    "    fm = FractionalMaxwellModel()\n",
    "    fm.fit(omega_master, G_master, test_mode='oscillation', use_log_residuals=True)\n",
    "    fm_pred = fm.predict(omega_master, test_mode='oscillation')\n",
    "    fm_r2 = r2_complex(G_master, fm_pred)\n",
    "except Exception as exc:\n",
    "    print(f\"Fractional Maxwell fit failed: {exc}\")\n",
    "\n",
    "print({'gm_r2': gm_r2, 'fm_r2': fm_r2})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6c41d4",
   "metadata": {},
   "source": [
    "## Bayesian workflow (NLSQ → best model → NUTS diagnostics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d9e2b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T06:04:17.889539Z",
     "iopub.status.busy": "2026-01-30T06:04:17.887783Z",
     "iopub.status.idle": "2026-01-30T06:04:17.935556Z",
     "shell.execute_reply": "2026-01-30T06:04:17.916313Z"
    }
   },
   "outputs": [],
   "source": [
    "# Helper functions for workflow\n",
    "if 'r2_complex' not in globals():\n",
    "    def r2_complex(y_true, y_pred):\n",
    "        \"\"\"Compute R² for complex-valued data.\"\"\"\n",
    "        y_true = np.asarray(y_true)\n",
    "        y_pred = np.asarray(y_pred)\n",
    "        ss_res = np.sum(np.abs(y_true - y_pred) ** 2)\n",
    "        ss_tot = np.sum(np.abs(y_true - np.mean(y_true)) ** 2)\n",
    "        return 1 - ss_res / ss_tot\n",
    "\n",
    "if '_coerce_pred' not in globals():\n",
    "    def _coerce_pred(pred):\n",
    "        \"\"\"Convert 2-column real array to complex if needed.\"\"\"\n",
    "        arr = np.asarray(pred)\n",
    "        if arr.ndim == 2 and arr.shape[1] == 2 and not np.iscomplexobj(arr):\n",
    "            arr = arr[:, 0] + 1j * arr[:, 1]\n",
    "        return arr\n",
    "\n",
    "NUTS_CONFIG = dict(num_chains=1, num_warmup=200, num_samples=500)  # Fast demo mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38b4981",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T06:04:17.965098Z",
     "iopub.status.busy": "2026-01-30T06:04:17.961715Z",
     "iopub.status.idle": "2026-01-30T06:04:18.009650Z",
     "shell.execute_reply": "2026-01-30T06:04:18.003360Z"
    }
   },
   "outputs": [],
   "source": [
    "def _detect_datasets():\n",
    "    \"\"\"Auto-detect datasets from global variables.\"\"\"\n",
    "    datasets = []\n",
    "    if 'G_star' in globals() and 'omega' in globals():\n",
    "        datasets.append({\n",
    "            'X': omega, 'y': G_star, 'test_mode': 'oscillation',\n",
    "            'x_label': 'Angular frequency (rad/s)', 'y_label': 'Complex modulus',\n",
    "        })\n",
    "    elif 'Gp' in globals() and 'Gpp' in globals() and 'omega' in globals():\n",
    "        datasets.append({\n",
    "            'X': omega, 'y': Gp + 1j * Gpp, 'test_mode': 'oscillation',\n",
    "            'x_label': 'Angular frequency (rad/s)', 'y_label': 'Complex modulus',\n",
    "        })\n",
    "    if 'master' in globals() and hasattr(master, 'x') and hasattr(master, 'y'):\n",
    "        test_mode = master.metadata.get('test_mode', 'oscillation') if hasattr(master, 'metadata') else 'oscillation'\n",
    "        datasets.append({\n",
    "            'X': master.x, 'y': master.y, 'test_mode': test_mode,\n",
    "            'x_label': 'Shifted frequency', 'y_label': 'Mastercurve modulus',\n",
    "        })\n",
    "    if 't' in globals() and 'G' in globals():\n",
    "        datasets.append({'X': t, 'y': G, 'test_mode': 'relaxation', 'x_label': 'Time (s)', 'y_label': 'Relaxation modulus'})\n",
    "    if 't' in globals() and 'E_t' in globals():\n",
    "        datasets.append({'X': t, 'y': E_t, 'test_mode': 'relaxation', 'x_label': 'Time (s)', 'y_label': 'Relaxation modulus'})\n",
    "    if 't' in globals() and 'sigma' in globals():\n",
    "        datasets.append({'X': t, 'y': sigma, 'test_mode': 'relaxation', 'x_label': 'Time (s)', 'y_label': 'Stress (Pa)'})\n",
    "    if 't' in globals() and 'J' in globals():\n",
    "        datasets.append({'X': t, 'y': J, 'test_mode': 'creep', 'x_label': 'Time (s)', 'y_label': 'Creep compliance'})\n",
    "    if 'all_df' in globals() and hasattr(all_df, 'columns') and 'phi' in all_df.columns:\n",
    "        for phi, frame in all_df.groupby('phi'):\n",
    "            x_vals = frame.iloc[:, 0].to_numpy()\n",
    "            y_vals = frame.iloc[:, 1].to_numpy()\n",
    "            datasets.append({\n",
    "                'X': x_vals, 'y': y_vals, 'test_mode': 'rotation',\n",
    "                'x_label': f'Shear rate 1/s (phi={phi})', 'y_label': 'Stress (Pa)', 'label': f'phi={phi}',\n",
    "            })\n",
    "    if not datasets:\n",
    "        raise ValueError('No datasets detected; ensure data variables are defined.')\n",
    "    return datasets\n",
    "\n",
    "def _collect_candidate_models():\n",
    "    \"\"\"Collect candidate models from global variables.\"\"\"\n",
    "    if 'candidates' in globals() and isinstance(candidates, (list, tuple)) and candidates:\n",
    "        return candidates\n",
    "    models = []\n",
    "    for name, obj in globals().items():\n",
    "        if isinstance(obj, type):\n",
    "            continue\n",
    "        if hasattr(obj, 'fit_bayesian') and hasattr(obj, 'predict'):\n",
    "            models.append((name, obj))\n",
    "    if not models:\n",
    "        raise ValueError('No candidate models found; define models before running.')\n",
    "    return models\n",
    "\n",
    "datasets = _detect_datasets()\n",
    "model_entries = _collect_candidate_models()\n",
    "print(f\"Found {len(datasets)} dataset(s) and {len(model_entries)} model(s)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281067a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T06:04:18.017319Z",
     "iopub.status.busy": "2026-01-30T06:04:18.017080Z",
     "iopub.status.idle": "2026-01-30T06:04:22.146840Z",
     "shell.execute_reply": "2026-01-30T06:04:22.139750Z"
    }
   },
   "outputs": [],
   "source": [
    "# NLSQ fitting loop - fit all models to all datasets\n",
    "all_fits = {}  # {dataset_idx: [fit_records]}\n",
    "\n",
    "for ds_idx, ds in enumerate(datasets):\n",
    "    fits = []\n",
    "    for name, model in model_entries:\n",
    "        fitted_model = model\n",
    "        fit_kwargs = {'test_mode': ds['test_mode'], 'use_log_residuals': True}\n",
    "        try:\n",
    "            fitted_model.fit(ds['X'], ds['y'], **fit_kwargs)\n",
    "        except TypeError:\n",
    "            fit_kwargs.pop('use_log_residuals', None)\n",
    "            try:\n",
    "                fitted_model.fit(ds['X'], ds['y'], **fit_kwargs)\n",
    "            except Exception as exc:\n",
    "                print(f\"Skipping {name} due to fit error: {exc}\")\n",
    "                continue\n",
    "        except Exception as exc:\n",
    "            print(f\"Skipping {name} due to fit error: {exc}\")\n",
    "            continue\n",
    "\n",
    "        predict_kwargs = {'test_mode': ds['test_mode']} if 'test_mode' in fit_kwargs else {}\n",
    "        try:\n",
    "            pred = _coerce_pred(fitted_model.predict(ds['X'], **predict_kwargs))\n",
    "        except Exception as exc:\n",
    "            print(f\"Skipping {name} due to predict error: {exc}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            r2_val = r2_complex(ds['y'], pred) if np.iscomplexobj(ds['y']) else fitted_model.score(ds['X'], ds['y'])\n",
    "        except Exception as exc:\n",
    "            print(f\"Skipping {name} due to scoring error: {exc}\")\n",
    "            continue\n",
    "\n",
    "        fits.append({'name': name, 'model': fitted_model, 'pred': pred, 'r2': float(r2_val)})\n",
    "\n",
    "    fits.sort(key=lambda rec: rec['r2'], reverse=True)\n",
    "    all_fits[ds_idx] = fits\n",
    "    if fits:\n",
    "        print(f\"Dataset {ds_idx} ranking: {[(r['name'], round(r['r2'], 3)) for r in fits]}\")\n",
    "    else:\n",
    "        print(f\"Dataset {ds_idx}: No successful fits\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae64ac6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T06:04:22.152039Z",
     "iopub.status.busy": "2026-01-30T06:04:22.151507Z",
     "iopub.status.idle": "2026-01-30T06:04:34.307168Z",
     "shell.execute_reply": "2026-01-30T06:04:34.304902Z"
    }
   },
   "outputs": [],
   "source": [
    "# Bayesian inference on best model per dataset\n",
    "def _plot_data_and_fits(ds, fits):\n",
    "    \"\"\"Plot data with model fits overlay.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(9, 6))\n",
    "    X_plot, y_true = ds['X'], ds['y']\n",
    "    if np.iscomplexobj(y_true):\n",
    "        x_axis = X_plot / (2 * np.pi) if ds['test_mode'] == 'oscillation' else X_plot\n",
    "        ax.loglog(x_axis, np.real(y_true), 'o', label=\"Data real\", alpha=0.6)\n",
    "        ax.loglog(x_axis, np.imag(y_true), 's', label=\"Data imag\", alpha=0.6)\n",
    "        for rec in fits:\n",
    "            pred = _coerce_pred(rec['pred'])\n",
    "            ax.loglog(x_axis, np.real(pred), '-', label=f\"{rec['name']} Re (R²={rec['r2']:.3f})\")\n",
    "            ax.loglog(x_axis, np.imag(pred), '--', label=f\"{rec['name']} Im\")\n",
    "    else:\n",
    "        ax.loglog(X_plot, y_true, 'o', label='Data', alpha=0.6)\n",
    "        for rec in fits:\n",
    "            ax.loglog(X_plot, rec['pred'], '-', label=f\"{rec['name']} (R²={rec['r2']:.3f})\")\n",
    "    ax.set_xlabel(ds.get('x_label', 'X'))\n",
    "    ax.set_ylabel(ds.get('y_label', 'Response'))\n",
    "    ax.set_title(ds.get('label', f\"test_mode={ds['test_mode']}\"))\n",
    "    ax.grid(True, which='both', ls='--', alpha=0.4)\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    display(fig)\n",
    "    plt.close(fig)\n",
    "\n",
    "bayes_results = {}  # {dataset_idx: inference_data}\n",
    "\n",
    "for ds_idx, ds in enumerate(datasets):\n",
    "    fits = all_fits.get(ds_idx, [])\n",
    "    if not fits:\n",
    "        continue\n",
    "\n",
    "    best = fits[0]\n",
    "    _plot_data_and_fits(ds, fits)\n",
    "\n",
    "    try:\n",
    "        bayes_result = best['model'].fit_bayesian(\n",
    "            ds['X'], ds['y'], **NUTS_CONFIG, test_mode=ds['test_mode'],\n",
    "        )\n",
    "        bayes_results[ds_idx] = bayes_result.to_inference_data()\n",
    "        print(f\"Dataset {ds_idx}: Bayesian inference complete for {best['name']}\")\n",
    "    except Exception as exc:\n",
    "        print(f\"Dataset {ds_idx}: Bayesian step failed for {best['name']}: {exc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcd36ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T06:04:34.310351Z",
     "iopub.status.busy": "2026-01-30T06:04:34.310146Z",
     "iopub.status.idle": "2026-01-30T06:04:37.416322Z",
     "shell.execute_reply": "2026-01-30T06:04:37.401662Z"
    }
   },
   "outputs": [],
   "source": [
    "# ArviZ diagnostics for Bayesian results\n",
    "def _plot_diagnostics(idata):\n",
    "    \"\"\"Generate ArviZ diagnostic plots.\"\"\"\n",
    "    diag_plotters = [\n",
    "        ('pair', lambda: az.plot_pair(idata, divergences=True, kind='kde')),\n",
    "        ('forest', lambda: az.plot_forest(idata, combined=True)),\n",
    "        ('energy', lambda: az.plot_energy(idata)),\n",
    "        ('autocorr', lambda: az.plot_autocorr(idata)),\n",
    "        ('rank', lambda: az.plot_rank(idata)),\n",
    "        ('ess', lambda: az.plot_ess(idata, kind='evolution')),\n",
    "    ]\n",
    "    for name, plot_fn in diag_plotters:\n",
    "        try:\n",
    "            obj = plot_fn()\n",
    "            plt.tight_layout()\n",
    "            display(obj)\n",
    "            plt.close('all')\n",
    "        except Exception as exc:\n",
    "            print(f\"Skipping {name} plot: {exc}\")\n",
    "\n",
    "for ds_idx, idata in bayes_results.items():\n",
    "    print(f\"\\n--- Diagnostics for dataset {ds_idx} ---\")\n",
    "    _plot_diagnostics(idata)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
