{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rheological Protocol Validation Index\n",
    "\n",
    "**Dashboard for tracking protocol validation across all rheological experiments.**\n",
    "\n",
    "This notebook provides:\n",
    "1. Protocol coverage matrix (which data files support which protocols)\n",
    "2. Links to individual protocol validation notebooks\n",
    "3. Summary of validation status\n",
    "4. Instructions for running the full validation suite\n",
    "\n",
    "## Supported Protocols\n",
    "\n",
    "| Protocol | Description | Key Checks |\n",
    "|----------|-------------|------------|\n",
    "| **Flow Curve** | σ vs γ̇ at steady state | Shear thinning, positive values |\n",
    "| **Creep** | J(t) = γ(t)/σ₀ | Monotonic compliance |\n",
    "| **Stress Relaxation** | G(t) = σ(t)/γ₀ | Monotonic decay |\n",
    "| **Startup Shear** | σ(t) at constant γ̇ | Overshoot detection |\n",
    "| **SAOS** | G'(ω), G''(ω) | Positive moduli, KK consistency |\n",
    "| **LAOS** | σ(γ, t) nonlinear | Harmonic analysis, Lissajous closure |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODE = \"FAST\"  # \"FAST\" or \"FULL\"\n",
    "\n",
    "if MODE == \"FAST\":\n",
    "    MAX_FILES_PER_PROTOCOL = 2\n",
    "    SKIP_HEAVY_PLOTS = True\n",
    "    SAVE_ARTIFACTS = False\n",
    "else:\n",
    "    MAX_FILES_PER_PROTOCOL = None  # all files\n",
    "    SKIP_HEAVY_PLOTS = False\n",
    "    SAVE_ARTIFACTS = True\n",
    "\n",
    "print(f\"Running in {MODE} mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Add project root to path for imports\n",
    "project_root = Path.cwd()\n",
    "while not (project_root / \"pyproject.toml\").exists() and project_root != project_root.parent:\n",
    "    project_root = project_root.parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "from examples.verification.utils.validation_utils import (\n",
    "    discover_files_by_protocol,\n",
    "    get_data_dir,\n",
    "    Protocol,\n",
    ")\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Data directory: {get_data_dir()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Inventory\n",
    "\n",
    "Discover available data files for each protocol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "protocols: list[Protocol] = [\n",
    "    \"flow_curve\",\n",
    "    \"creep\",\n",
    "    \"stress_relaxation\",\n",
    "    \"startup_shear\",\n",
    "    \"saos\",\n",
    "    \"laos\",\n",
    "]\n",
    "\n",
    "data_dir = get_data_dir()\n",
    "\n",
    "# Build inventory\n",
    "inventory = []\n",
    "for protocol in protocols:\n",
    "    files = discover_files_by_protocol(data_dir, protocol)\n",
    "    inventory.append({\n",
    "        \"Protocol\": protocol,\n",
    "        \"File Count\": len(files),\n",
    "        \"Extensions\": \", \".join(sorted(set(f.suffix for f in files))),\n",
    "        \"Directories\": \", \".join(sorted(set(f.parent.name for f in files))),\n",
    "    })\n",
    "\n",
    "inventory_df = pd.DataFrame(inventory)\n",
    "inventory_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Protocol Coverage Matrix\n",
    "\n",
    "Sample files for each protocol (first 3 per protocol)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for protocol in protocols:\n",
    "    files = discover_files_by_protocol(data_dir, protocol)\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Protocol: {protocol.upper()} ({len(files)} files)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    for f in files[:3]:\n",
    "        print(f\"  - {f.relative_to(data_dir)}\")\n",
    "    if len(files) > 3:\n",
    "        print(f\"  ... and {len(files) - 3} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Validation Notebook Links\n",
    "\n",
    "Click to open each protocol validation notebook:\n",
    "\n",
    "| Notebook | Protocol | Status |\n",
    "|----------|----------|--------|\n",
    "| [01_validate_flow_curve.ipynb](01_validate_flow_curve.ipynb) | Flow Curve | Ready |\n",
    "| [02_validate_creep.ipynb](02_validate_creep.ipynb) | Creep | Ready |\n",
    "| [03_validate_stress_relaxation.ipynb](03_validate_stress_relaxation.ipynb) | Stress Relaxation | Ready |\n",
    "| [04_validate_startup_shear.ipynb](04_validate_startup_shear.ipynb) | Startup Shear | Ready |\n",
    "| [05_validate_saos.ipynb](05_validate_saos.ipynb) | SAOS | Ready |\n",
    "| [06_validate_laos.ipynb](06_validate_laos.ipynb) | LAOS | Ready |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Validation Status Summary\n",
    "\n",
    "Check for existing validation reports from previous runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from examples.verification.utils.validation_utils import get_output_dir\n",
    "\n",
    "status_summary = []\n",
    "\n",
    "for protocol in protocols:\n",
    "    output_dir = get_output_dir(protocol)\n",
    "    report_path = output_dir / \"validation_report.json\"\n",
    "    \n",
    "    if report_path.exists():\n",
    "        with open(report_path) as f:\n",
    "            report = json.load(f)\n",
    "        status = \"PASS\" if report.get(\"all_passed\", False) else \"FAIL\"\n",
    "        n_files = report.get(\"n_files_validated\", \"?\")\n",
    "        timestamp = report.get(\"generated_at\", \"unknown\")[:19]\n",
    "    else:\n",
    "        status = \"NOT RUN\"\n",
    "        n_files = \"-\"\n",
    "        timestamp = \"-\"\n",
    "    \n",
    "    status_summary.append({\n",
    "        \"Protocol\": protocol,\n",
    "        \"Status\": status,\n",
    "        \"Files Validated\": n_files,\n",
    "        \"Last Run\": timestamp,\n",
    "    })\n",
    "\n",
    "status_df = pd.DataFrame(status_summary)\n",
    "status_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Instructions for Running Validation Suite\n",
    "\n",
    "### Quick Validation (FAST mode)\n",
    "\n",
    "```bash\n",
    "# Run all validation notebooks in FAST mode (2 files per protocol)\n",
    "cd examples/verification\n",
    "for nb in 0[1-6]_validate_*.ipynb; do\n",
    "    uv run jupyter execute \"$nb\" --inplace\n",
    "done\n",
    "```\n",
    "\n",
    "### Full Validation (FULL mode)\n",
    "\n",
    "Edit each notebook to set `MODE = \"FULL\"`, then run:\n",
    "\n",
    "```bash\n",
    "# Run all validation notebooks in FULL mode (all files)\n",
    "cd examples/verification\n",
    "for nb in 0[1-6]_validate_*.ipynb; do\n",
    "    uv run jupyter execute \"$nb\" --inplace\n",
    "done\n",
    "```\n",
    "\n",
    "### Individual Protocol\n",
    "\n",
    "```bash\n",
    "# Run a single protocol validation\n",
    "uv run jupyter execute examples/verification/01_validate_flow_curve.ipynb --inplace\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Output Directory Structure\n",
    "\n",
    "Validation artifacts are saved to:\n",
    "\n",
    "```\n",
    "examples/verification/outputs/\n",
    "├── flow_curve/\n",
    "│   ├── cleaned_data/      # Preprocessed data files\n",
    "│   ├── derived_quantities/ # Computed quantities (η, J, G, etc.)\n",
    "│   ├── plots/             # Validation plots\n",
    "│   └── validation_report.json\n",
    "├── creep/\n",
    "├── stress_relaxation/\n",
    "├── startup_shear/\n",
    "├── saos/\n",
    "└── laos/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check output directory structure\n",
    "outputs_root = Path(\"outputs\")\n",
    "if outputs_root.exists():\n",
    "    for protocol_dir in sorted(outputs_root.iterdir()):\n",
    "        if protocol_dir.is_dir():\n",
    "            n_files = sum(1 for _ in protocol_dir.rglob(\"*\") if _.is_file())\n",
    "            print(f\"{protocol_dir.name}/: {n_files} files\")\n",
    "else:\n",
    "    print(\"Output directories not yet created. Run validation notebooks first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Standard rheological protocol references:\n",
    "\n",
    "- **SAOS**: Ferry, J.D. \"Viscoelastic Properties of Polymers\" (1980)\n",
    "- **LAOS**: Ewoldt et al. \"New measures for characterizing nonlinear viscoelasticity\" (2008) J. Rheol.\n",
    "- **Flow curves**: Macosko, C.W. \"Rheology: Principles, Measurements, and Applications\" (1994)\n",
    "- **Creep/Relaxation**: Tschoegl, N.W. \"Phenomenological Theory of Linear Viscoelastic Behavior\" (1989)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
