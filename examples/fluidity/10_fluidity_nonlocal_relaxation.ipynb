{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FluidityNonlocal: Stress Relaxation with Spatial Fluidity Diffusion\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "1. **Understand spatial relaxation dynamics**: How fluidity diffusion homogenizes stress across the gap during relaxation\n",
    "2. **NLSQ fitting**: Fit relaxation modulus G(t) with spatial coupling\n",
    "3. **Bayesian inference**: Quantify parameter uncertainty (D_f, G, eta_s, lambda_0, tau_eq, a, c)\n",
    "4. **Fluidity homogenization**: Visualize spatial profile evolution from initial heterogeneity to uniform state\n",
    "5. **Model diagnostics**: ArviZ convergence checks (R-hat, ESS, trace plots)\n",
    "\n",
    "---\n",
    "\n",
    "## Theoretical Background\n",
    "\n",
    "### Relaxation Protocol\n",
    "\n",
    "Sudden strain imposition γ₀ at t=0, then zero strain rate:\n",
    "\n",
    "$$\n",
    "\\gamma(t) = \\gamma_0 H(t), \\quad \\dot{\\gamma}(t) = \\gamma_0 \\delta(t)\n",
    "$$\n",
    "\n",
    "where H(t) is the Heaviside step function.\n",
    "\n",
    "### Governing Equations\n",
    "\n",
    "**Viscoelastic stress** (Maxwell backbone):\n",
    "$$\n",
    "\\sigma(y,t) + \\lambda(y,t) \\frac{\\partial \\sigma}{\\partial t} = \\eta_s \\dot{\\gamma}(t)\n",
    "$$\n",
    "\n",
    "**Fluidity evolution** (diffusion + thixotropy):\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial t} = \\frac{1 - f}{\\tau_{\\text{eq}}} + a f |\\dot{\\gamma}|^c + D_f \\frac{\\partial^2 f}{\\partial y^2}\n",
    "$$\n",
    "\n",
    "where λ(y,t) = 1/f(y,t) is the relaxation time.\n",
    "\n",
    "### Relaxation Modulus\n",
    "\n",
    "$$\n",
    "G(t) = \\frac{\\langle \\sigma(y,t) \\rangle}{\\gamma_0}\n",
    "$$\n",
    "\n",
    "Initial condition: σ(y,0) = G·γ₀ (instantaneous elastic response), f(y,0) can be spatially heterogeneous from prior shear history.\n",
    "\n",
    "### Key Physics\n",
    "\n",
    "1. **Elastic jump**: G(0⁺) = G (shear modulus)\n",
    "2. **Spatial homogenization**: D_f diffuses fluidity from high-f (fluid) to low-f (solid) regions\n",
    "3. **Structural recovery**: f → 1 (equilibrium) via 1/τ_eq aging\n",
    "4. **Decay timescale**: Controlled by λ_avg(t) = 1/⟨f(y,t)⟩ and τ_eq\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab setup (installs RheoJAX if not present)\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"Running in Google Colab\")\n",
    "    \n",
    "    # Install rheojax\n",
    "    !pip install -q rheojax\n",
    "    \n",
    "    # Create output directory\n",
    "    !mkdir -p outputs/fluidity/nonlocal/relaxation\n",
    "    \n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"Running locally\")\n",
    "    \n",
    "    # Create output directory (local)\n",
    "    import os\n",
    "    os.makedirs(\"../outputs/fluidity/nonlocal/relaxation\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Float64 enforcement (CRITICAL for numerical stability)\n",
    "from rheojax.core.jax_config import safe_import_jax\n",
    "jax, jnp = safe_import_jax()\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# RheoJAX imports\n",
    "from rheojax.models.fluidity import FluidityNonlocal\n",
    "from rheojax.core.data import RheoData\n",
    "from rheojax.logging import configure_logging, get_logger\n",
    "\n",
    "# Bayesian imports\n",
    "import arviz as az\n",
    "\n",
    "# Configure logging\n",
    "configure_logging(level=\"INFO\")\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "# Plotting aesthetics\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams.update({\n",
    "    'font.size': 11,\n",
    "    'axes.labelsize': 12,\n",
    "    'axes.titlesize': 13,\n",
    "    'legend.fontsize': 10,\n",
    "    'figure.dpi': 100\n",
    "})\n",
    "\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "print(f\"JAX devices: {jax.devices()}\")\n",
    "print(f\"Float64 enabled: {jax.config.jax_enable_x64}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Load Calibrated Parameters or Use Defaults\n",
    "\n",
    "We'll use realistic parameters for a yield stress fluid with spatial heterogeneity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default parameters (representative of carbopol gel)\n",
    "params_default = {\n",
    "    'G': 100.0,           # Pa - Elastic modulus\n",
    "    'eta_s': 10.0,        # Pa·s - Solvent viscosity\n",
    "    'lambda_0': 1.0,      # s - Reference relaxation time (1/f_eq)\n",
    "    'tau_eq': 10.0,       # s - Structural recovery time\n",
    "    'a': 1.0,             # Dimensionless - Rejuvenation strength\n",
    "    'c': 1.0,             # Dimensionless - Shear-rate exponent\n",
    "    'D_f': 1e-4,          # m²/s - Fluidity diffusion coefficient\n",
    "    'gap_width': 1e-3     # m - Gap size (1 mm)\n",
    "}\n",
    "\n",
    "# Try loading from startup simulation if available\n",
    "output_dir_local = Path(\"../outputs/fluidity/nonlocal/relaxation\")\n",
    "output_dir_colab = Path(\"outputs/fluidity/nonlocal/relaxation\")\n",
    "output_dir = output_dir_colab if IN_COLAB else output_dir_local\n",
    "\n",
    "params_file = output_dir.parent / \"startup\" / \"fitted_params.npz\"\n",
    "\n",
    "if params_file.exists():\n",
    "    logger.info(f\"Loading parameters from {params_file}\")\n",
    "    loaded = np.load(params_file)\n",
    "    params = {k: float(loaded[k]) for k in loaded.files}\n",
    "    print(\"Loaded calibrated parameters from startup simulation\")\n",
    "else:\n",
    "    logger.info(\"Using default parameters\")\n",
    "    params = params_default.copy()\n",
    "    print(\"Using default parameters (no calibrated file found)\")\n",
    "\n",
    "# Display parameters\n",
    "print(\"\\nModel Parameters:\")\n",
    "for key, val in params.items():\n",
    "    print(f\"  {key:12s} = {val:.4e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Generate Synthetic Relaxation Data\n",
    "\n",
    "Simulate stress relaxation from an initial heterogeneous fluidity profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model with parameters\n",
    "model_true = FluidityNonlocal(\n",
    "    n_points=51,\n",
    "    gap_width=params['gap_width'],\n",
    "    coupling='full'\n",
    ")\n",
    "\n",
    "# Set parameters\n",
    "model_true.parameters.set_values({\n",
    "    'G': params['G'],\n",
    "    'eta_s': params['eta_s'],\n",
    "    'lambda_0': params['lambda_0'],\n",
    "    'tau_eq': params['tau_eq'],\n",
    "    'a': params['a'],\n",
    "    'c': params['c'],\n",
    "    'D_f': params['D_f']\n",
    "})\n",
    "\n",
    "# Relaxation protocol parameters\n",
    "gamma_0 = 0.1        # Applied strain (10%)\n",
    "t_end = 100.0        # s - Total relaxation time\n",
    "n_times = 200        # Time points\n",
    "\n",
    "# Initial fluidity profile (heterogeneous from prior shear)\n",
    "# Example: gradient from f=0.5 (center) to f=1.5 (walls)\n",
    "y_grid = np.linspace(0, params['gap_width'], model_true.n_points)\n",
    "f_init = 1.0 + 0.5 * np.cos(np.pi * y_grid / params['gap_width'])  # Cosine profile\n",
    "\n",
    "print(f\"Generating relaxation data with γ₀={gamma_0:.2%}, t_end={t_end}s\")\n",
    "print(f\"Initial fluidity range: [{f_init.min():.3f}, {f_init.max():.3f}]\")\n",
    "\n",
    "# Simulate relaxation\n",
    "result = model_true.simulate_relaxation(\n",
    "    gamma_0=gamma_0,\n",
    "    t_end=t_end,\n",
    "    n_times=n_times,\n",
    "    f_init=f_init\n",
    ")\n",
    "\n",
    "t_relax = result['time']\n",
    "G_t_true = result['G_t']  # Relaxation modulus\n",
    "f_profile = result['fluidity_profile']  # (n_times, n_points)\n",
    "\n",
    "# Add 3% Gaussian noise\n",
    "rng = np.random.RandomState(42)\n",
    "noise_level = 0.03\n",
    "noise = rng.normal(0, noise_level * np.std(G_t_true), size=G_t_true.shape)\n",
    "G_t_noisy = G_t_true + noise\n",
    "\n",
    "print(f\"Generated {len(t_relax)} time points from {t_relax[0]:.2e}s to {t_relax[-1]:.2e}s\")\n",
    "print(f\"G(0⁺) = {G_t_noisy[0]:.2f} Pa (expected: {params['G']:.2f} Pa)\")\n",
    "print(f\"G(t_end) = {G_t_noisy[-1]:.2f} Pa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize synthetic data\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Relaxation modulus\n",
    "ax = axes[0]\n",
    "ax.plot(t_relax, G_t_true, 'b-', linewidth=2, label='True G(t)')\n",
    "ax.plot(t_relax, G_t_noisy, 'ro', markersize=4, alpha=0.5, label='Noisy data (3%)')\n",
    "ax.set_xlabel('Time (s)')\n",
    "ax.set_ylabel('Relaxation Modulus G(t) (Pa)')\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.set_title('Stress Relaxation Data')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Fluidity profile evolution\n",
    "ax = axes[1]\n",
    "y_mm = y_grid * 1e3  # Convert to mm\n",
    "\n",
    "# Plot profiles at different times\n",
    "time_indices = [0, len(t_relax)//4, len(t_relax)//2, 3*len(t_relax)//4, -1]\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(time_indices)))\n",
    "\n",
    "for i, idx in enumerate(time_indices):\n",
    "    ax.plot(y_mm, f_profile[idx, :], color=colors[i], \n",
    "            linewidth=2, label=f't={t_relax[idx]:.1f}s')\n",
    "\n",
    "ax.set_xlabel('Position y (mm)')\n",
    "ax.set_ylabel('Fluidity f(y)')\n",
    "ax.set_title('Fluidity Profile Homogenization')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'synthetic_relaxation_data.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFluidity homogenization:\")\n",
    "print(f\"  t=0s:     Δf = {f_profile[0, :].max() - f_profile[0, :].min():.4f}\")\n",
    "print(f\"  t={t_relax[-1]:.1f}s: Δf = {f_profile[-1, :].max() - f_profile[-1, :].min():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. NLSQ Fitting with `test_mode='relaxation'`\n",
    "\n",
    "Fit the relaxation modulus to estimate parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RheoData object\n",
    "rheo_data = RheoData(\n",
    "    x=t_relax,\n",
    "    y=G_t_noisy,\n",
    "    x_label='time',\n",
    "    y_label='G_t',\n",
    "    test_mode='relaxation'\n",
    ")\n",
    "\n",
    "# Initialize model for fitting\n",
    "model_fit = FluidityNonlocal(\n",
    "    n_points=51,\n",
    "    gap_width=params['gap_width'],\n",
    "    coupling='full'\n",
    ")\n",
    "\n",
    "# Set initial guesses (perturbed from true values)\n",
    "initial_guess = {\n",
    "    'G': params['G'] * 0.8,\n",
    "    'eta_s': params['eta_s'] * 1.2,\n",
    "    'lambda_0': params['lambda_0'] * 0.9,\n",
    "    'tau_eq': params['tau_eq'] * 1.1,\n",
    "    'a': params['a'] * 0.85,\n",
    "    'c': params['c'] * 1.05,\n",
    "    'D_f': params['D_f'] * 1.3\n",
    "}\n",
    "\n",
    "model_fit.parameters.set_values(initial_guess)\n",
    "\n",
    "print(\"Starting NLSQ optimization...\")\n",
    "print(f\"Initial guess (perturbed from true):\")\n",
    "for key, val in initial_guess.items():\n",
    "    true_val = params[key]\n",
    "    print(f\"  {key:12s} = {val:.4e} (true: {true_val:.4e}, error: {100*(val-true_val)/true_val:+.1f}%)\")\n",
    "\n",
    "# Fit with NLSQ\n",
    "result_nlsq = model_fit.fit(\n",
    "    rheo_data,\n",
    "    gamma_0=gamma_0,\n",
    "    f_init=f_init,  # Use same initial condition\n",
    "    max_iter=2000,\n",
    "    verbose=True\n",
    ", method='scipy')\n",
    "\n",
    "print(f\"\\nNLSQ Optimization complete:\")\n",
    "print(f\"  R² = {result_nlsq.r_squared:.6f}\")\n",
    "print(f\"  Iterations: {result_nlsq.n_iter}\")\n",
    "print(f\"  Success: {result_nlsq.success}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare fitted vs true parameters\n",
    "fitted_params = model_fit.parameters.get_values()\n",
    "\n",
    "print(\"\\nParameter Recovery:\")\n",
    "print(f\"{'Parameter':<12s} {'True':>12s} {'Fitted':>12s} {'Error':>10s}\")\n",
    "print(\"-\" * 50)\n",
    "for key in ['G', 'eta_s', 'lambda_0', 'tau_eq', 'a', 'c', 'D_f']:\n",
    "    true_val = params[key]\n",
    "    fitted_val = fitted_params[key]\n",
    "    error = 100 * (fitted_val - true_val) / true_val\n",
    "    print(f\"{key:<12s} {true_val:>12.4e} {fitted_val:>12.4e} {error:>9.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize NLSQ fit\n",
    "G_t_pred = model_fit.predict(rheo_data, gamma_0=gamma_0, f_init=f_init)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Fitted curve\n",
    "ax = axes[0]\n",
    "ax.plot(t_relax, G_t_noisy, 'bo', markersize=5, alpha=0.5, label='Data')\n",
    "ax.plot(t_relax, G_t_true, 'g--', linewidth=2, label='True model')\n",
    "ax.plot(t_relax, G_t_pred, 'r-', linewidth=2, label=f'NLSQ fit (R²={result_nlsq.r_squared:.4f})')\n",
    "ax.set_xlabel('Time (s)')\n",
    "ax.set_ylabel('G(t) (Pa)')\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.set_title('NLSQ Fit: Relaxation Modulus')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Residuals\n",
    "ax = axes[1]\n",
    "residuals = G_t_noisy - G_t_pred\n",
    "ax.plot(t_relax, residuals, 'ko', markersize=4, alpha=0.6)\n",
    "ax.axhline(0, color='r', linestyle='--', linewidth=1)\n",
    "ax.fill_between(t_relax, -2*np.std(residuals), 2*np.std(residuals), \n",
    "                 color='gray', alpha=0.2, label='±2σ')\n",
    "ax.set_xlabel('Time (s)')\n",
    "ax.set_ylabel('Residuals (Pa)')\n",
    "ax.set_xscale('log')\n",
    "ax.set_title(f'Residuals (σ={np.std(residuals):.3f} Pa)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'nlsq_fit_relaxation.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Bayesian Inference with NumPyro\n",
    "\n",
    "Quantify parameter uncertainties using NUTS sampling with NLSQ warm-start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up priors (informative based on NLSQ fit)\n",
    "fitted_vals = model_fit.parameters.get_values()\n",
    "\n",
    "# Use 20% coefficient of variation for priors\n",
    "cv = 0.2\n",
    "for param_name in ['G', 'eta_s', 'lambda_0', 'tau_eq', 'a', 'c', 'D_f']:\n",
    "    param = model_fit.parameters.get(param_name)\n",
    "    fitted_val = fitted_vals[param_name]\n",
    "    \n",
    "    # Set prior mean to fitted value\n",
    "    # Set prior std to 20% of fitted value\n",
    "    param.prior_type = 'normal'\n",
    "    param.prior_params = {\n",
    "        'loc': fitted_val,\n",
    "        'scale': cv * fitted_val\n",
    "    }\n",
    "\n",
    "print(\"Priors set based on NLSQ fit (Normal with 20% CV):\")\n",
    "for param_name in ['G', 'eta_s', 'lambda_0', 'tau_eq', 'a', 'c', 'D_f']:\n",
    "    param = model_fit.parameters.get(param_name)\n",
    "    print(f\"  {param_name:12s}: N({param.prior_params['loc']:.4e}, {param.prior_params['scale']:.4e})\")\n",
    "\n",
    "# Run Bayesian inference\n",
    "print(\"\\nStarting NUTS sampling (4 chains, warm-start from NLSQ)...\")\n",
    "result_bayes = model_fit.fit_bayesian(\n",
    "    rheo_data,\n",
    "    num_warmup=1000,\n",
    "    num_samples=2000,\n",
    "    num_chains=4,\n",
    "    seed=42,\n",
    "    gamma_0=gamma_0,\n",
    "    f_init=f_init\n",
    ")\n",
    "\n",
    "print(\"\\nBayesian inference complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. ArviZ Diagnostics\n",
    "\n",
    "Convergence checks: R-hat, ESS, trace plots, pair plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to ArviZ InferenceData\n",
    "idata = az.from_numpyro(result_bayes.mcmc)\n",
    "\n",
    "# Summary statistics\n",
    "summary = az.summary(idata, hdi_prob=0.95)\n",
    "print(\"\\nPosterior Summary (95% HDI):\")\n",
    "print(summary)\n",
    "\n",
    "# Check convergence\n",
    "print(\"\\nConvergence Diagnostics:\")\n",
    "print(f\"  Max R-hat: {summary['r_hat'].max():.4f} (target: <1.01)\")\n",
    "print(f\"  Min ESS bulk: {summary['ess_bulk'].min():.0f} (target: >400)\")\n",
    "print(f\"  Min ESS tail: {summary['ess_tail'].min():.0f} (target: >400)\")\n",
    "\n",
    "if summary['r_hat'].max() > 1.01:\n",
    "    print(\"  WARNING: R-hat > 1.01 detected. Consider increasing num_warmup/num_samples.\")\n",
    "else:\n",
    "    print(\"  ✓ All R-hat values < 1.01 (good convergence)\")\n",
    "\n",
    "if summary['ess_bulk'].min() < 400:\n",
    "    print(\"  WARNING: Low ESS detected. Consider increasing num_samples.\")\n",
    "else:\n",
    "    print(\"  ✓ All ESS values > 400 (sufficient samples)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trace plots\n",
    "fig = az.plot_trace(\n",
    "    idata,\n",
    "    var_names=['G', 'eta_s', 'lambda_0', 'tau_eq', 'a', 'c', 'D_f'],\n",
    "    compact=True,\n",
    "    figsize=(14, 10)\n",
    ")\n",
    "plt.suptitle('NUTS Trace Plots', y=1.001, fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'trace_plots.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair plot (correlations)\n",
    "fig = az.plot_pair(\n",
    "    idata,\n",
    "    var_names=['G', 'eta_s', 'lambda_0', 'tau_eq', 'D_f'],\n",
    "    kind='hexbin',\n",
    "    marginals=True,\n",
    "    figsize=(12, 12)\n",
    ")\n",
    "plt.suptitle('Posterior Correlations', y=1.001, fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'pair_plot.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forest plot (credible intervals)\n",
    "fig = az.plot_forest(\n",
    "    idata,\n",
    "    var_names=['G', 'eta_s', 'lambda_0', 'tau_eq', 'a', 'c', 'D_f'],\n",
    "    combined=True,\n",
    "    hdi_prob=0.95,\n",
    "    figsize=(10, 6)\n",
    ")\n",
    "plt.title('95% Credible Intervals', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'forest_plot.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior predictive check\n",
    "posterior_samples = result_bayes.posterior_samples\n",
    "\n",
    "# Draw 100 posterior samples\n",
    "n_posterior_draws = 100\n",
    "indices = np.random.choice(len(posterior_samples['G']), size=n_posterior_draws, replace=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot posterior predictions\n",
    "for idx in indices:\n",
    "    model_fit.parameters.set_values({\n",
    "        'G': float(posterior_samples['G'][idx]),\n",
    "        'eta_s': float(posterior_samples['eta_s'][idx]),\n",
    "        'lambda_0': float(posterior_samples['lambda_0'][idx]),\n",
    "        'tau_eq': float(posterior_samples['tau_eq'][idx]),\n",
    "        'a': float(posterior_samples['a'][idx]),\n",
    "        'c': float(posterior_samples['c'][idx]),\n",
    "        'D_f': float(posterior_samples['D_f'][idx])\n",
    "    })\n",
    "    G_t_post = model_fit.predict(rheo_data, gamma_0=gamma_0, f_init=f_init)\n",
    "    ax.plot(t_relax, G_t_post, 'r-', alpha=0.05, linewidth=1)\n",
    "\n",
    "# Overlay data\n",
    "ax.plot(t_relax, G_t_noisy, 'bo', markersize=5, alpha=0.6, label='Data', zorder=10)\n",
    "ax.plot(t_relax, G_t_true, 'g--', linewidth=2, label='True model', zorder=11)\n",
    "\n",
    "ax.set_xlabel('Time (s)')\n",
    "ax.set_ylabel('G(t) (Pa)')\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.set_title('Posterior Predictive Check (100 draws)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'posterior_predictive.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Fluidity Profile Homogenization During Relaxation\n",
    "\n",
    "Visualize how spatial fluidity gradients diffuse over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use median posterior parameters for simulation\n",
    "median_params = {\n",
    "    'G': float(np.median(posterior_samples['G'])),\n",
    "    'eta_s': float(np.median(posterior_samples['eta_s'])),\n",
    "    'lambda_0': float(np.median(posterior_samples['lambda_0'])),\n",
    "    'tau_eq': float(np.median(posterior_samples['tau_eq'])),\n",
    "    'a': float(np.median(posterior_samples['a'])),\n",
    "    'c': float(np.median(posterior_samples['c'])),\n",
    "    'D_f': float(np.median(posterior_samples['D_f']))\n",
    "}\n",
    "\n",
    "model_median = FluidityNonlocal(\n",
    "    n_points=51,\n",
    "    gap_width=params['gap_width'],\n",
    "    coupling='full'\n",
    ")\n",
    "model_median.parameters.set_values(median_params)\n",
    "\n",
    "# Simulate relaxation with median parameters\n",
    "result_median = model_median.simulate_relaxation(\n",
    "    gamma_0=gamma_0,\n",
    "    t_end=t_end,\n",
    "    n_times=n_times,\n",
    "    f_init=f_init\n",
    ")\n",
    "\n",
    "f_profile_median = result_median['fluidity_profile']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot fluidity evolution: 2D heatmap + line profiles\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Heatmap (fluidity vs position and time)\n",
    "ax = axes[0]\n",
    "y_mm = y_grid * 1e3\n",
    "t_plot, y_plot = np.meshgrid(t_relax, y_mm)\n",
    "\n",
    "contour = ax.contourf(t_plot, y_plot, f_profile_median.T, levels=20, cmap='viridis')\n",
    "ax.set_xlabel('Time (s)')\n",
    "ax.set_ylabel('Position y (mm)')\n",
    "ax.set_xscale('log')\n",
    "ax.set_title('Fluidity f(y, t) Evolution')\n",
    "cbar = plt.colorbar(contour, ax=ax)\n",
    "cbar.set_label('Fluidity f')\n",
    "\n",
    "# Plot 2: Line profiles at specific times\n",
    "ax = axes[1]\n",
    "time_indices = [0, len(t_relax)//4, len(t_relax)//2, 3*len(t_relax)//4, -1]\n",
    "colors = plt.cm.plasma(np.linspace(0, 1, len(time_indices)))\n",
    "\n",
    "for i, idx in enumerate(time_indices):\n",
    "    ax.plot(y_mm, f_profile_median[idx, :], color=colors[i], \n",
    "            linewidth=2, marker='o', markersize=4, label=f't={t_relax[idx]:.2e}s')\n",
    "\n",
    "ax.set_xlabel('Position y (mm)')\n",
    "ax.set_ylabel('Fluidity f(y)')\n",
    "ax.set_title('Fluidity Profiles at Different Times')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'fluidity_homogenization.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Quantify homogenization\n",
    "f_variance = np.var(f_profile_median, axis=1)  # Variance across gap at each time\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(t_relax, f_variance, 'b-', linewidth=2)\n",
    "ax.set_xlabel('Time (s)')\n",
    "ax.set_ylabel('Fluidity Variance Var[f(y)]')\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.set_title('Spatial Homogenization Rate')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Estimate homogenization time (when variance drops to 1% of initial)\n",
    "threshold = 0.01 * f_variance[0]\n",
    "homogenization_idx = np.where(f_variance < threshold)[0]\n",
    "if len(homogenization_idx) > 0:\n",
    "    t_homog = t_relax[homogenization_idx[0]]\n",
    "    ax.axvline(t_homog, color='r', linestyle='--', linewidth=2, \n",
    "               label=f'Homogenization time ≈ {t_homog:.2f}s')\n",
    "    ax.legend()\n",
    "    print(f\"\\nHomogenization time (Var[f] < 1% initial): {t_homog:.2f}s\")\n",
    "    print(f\"Diffusive length scale: √(D_f·t_homog) = {np.sqrt(median_params['D_f']*t_homog)*1e3:.4f} mm\")\n",
    "    print(f\"Gap width: {params['gap_width']*1e3:.4f} mm\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'homogenization_rate.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save fitted parameters\n",
    "np.savez(\n",
    "    output_dir / 'fitted_params.npz',\n",
    "    **median_params,\n",
    "    gap_width=params['gap_width'],\n",
    "    gamma_0=gamma_0\n",
    ")\n",
    "\n",
    "# Save posterior samples\n",
    "np.savez(\n",
    "    output_dir / 'posterior_samples.npz',\n",
    "    **posterior_samples\n",
    ")\n",
    "\n",
    "# Save synthetic data\n",
    "np.savez(\n",
    "    output_dir / 'synthetic_data.npz',\n",
    "    t=t_relax,\n",
    "    G_t_true=G_t_true,\n",
    "    G_t_noisy=G_t_noisy,\n",
    "    f_profile=f_profile,\n",
    "    f_init=f_init,\n",
    "    y_grid=y_grid\n",
    ")\n",
    "\n",
    "# Save summary statistics\n",
    "summary.to_csv(output_dir / 'posterior_summary.csv')\n",
    "\n",
    "# Save ArviZ InferenceData\n",
    "idata.to_netcdf(output_dir / 'inference_data.nc')\n",
    "\n",
    "print(f\"Results saved to {output_dir}/\")\n",
    "print(f\"  - fitted_params.npz\")\n",
    "print(f\"  - posterior_samples.npz\")\n",
    "print(f\"  - synthetic_data.npz\")\n",
    "print(f\"  - posterior_summary.csv\")\n",
    "print(f\"  - inference_data.nc\")\n",
    "print(f\"  - *.png (plots)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "### 1. Spatial Relaxation Dynamics\n",
    "- **Initial heterogeneity**: Fluidity profile f(y, t=0) can be non-uniform from prior shear history\n",
    "- **Diffusive homogenization**: D_f controls spatial smoothing timescale τ_diff ~ h²/D_f\n",
    "- **Structural aging**: τ_eq drives recovery toward equilibrium f → 1\n",
    "\n",
    "### 2. Relaxation Modulus Decay\n",
    "- **Elastic jump**: G(0⁺) = G (instantaneous)\n",
    "- **Multi-timescale decay**: Controlled by λ_avg(t) = 1/⟨f(y,t)⟩ and τ_eq\n",
    "- **Spatial coupling**: Nonlocal diffusion creates slower relaxation than local model\n",
    "\n",
    "### 3. Parameter Identifiability\n",
    "- **G**: Well-identified from G(0⁺)\n",
    "- **D_f**: Controls homogenization rate (measurable from initial heterogeneity)\n",
    "- **τ_eq, λ_0**: Control long-time decay\n",
    "- **a, c**: Weak influence in relaxation (better identified in flow protocols)\n",
    "\n",
    "### 4. NLSQ + Bayesian Workflow\n",
    "- **NLSQ**: Fast point estimate (~seconds to minutes)\n",
    "- **Bayesian**: Uncertainty quantification (~minutes with 4 chains)\n",
    "- **Convergence**: R-hat < 1.01, ESS > 400 confirms reliable posteriors\n",
    "\n",
    "### 5. Practical Insights\n",
    "- **Homogenization time**: Critical for experimental design (wait time between tests)\n",
    "- **Gap width dependence**: Larger gaps require longer relaxation for homogenization\n",
    "- **Initial condition sensitivity**: Relaxation protocol reveals spatial effects better than steady shear\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Compare to local model**: Quantify impact of spatial coupling\n",
    "2. **Vary gap width**: Study h-dependence of relaxation timescales\n",
    "3. **Different initial conditions**: Test response to various f(y, t=0) profiles\n",
    "4. **Multi-protocol fitting**: Combine relaxation + startup + LAOS for better parameter constraints\n",
    "\n",
    "---\n",
    "\n",
    "**References:**\n",
    "- Bocquet, L., Colin, A., & Ajdari, A. (2009). *Phys. Rev. Lett.* 103, 036001.\n",
    "- Picard, G., Ajdari, A., Bocquet, L., & Lequeux, F. (2002). *Eur. Phys. J. E* 15, 371.\n",
    "- Moorcroft, R. L., & Fielding, S. M. (2013). *Phys. Rev. Lett.* 110, 086001."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}