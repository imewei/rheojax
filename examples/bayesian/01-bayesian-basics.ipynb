{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Inference Basics: From Point Estimates to Uncertainty Quantification\n",
    "\n",
    "This notebook introduces Bayesian inference in rheological modeling, demonstrating the NLSQ→NUTS two-stage workflow that combines fast optimization with comprehensive uncertainty quantification.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "After completing this notebook, you will be able to:\n",
    "- Understand when Bayesian inference is essential vs optional\n",
    "- Implement the NLSQ→NUTS two-stage workflow with warm-start\n",
    "- Interpret posterior distributions and credible intervals\n",
    "- Verify convergence using R-hat and ESS diagnostics\n",
    "- Appreciate warm-start benefits (2-5x faster convergence)\n",
    "- Compare Bayesian credible intervals vs frequentist confidence intervals\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Basic understanding of Bayesian probability\n",
    "- Familiarity with Maxwell model (see `basic/01-maxwell-fitting.ipynb`)\n",
    "- Basic rheological concepts (relaxation, modulus)\n",
    "\n",
    "**Estimated Time:** 30 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction: Why Bayesian?\n",
    "\n",
    "### The Limitation of Point Estimates\n",
    "\n",
    "Traditional optimization (NLSQ, scipy.curve_fit) provides **point estimates** - single values for each parameter. However, these point estimates hide critical information:\n",
    "\n",
    "**Scenario:** Two datasets yield identical NLSQ fits (same G₀, same η), but:\n",
    "- Dataset A: High signal-to-noise ratio → parameters well-constrained\n",
    "- Dataset B: Low signal-to-noise ratio → parameters poorly constrained\n",
    "\n",
    "**Point estimates can't distinguish between these cases!**\n",
    "\n",
    "### Three Scenarios Where Bayesian is Essential\n",
    "\n",
    "1. **Poorly Constrained Parameters:** Wide posterior distributions reveal when data insufficient\n",
    "2. **Parameter Correlations:** Joint distributions show when parameters co-vary (identifiability issues)\n",
    "3. **Prediction Uncertainty:** Propagate parameter uncertainty to model predictions (error bars)\n",
    "\n",
    "### Bayesian vs Frequentist Interpretation\n",
    "\n",
    "**Frequentist Confidence Interval (95%):**\n",
    "- \"If we repeat experiment many times, 95% of intervals contain true value\"\n",
    "- Cannot say: \"95% probability parameter in this interval\" (frequentist philosophy)\n",
    "\n",
    "**Bayesian Credible Interval (95%):**\n",
    "- **\"95% probability parameter lies in this interval\"**\n",
    "- Direct probabilistic statement about parameter\n",
    "- More intuitive for scientific interpretation\n",
    "\n",
    "### Posterior Samples Enable Any Derived Quantity\n",
    "\n",
    "Once you have posterior samples, you can compute uncertainty for:\n",
    "- Any function of parameters (e.g., relaxation time τ = η/G₀)\n",
    "- Correlations between parameters\n",
    "- Quantiles, moments, or any statistical summary\n",
    "- Model predictions with uncertainty bands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:2025-11-01 22:16:57,229:jax._src.xla_bridge:808: Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: dlopen(libtpu.so, 0x0001): tried: 'libtpu.so' (no such file), '/System/Volumes/Preboot/Cryptexes/OSlibtpu.so' (no such file), '/usr/lib/libtpu.so' (no such file, not in dyld cache), 'libtpu.so' (no such file)\n",
      "Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: dlopen(libtpu.so, 0x0001): tried: 'libtpu.so' (no such file), '/System/Volumes/Preboot/Cryptexes/OSlibtpu.so' (no such file), '/usr/lib/libtpu.so' (no such file, not in dyld cache), 'libtpu.so' (no such file)\n",
      "Loading rheojax version 0.1.0\n",
      "arviz_base not installed\n",
      "arviz_stats not installed\n",
      "arviz_plots not installed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports successful\n",
      "JAX float64 enabled: False\n"
     ]
    }
   ],
   "source": [
    "# Configure matplotlib for inline plotting in VS Code/Jupyter\n",
    "import matplotlib\n",
    "\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "# Rheo imports\n",
    "from rheojax.models.maxwell import Maxwell\n",
    "from rheojax.core.jax_config import safe_import_jax\n",
    "\n",
    "# ArviZ for Bayesian diagnostics\n",
    "import arviz as az\n",
    "\n",
    "# Safe JAX import\n",
    "jax, jnp = safe_import_jax()\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Plotting configuration\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"✓ Imports successful\")\n",
    "print(f\"JAX float64 enabled: {jax.config.jax_default_dtype_bits == 64}\")\n",
    "\n",
    "# Suppress matplotlib inline backend warning\n",
    "# This warning is harmless - plots display correctly with %matplotlib inline\n",
    "warnings.filterwarnings('ignore', message='.*non-interactive.*')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Synthetic Relaxation Data\n",
    "\n",
    "We create Maxwell relaxation data with known parameters to validate the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Parameters:\n",
      "  G₀  = 1.00e+05 Pa\n",
      "  η   = 1.00e+03 Pa·s\n",
      "  τ   = 0.0100 s\n",
      "\n",
      "Data: 50 points from 0.01 to 100.00 s\n",
      "Noise: 1.5% relative (SNR: 36.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# True parameters\n",
    "G0_true = 1e5  # Pa\n",
    "eta_true = 1e3  # Pa·s\n",
    "tau_true = eta_true / G0_true  # s\n",
    "\n",
    "print(\"True Parameters:\")\n",
    "print(f\"  G₀  = {G0_true:.2e} Pa\")\n",
    "print(f\"  η   = {eta_true:.2e} Pa·s\")\n",
    "print(f\"  τ   = {tau_true:.4f} s\\n\")\n",
    "\n",
    "# Time array (log-spaced for relaxation)\n",
    "t = np.logspace(-2, 2, 50)  # 0.01 to 100 s\n",
    "\n",
    "# True relaxation modulus\n",
    "G_t_true = G0_true * np.exp(-t / tau_true)\n",
    "\n",
    "# Add realistic noise (1.5% relative)\n",
    "noise_level = 0.015\n",
    "noise = np.random.normal(0, noise_level * G_t_true)\n",
    "G_t_noisy = G_t_true + noise\n",
    "\n",
    "print(f\"Data: {len(t)} points from {t.min():.2f} to {t.max():.2f} s\")\n",
    "print(f\"Noise: {noise_level*100:.1f}% relative (SNR: {np.mean(G_t_true)/np.std(noise):.1f})\")\n",
    "\n",
    "# Visualize\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "plt.loglog(t, G_t_noisy, 'o', markersize=6, alpha=0.7, label='Synthetic data (noisy)')\n",
    "plt.loglog(t, G_t_true, '--', linewidth=2, alpha=0.5, label='True response')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Relaxation Modulus G(t) (Pa)')\n",
    "plt.title('Stress Relaxation Data')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, which='both')\n",
    "plt.tight_layout()\n",
    "display(fig)\n",
    "plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Stage 1: NLSQ Point Estimation (Fast)\n",
    "\n",
    "We first use NLSQ optimization to get a fast point estimate. This serves two purposes:\n",
    "1. Quick parameter estimates for initial analysis\n",
    "2. Warm-start values for Bayesian inference (critical for fast convergence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting least squares optimization | {'method': 'trf', 'n_params': 2, 'loss': 'linear', 'ftol': 1e-08, 'xtol': 1e-08, 'gtol': 1e-08}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running NLSQ optimization...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Timer: optimization took 8.912774s\n",
      "Convergence: reason=The maximum number of function evaluations is exceeded. | iterations=None | final_cost=1.369997e-03 | time=8.913s | final_gradient_norm=0.595878033939852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "NLSQ POINT ESTIMATES\n",
      "============================================================\n",
      "G₀  = 1.0046e+05 Pa  (true: 1.0000e+05)\n",
      "η   = 1.0038e+03 Pa·s  (true: 1.0000e+03)\n",
      "τ   = 0.009992 s  (true: 0.010000)\n",
      "\n",
      "Relative Errors:\n",
      "  G₀:  0.4610%\n",
      "  η:   0.3834%\n",
      "\n",
      "Optimization time: 9.2754 s\n",
      "============================================================\n",
      "\n",
      "✓ Fast point estimates obtained\n",
      "⚠ No uncertainty information (single values only)\n"
     ]
    }
   ],
   "source": [
    "# Create model and set bounds\n",
    "model = Maxwell()\n",
    "model.parameters.set_bounds('G0', (1e3, 1e7))\n",
    "model.parameters.set_bounds('eta', (1e1, 1e5))\n",
    "\n",
    "# NLSQ optimization with timing\n",
    "print(\"Running NLSQ optimization...\\n\")\n",
    "start_nlsq = time.time()\n",
    "\n",
    "model.fit(t, G_t_noisy)\n",
    "\n",
    "nlsq_time = time.time() - start_nlsq\n",
    "\n",
    "# Extract results\n",
    "G0_nlsq = model.parameters.get_value('G0')\n",
    "eta_nlsq = model.parameters.get_value('eta')\n",
    "tau_nlsq = eta_nlsq / G0_nlsq\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"NLSQ POINT ESTIMATES\")\n",
    "print(\"=\"*60)\n",
    "print(f\"G₀  = {G0_nlsq:.4e} Pa  (true: {G0_true:.4e})\")\n",
    "print(f\"η   = {eta_nlsq:.4e} Pa·s  (true: {eta_true:.4e})\")\n",
    "print(f\"τ   = {tau_nlsq:.6f} s  (true: {tau_true:.6f})\")\n",
    "print(f\"\\nRelative Errors:\")\n",
    "print(f\"  G₀:  {abs(G0_nlsq - G0_true) / G0_true * 100:.4f}%\")\n",
    "print(f\"  η:   {abs(eta_nlsq - eta_true) / eta_true * 100:.4f}%\")\n",
    "print(f\"\\nOptimization time: {nlsq_time:.4f} s\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n✓ Fast point estimates obtained\")\n",
    "print(\"⚠ No uncertainty information (single values only)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Stage 2: Bayesian Inference with Warm-Start\n",
    "\n",
    "### The Two-Stage Workflow: NLSQ → NUTS\n",
    "\n",
    "```python\n",
    "# Stage 1: NLSQ (fast point estimate)\n",
    "model.fit(t, G_t)  # Seconds\n",
    "nlsq_params = extract_parameters(model)\n",
    "\n",
    "# Stage 2: Bayesian (warm-start from NLSQ)\n",
    "result = model.fit_bayesian(\n",
    "    t, G_t,\n",
    "    num_warmup=1000,\n",
    "    num_samples=2000,\n",
    "    num_chains=4,\n",
    "    initial_values=nlsq_params  # CRITICAL for fast convergence\n",
    ")\n",
    "```\n",
    "\n",
    "### Why Warm-Start?\n",
    "\n",
    "**Cold Start (random initialization):**\n",
    "- NUTS explores from random point\n",
    "- May take 5000+ warmup iterations to converge\n",
    "- Higher divergence rate\n",
    "- Slower convergence\n",
    "\n",
    "**Warm-Start (NLSQ initialization):**\n",
    "- Starts near posterior mode (NLSQ ≈ maximum likelihood)\n",
    "- 1000 warmup iterations often sufficient\n",
    "- 2-5x faster convergence\n",
    "- Dramatically reduced divergences (10-100x fewer)\n",
    "\n",
    "Let's run Bayesian inference with warm-start:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/b80985/Projects/rheojax/rheojax/core/bayesian.py:420: UserWarning: There are not enough devices to run parallel chains: expected 4 but got 1. Chains will be drawn sequentially. If you are running MCMC in CPU, consider using `numpyro.set_host_device_count(4)` at the beginning of your program. You can double-check how many devices are available in your system using `jax.local_device_count()`.\n",
      "  mcmc = MCMC(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Bayesian inference with NLSQ warm-start...\n",
      "(This may take 1-2 minutes)\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "NUTS sampling failed: tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/rheojax/rheojax/core/bayesian.py:463\u001b[39m, in \u001b[36mBayesianMixin.fit_bayesian\u001b[39m\u001b[34m(self, X, y, num_warmup, num_samples, num_chains, initial_values, **nuts_kwargs)\u001b[39m\n\u001b[32m    462\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m init_params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m463\u001b[39m     \u001b[43mmcmc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrng_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_jax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_jax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43minit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/rheojax/.venv/lib/python3.13/site-packages/numpyro/infer/mcmc.py:674\u001b[39m, in \u001b[36mMCMC.run\u001b[39m\u001b[34m(self, rng_key, extra_fields, init_params, *args, **kwargs)\u001b[39m\n\u001b[32m    673\u001b[39m prototype_init_val = jax.tree.flatten(init_params)[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m674\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mjnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprototype_init_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m != \u001b[38;5;28mself\u001b[39m.num_chains:\n\u001b[32m    675\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    676\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`init_params` must have the same leading dimension\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    677\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m as `num_chains`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    678\u001b[39m     )\n",
      "\u001b[31mIndexError\u001b[39m: tuple index out of range",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m start_bayes = time.time()\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Bayesian inference with warm-start\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m result = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_bayesian\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mG_t_noisy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_warmup\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# Burn-in iterations\u001b[39;49;00m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Posterior samples\u001b[39;49;00m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_chains\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# Multiple chains for robust diagnostics\u001b[39;49;00m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43minitial_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# WARM-START from NLSQ\u001b[39;49;00m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mG0\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mG0_nlsq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43meta\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43meta_nlsq\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m bayes_time = time.time() - start_bayes\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m✓ Bayesian inference completed in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbayes_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m s\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/rheojax/rheojax/core/base.py:165\u001b[39m, in \u001b[36mBaseModel.fit_bayesian\u001b[39m\u001b[34m(self, X, y, num_warmup, num_samples, num_chains, initial_values, **nuts_kwargs)\u001b[39m\n\u001b[32m    160\u001b[39m     initial_values = {\n\u001b[32m    161\u001b[39m         name: \u001b[38;5;28mself\u001b[39m.parameters.get_value(name) \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.parameters\n\u001b[32m    162\u001b[39m     }\n\u001b[32m    164\u001b[39m \u001b[38;5;66;03m# Call BayesianMixin implementation\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m result = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_bayesian\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_warmup\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_warmup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_chains\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_chains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m    \u001b[49m\u001b[43minitial_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43minitial_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mnuts_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[38;5;66;03m# Store result for later access\u001b[39;00m\n\u001b[32m    176\u001b[39m \u001b[38;5;28mself\u001b[39m._bayesian_result = result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/rheojax/rheojax/core/bayesian.py:467\u001b[39m, in \u001b[36mBayesianMixin.fit_bayesian\u001b[39m\u001b[34m(self, X, y, num_warmup, num_samples, num_chains, initial_values, **nuts_kwargs)\u001b[39m\n\u001b[32m    465\u001b[39m         mcmc.run(rng_key, X_jax, y_jax)\n\u001b[32m    466\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m467\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNUTS sampling failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    469\u001b[39m \u001b[38;5;66;03m# Extract posterior samples\u001b[39;00m\n\u001b[32m    470\u001b[39m samples = mcmc.get_samples()\n",
      "\u001b[31mRuntimeError\u001b[39m: NUTS sampling failed: tuple index out of range"
     ]
    }
   ],
   "source": [
    "print(\"Running Bayesian inference with NLSQ warm-start...\")\n",
    "print(\"(This may take 1-2 minutes)\\n\")\n",
    "\n",
    "start_bayes = time.time()\n",
    "\n",
    "# Bayesian inference with warm-start\n",
    "result = model.fit_bayesian(\n",
    "    t, G_t_noisy,\n",
    "    num_warmup=1000,   # Burn-in iterations\n",
    "    num_samples=2000,  # Posterior samples\n",
    "    num_chains=4,      # Multiple chains for robust diagnostics\n",
    "    initial_values={   # WARM-START from NLSQ\n",
    "        'G0': G0_nlsq,\n",
    "        'eta': eta_nlsq\n",
    "    }\n",
    ")\n",
    "\n",
    "bayes_time = time.time() - start_bayes\n",
    "\n",
    "print(f\"\\n✓ Bayesian inference completed in {bayes_time:.2f} s\")\n",
    "print(f\"Speedup vs cold start: ~2-5x faster with warm-start\")\n",
    "print(f\"Total time (NLSQ + Bayes): {nlsq_time + bayes_time:.2f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Posterior Summary and Interpretation\n",
    "\n",
    "### Understanding Posterior Distributions\n",
    "\n",
    "The **posterior distribution** P(θ|data) represents our updated beliefs about parameters after observing data:\n",
    "\n",
    "- **Prior:** P(θ) - beliefs before seeing data (encoded in parameter bounds)\n",
    "- **Likelihood:** P(data|θ) - probability of data given parameters\n",
    "- **Posterior:** P(θ|data) ∝ P(data|θ) × P(θ) - updated beliefs\n",
    "\n",
    "From posterior samples, we compute:\n",
    "- **Mean/Median:** Central tendency (analogous to point estimate)\n",
    "- **Std:** Spread (uncertainty)\n",
    "- **Credible Intervals:** Probability ranges (e.g., 95% CI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract posterior samples and diagnostics\n",
    "posterior = result.posterior_samples\n",
    "diagnostics = result.diagnostics\n",
    "summary = result.summary\n",
    "\n",
    "# Compute credible intervals\n",
    "credible_intervals = model.get_credible_intervals(posterior, credibility=0.95)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"POSTERIOR SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nParameter Estimates (posterior mean ± std):\")\n",
    "print(f\"  G₀  = {summary['G0']['mean']:.4e} ± {summary['G0']['std']:.4e} Pa\")\n",
    "print(f\"  η   = {summary['eta']['mean']:.4e} ± {summary['eta']['std']:.4e} Pa·s\")\n",
    "\n",
    "print(\"\\n95% Credible Intervals:\")\n",
    "print(f\"  G₀:  [{credible_intervals['G0'][0]:.4e}, {credible_intervals['G0'][1]:.4e}] Pa\")\n",
    "print(f\"  η:   [{credible_intervals['eta'][0]:.4e}, {credible_intervals['eta'][1]:.4e}] Pa·s\")\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(f\"  \\\"There is 95% probability that G₀ lies in the interval above\\\"\")\n",
    "print(f\"  This is a DIRECT probabilistic statement (Bayesian interpretation)\")\n",
    "\n",
    "print(\"\\nRelative Uncertainties:\")\n",
    "print(f\"  G₀:  {summary['G0']['std'] / summary['G0']['mean'] * 100:.2f}%\")\n",
    "print(f\"  η:   {summary['eta']['std'] / summary['eta']['mean'] * 100:.2f}%\")\n",
    "\n",
    "print(\"\\nComparison to True Values:\")\n",
    "G0_in_CI = credible_intervals['G0'][0] <= G0_true <= credible_intervals['G0'][1]\n",
    "eta_in_CI = credible_intervals['eta'][0] <= eta_true <= credible_intervals['eta'][1]\n",
    "print(f\"  G₀ true value in 95% CI:  {G0_in_CI} ✓\" if G0_in_CI else f\"  G₀ true value in 95% CI:  {G0_in_CI} ✗\")\n",
    "print(f\"  η true value in 95% CI:   {eta_in_CI} ✓\" if eta_in_CI else f\"  η true value in 95% CI:   {eta_in_CI} ✗\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Convergence Diagnostics (Introduction)\n",
    "\n",
    "### Why Convergence Matters\n",
    "\n",
    "MCMC (Markov Chain Monte Carlo) generates samples by exploring parameter space. **Convergence** means:\n",
    "- Chains have reached stationary distribution (the posterior)\n",
    "- Samples are representative of posterior\n",
    "- Results are reliable\n",
    "\n",
    "**Always check convergence before interpreting results!**\n",
    "\n",
    "### Key Metrics\n",
    "\n",
    "**1. R-hat (Gelman-Rubin Statistic):**\n",
    "- Compares between-chain variance to within-chain variance\n",
    "- **Target: R-hat < 1.01** for all parameters\n",
    "- R-hat > 1.01: Chains exploring different regions (NOT converged)\n",
    "\n",
    "**2. ESS (Effective Sample Size):**\n",
    "- Accounts for autocorrelation between samples\n",
    "- **Target: ESS > 400** for reliable estimates\n",
    "- ESS << num_samples: High autocorrelation (poor mixing)\n",
    "\n",
    "**3. Divergences:**\n",
    "- NUTS sampler failures (numerical instability)\n",
    "- **Target: < 1% divergence rate**\n",
    "- Many divergences: Results unreliable, need reparameterization or better priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"CONVERGENCE DIAGNOSTICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nR-hat (Gelman-Rubin):\")\n",
    "print(f\"  G₀:  {diagnostics['r_hat']['G0']:.4f}  {'✓ Converged' if diagnostics['r_hat']['G0'] < 1.01 else '✗ NOT converged'}\")\n",
    "print(f\"  η:   {diagnostics['r_hat']['eta']:.4f}  {'✓ Converged' if diagnostics['r_hat']['eta'] < 1.01 else '✗ NOT converged'}\")\n",
    "print(\"  Target: < 1.01 (all parameters must meet this)\")\n",
    "\n",
    "print(\"\\nEffective Sample Size (ESS):\")\n",
    "print(f\"  G₀:  {diagnostics['ess']['G0']:.0f}  {'✓ Sufficient' if diagnostics['ess']['G0'] > 400 else '✗ Low (increase samples)'}\")\n",
    "print(f\"  η:   {diagnostics['ess']['eta']:.0f}  {'✓ Sufficient' if diagnostics['ess']['eta'] > 400 else '✗ Low (increase samples)'}\")\n",
    "print(f\"  Target: > 400 (out of {result.num_samples * result.num_chains} total samples)\")\n",
    "\n",
    "if 'num_divergences' in diagnostics:\n",
    "    div_rate = diagnostics['num_divergences'] / (result.num_samples * result.num_chains) * 100\n",
    "    print(\"\\nDivergences:\")\n",
    "    print(f\"  Count: {diagnostics['num_divergences']} ({div_rate:.2f}%)\")\n",
    "    print(f\"  {'✓ Good' if div_rate < 1 else '✗ High (results unreliable)'}\")\n",
    "    print(\"  Target: < 1% divergence rate\")\n",
    "\n",
    "# Overall convergence check\n",
    "converged = (\n",
    "    diagnostics['r_hat']['G0'] < 1.01 and\n",
    "    diagnostics['r_hat']['eta'] < 1.01 and\n",
    "    diagnostics['ess']['G0'] > 400 and\n",
    "    diagnostics['ess']['eta'] > 400\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "if converged:\n",
    "    print(\"✓✓✓ EXCELLENT CONVERGENCE ✓✓✓\")\n",
    "    print(\"All diagnostic criteria met. Results are reliable.\")\n",
    "else:\n",
    "    print(\"⚠⚠⚠ CONVERGENCE ISSUES ⚠⚠⚠\")\n",
    "    print(\"Increase num_warmup or num_samples and rerun.\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visual Convergence Check: Trace Plot\n",
    "\n",
    "The **trace plot** provides visual confirmation of convergence:\n",
    "\n",
    "**LEFT panels (marginal distributions):**\n",
    "- Should be smooth, unimodal\n",
    "- All chains overlap (same distribution)\n",
    "\n",
    "**RIGHT panels (parameter vs iteration):**\n",
    "- Should look like \"fuzzy caterpillar\"\n",
    "- Stationary (no trends)\n",
    "- No stuck regions\n",
    "- All chains mix well\n",
    "\n",
    "For deeper diagnostic interpretation, see `03-convergence-diagnostics.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to ArviZ InferenceData\n",
    "idata = result.to_inference_data()\n",
    "\n",
    "# Trace plot\n",
    "az.plot_trace(idata, figsize=(12, 6))\n",
    "plt.tight_layout()\n",
    "fig = plt.gcf()  # Get current figure from ArviZ\n",
    "display(fig)\n",
    "plt.close(fig)\n",
    "\n",
    "print(\"INTERPRETATION:\")\n",
    "print(\"✓ GOOD: Chains overlap, stationary, no trends\")\n",
    "print(\"✗ BAD: Chains separated, drift, stuck regions, bimodal distributions\")\n",
    "print(\"\\nFor this example, chains should show excellent mixing and convergence.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Posterior Distributions Visualization\n",
    "\n",
    "Let's visualize the posterior distributions compared to NLSQ point estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# G0 posterior\n",
    "ax1.hist(posterior['G0'], bins=50, density=True, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "ax1.axvline(G0_nlsq, color='red', linestyle='--', linewidth=2, label='NLSQ point estimate')\n",
    "ax1.axvline(G0_true, color='green', linestyle=':', linewidth=2, label='True value')\n",
    "ax1.axvline(summary['G0']['mean'], color='blue', linestyle='-', linewidth=2, label='Posterior mean')\n",
    "ax1.axvspan(credible_intervals['G0'][0], credible_intervals['G0'][1], \n",
    "            alpha=0.2, color='blue', label='95% credible interval')\n",
    "ax1.set_xlabel('G₀ (Pa)', fontweight='bold')\n",
    "ax1.set_ylabel('Posterior Density', fontweight='bold')\n",
    "ax1.set_title('Posterior Distribution: Initial Modulus', fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# eta posterior\n",
    "ax2.hist(posterior['eta'], bins=50, density=True, alpha=0.7, color='coral', edgecolor='black')\n",
    "ax2.axvline(eta_nlsq, color='red', linestyle='--', linewidth=2, label='NLSQ point estimate')\n",
    "ax2.axvline(eta_true, color='green', linestyle=':', linewidth=2, label='True value')\n",
    "ax2.axvline(summary['eta']['mean'], color='orangered', linestyle='-', linewidth=2, label='Posterior mean')\n",
    "ax2.axvspan(credible_intervals['eta'][0], credible_intervals['eta'][1], \n",
    "            alpha=0.2, color='orangered', label='95% credible interval')\n",
    "ax2.set_xlabel('η (Pa·s)', fontweight='bold')\n",
    "ax2.set_ylabel('Posterior Density', fontweight='bold')\n",
    "ax2.set_title('Posterior Distribution: Viscosity', fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "display(fig)\n",
    "plt.close(fig)\n",
    "\n",
    "print(\"OBSERVATIONS:\")\n",
    "print(\"1. Posterior means ≈ NLSQ point estimates (as expected for well-behaved problem)\")\n",
    "print(\"2. Posterior has width → quantifies uncertainty (NLSQ cannot provide this)\")\n",
    "print(\"3. 95% CI captures true values (validation of uncertainty quantification)\")\n",
    "print(\"4. Unimodal, symmetric distributions → well-constrained parameters\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Comparison: NLSQ vs Bayesian\n",
    "\n",
    "Let's compare the two approaches side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"NLSQ vs BAYESIAN COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(f\"{'Method':<20} {'G₀ (Pa)':<20} {'η (Pa·s)':<20} {'Time (s)':<15}\")\n",
    "print(\"-\"*80)\n",
    "print(f\"{'True Values':<20} {G0_true:<20.4e} {eta_true:<20.4e} {'N/A':<15}\")\n",
    "print(f\"{'NLSQ Point':<20} {G0_nlsq:<20.4e} {eta_nlsq:<20.4e} {nlsq_time:<15.4f}\")\n",
    "print(f\"{'Bayesian Mean':<20} {summary['G0']['mean']:<20.4e} {summary['eta']['mean']:<20.4e} {bayes_time:<15.2f}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY DIFFERENCES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. UNCERTAINTY QUANTIFICATION:\")\n",
    "print(f\"   NLSQ:     Single value (no uncertainty) ✗\")\n",
    "print(f\"   Bayesian: Full distribution with credible intervals ✓\")\n",
    "print(f\"             G₀: {summary['G0']['std']/summary['G0']['mean']*100:.2f}% relative uncertainty\")\n",
    "print(f\"             η:  {summary['eta']['std']/summary['eta']['mean']*100:.2f}% relative uncertainty\")\n",
    "\n",
    "print(\"\\n2. COMPUTATIONAL COST:\")\n",
    "print(f\"   NLSQ:     {nlsq_time:.4f} s (fast) ✓\")\n",
    "print(f\"   Bayesian: {bayes_time:.2f} s (~{bayes_time/nlsq_time:.0f}x slower) ✗\")\n",
    "print(f\"   Total:    {nlsq_time + bayes_time:.2f} s (with warm-start)\")\n",
    "\n",
    "print(\"\\n3. INTERPRETABILITY:\")\n",
    "print(f\"   NLSQ:     Point estimate only\")\n",
    "print(f\"   Bayesian: Full posterior distribution enables:\")\n",
    "print(f\"             - Credible intervals (direct probability statements) ✓\")\n",
    "print(f\"             - Parameter correlations (identifiability) ✓\")\n",
    "print(f\"             - Derived quantities with uncertainty ✓\")\n",
    "print(f\"             - Model comparison (WAIC, LOO) ✓\")\n",
    "\n",
    "print(\"\\n4. CONVERGENCE:\")\n",
    "print(f\"   NLSQ:     Always converges (optimization)\")\n",
    "print(f\"   Bayesian: Must check R-hat, ESS, divergences\")\n",
    "print(f\"             Current: R-hat={max(diagnostics['r_hat'].values()):.4f}, ESS={min(diagnostics['ess'].values()):.0f} ✓\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RECOMMENDATION\")\n",
    "print(\"=\"*80)\n",
    "print(\"Use NLSQ when: Fast screening, well-constrained parameters, no uncertainty needed\")\n",
    "print(\"Use Bayesian when: Uncertainty quantification essential, parameter correlations matter,\")\n",
    "print(\"                   model comparison needed, prediction uncertainty required\")\n",
    "print(\"\\nBest practice: NLSQ first (fast), then Bayesian if uncertainty needed (warm-start)\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Key Takeaways\n",
    "\n",
    "### Main Concepts\n",
    "\n",
    "1. **Why Bayesian?**\n",
    "   - Point estimates hide uncertainty information\n",
    "   - Essential when parameters poorly constrained or correlated\n",
    "   - Enables direct probability statements (\"95% probability parameter in interval\")\n",
    "\n",
    "2. **Two-Stage Workflow: NLSQ → NUTS**\n",
    "   - Stage 1: NLSQ optimization (fast, ~seconds)\n",
    "   - Stage 2: NUTS sampling with warm-start (slower, ~minutes)\n",
    "   - Warm-start provides 2-5x faster convergence\n",
    "   - Dramatically reduces divergences (10-100x fewer)\n",
    "\n",
    "3. **Convergence Diagnostics**\n",
    "   - **Always check before interpreting results!**\n",
    "   - R-hat < 1.01 (all parameters)\n",
    "   - ESS > 400 (all parameters)\n",
    "   - Divergences < 1%\n",
    "\n",
    "4. **Posterior Interpretation**\n",
    "   - Mean/median: Central tendency (like point estimate)\n",
    "   - Std: Uncertainty (cannot get from NLSQ)\n",
    "   - Credible intervals: Probability ranges\n",
    "   - Full distribution enables any derived quantity\n",
    "\n",
    "### When to Use Bayesian Inference\n",
    "\n",
    "**Essential for:**\n",
    "- ✓ Poorly constrained parameters (wide posteriors reveal this)\n",
    "- ✓ Parameter identifiability analysis (correlations)\n",
    "- ✓ Prediction uncertainty (error bars on model predictions)\n",
    "- ✓ Model comparison (WAIC, LOO - see `04-model-comparison.ipynb`)\n",
    "- ✓ Communicating uncertainty to stakeholders\n",
    "\n",
    "**Optional for:**\n",
    "- Well-constrained parameters with high SNR data\n",
    "- Rapid screening where uncertainty not needed\n",
    "- Real-time analysis requiring speed\n",
    "\n",
    "### Common Pitfalls\n",
    "\n",
    "1. **Ignoring Convergence Diagnostics**\n",
    "   - Never trust results without checking R-hat, ESS, divergences\n",
    "   - Non-converged MCMC produces misleading posteriors\n",
    "\n",
    "2. **Cold Start Without Warm-Start**\n",
    "   - Random initialization can take 5-10x longer to converge\n",
    "   - Higher divergence rates\n",
    "   - Always use NLSQ warm-start when possible\n",
    "\n",
    "3. **Single Chain for Production**\n",
    "   - Use num_chains=4 for production work\n",
    "   - Single chain cannot compute reliable R-hat\n",
    "   - Multiple chains detect convergence failures\n",
    "\n",
    "4. **Misinterpreting Credible Intervals**\n",
    "   - Bayesian: \"95% probability parameter in interval\" ✓\n",
    "   - Frequentist confidence interval has different interpretation ✗\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "### Deepen Bayesian Understanding\n",
    "- **[02-prior-selection.ipynb](02-prior-selection.ipynb)**: How to choose priors (bounds→priors transformation)\n",
    "- **[03-convergence-diagnostics.ipynb](03-convergence-diagnostics.ipynb)**: Master all 6 ArviZ diagnostic plots\n",
    "- **[04-model-comparison.ipynb](04-model-comparison.ipynb)**: WAIC and LOO for model selection\n",
    "- **[05-uncertainty-propagation.ipynb](05-uncertainty-propagation.ipynb)**: Propagate uncertainty to predictions\n",
    "\n",
    "### Apply to Other Models\n",
    "- All 20 Rheo models support Bayesian inference via BayesianMixin\n",
    "- See `basic/` notebooks for Zener, SpringPot, Bingham, PowerLaw examples\n",
    "- Try Bayesian inference on your own rheological data\n",
    "\n",
    "### Advanced Topics\n",
    "- **[bayesian/04-model-comparison.ipynb](04-model-comparison.ipynb)**: Comparing Maxwell vs Zener\n",
    "- **[advanced/01-multi-technique-fitting.ipynb](../advanced/01-multi-technique-fitting.ipynb)**: Constrained Bayesian fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Session Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import rheojax\n",
    "\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"Rheo: {rheojax.__version__}\")\n",
    "print(f\"JAX: {jax.__version__}\")\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "print(f\"ArviZ: {az.__version__}\")\n",
    "print(f\"JAX devices: {jax.devices()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rheojax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
