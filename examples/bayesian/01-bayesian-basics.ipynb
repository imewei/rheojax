{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Inference Basics: From Point Estimates to Uncertainty Quantification\n",
    "\n",
    "This notebook introduces Bayesian inference in rheological modeling, demonstrating the NLSQ\u2192NUTS two-stage workflow that combines fast optimization with comprehensive uncertainty quantification.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "After completing this notebook, you will be able to:\n",
    "- Understand when Bayesian inference is essential vs optional\n",
    "- Implement the NLSQ\u2192NUTS two-stage workflow with warm-start\n",
    "- Interpret posterior distributions and credible intervals\n",
    "- Verify convergence using R-hat and ESS diagnostics\n",
    "- Appreciate warm-start benefits (2-5x faster convergence)\n",
    "- Compare Bayesian credible intervals vs frequentist confidence intervals\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Basic understanding of Bayesian probability\n",
    "- Familiarity with Maxwell model (see `basic/01-maxwell-fitting.ipynb`)\n",
    "- Basic rheological concepts (relaxation, modulus)\n",
    "\n",
    "**Estimated Time:** 30 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction: Why Bayesian?\n",
    "\n",
    "### The Limitation of Point Estimates\n",
    "\n",
    "Traditional optimization (NLSQ, scipy.curve_fit) provides **point estimates** - single values for each parameter. However, these point estimates hide critical information:\n",
    "\n",
    "**Scenario:** Two datasets yield identical NLSQ fits (same G\u2080, same \u03b7), but:\n",
    "- Dataset A: High signal-to-noise ratio \u2192 parameters well-constrained\n",
    "- Dataset B: Low signal-to-noise ratio \u2192 parameters poorly constrained\n",
    "\n",
    "**Point estimates can't distinguish between these cases!**\n",
    "\n",
    "### Three Scenarios Where Bayesian is Essential\n",
    "\n",
    "1. **Poorly Constrained Parameters:** Wide posterior distributions reveal when data insufficient\n",
    "2. **Parameter Correlations:** Joint distributions show when parameters co-vary (identifiability issues)\n",
    "3. **Prediction Uncertainty:** Propagate parameter uncertainty to model predictions (error bars)\n",
    "\n",
    "### Bayesian vs Frequentist Interpretation\n",
    "\n",
    "**Frequentist Confidence Interval (95%):**\n",
    "- \"If we repeat experiment many times, 95% of intervals contain true value\"\n",
    "- Cannot say: \"95% probability parameter in this interval\" (frequentist philosophy)\n",
    "\n",
    "**Bayesian Credible Interval (95%):**\n",
    "- **\"95% probability parameter lies in this interval\"**\n",
    "- Direct probabilistic statement about parameter\n",
    "- More intuitive for scientific interpretation\n",
    "\n",
    "### Posterior Samples Enable Any Derived Quantity\n",
    "\n",
    "Once you have posterior samples, you can compute uncertainty for:\n",
    "- Any function of parameters (e.g., relaxation time \u03c4 = \u03b7/G\u2080)\n",
    "- Correlations between parameters\n",
    "- Quantiles, moments, or any statistical summary\n",
    "- Model predictions with uncertainty bands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable inline plotting for Jupyter notebooks\n",
    "%matplotlib inline\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "# Rheo imports\n",
    "from rheojax.models.maxwell import Maxwell\n",
    "from rheojax.core.jax_config import safe_import_jax\n",
    "\n",
    "# ArviZ for Bayesian diagnostics\n",
    "import arviz as az\n",
    "\n",
    "# Safe JAX import\n",
    "jax, jnp = safe_import_jax()\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Plotting configuration\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"\u2713 Imports successful\")\n",
    "print(f\"JAX float64 enabled: {jax.config.jax_default_dtype_bits == 64}\")\n",
    "\n",
    "# Suppress matplotlib inline backend warning\n",
    "# This warning is harmless - plots display correctly with %matplotlib inline\n",
    "warnings.filterwarnings('ignore', message='.*non-interactive.*')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Synthetic Relaxation Data\n",
    "\n",
    "We create Maxwell relaxation data with known parameters to validate the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True parameters\n",
    "G0_true = 1e5  # Pa\n",
    "eta_true = 1e3  # Pa\u00b7s\n",
    "tau_true = eta_true / G0_true  # s\n",
    "\n",
    "print(\"True Parameters:\")\n",
    "print(f\"  G\u2080  = {G0_true:.2e} Pa\")\n",
    "print(f\"  \u03b7   = {eta_true:.2e} Pa\u00b7s\")\n",
    "print(f\"  \u03c4   = {tau_true:.4f} s\\n\")\n",
    "\n",
    "# Time array (log-spaced for relaxation)\n",
    "t = np.logspace(-2, 2, 50)  # 0.01 to 100 s\n",
    "\n",
    "# True relaxation modulus\n",
    "G_t_true = G0_true * np.exp(-t / tau_true)\n",
    "\n",
    "# Add realistic noise (1.5% relative)\n",
    "noise_level = 0.015\n",
    "noise = np.random.normal(0, noise_level * G_t_true)\n",
    "G_t_noisy = G_t_true + noise\n",
    "\n",
    "print(f\"Data: {len(t)} points from {t.min():.2f} to {t.max():.2f} s\")\n",
    "print(f\"Noise: {noise_level*100:.1f}% relative (SNR: {np.mean(G_t_true)/np.std(noise):.1f})\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.loglog(t, G_t_noisy, 'o', markersize=6, alpha=0.7, label='Synthetic data (noisy)')\n",
    "plt.loglog(t, G_t_true, '--', linewidth=2, alpha=0.5, label='True response')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Relaxation Modulus G(t) (Pa)')\n",
    "plt.title('Stress Relaxation Data')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, which='both')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Stage 1: NLSQ Point Estimation (Fast)\n",
    "\n",
    "We first use NLSQ optimization to get a fast point estimate. This serves two purposes:\n",
    "1. Quick parameter estimates for initial analysis\n",
    "2. Warm-start values for Bayesian inference (critical for fast convergence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model and set bounds\n",
    "model = Maxwell()\n",
    "model.parameters.set_bounds('G0', (1e3, 1e7))\n",
    "model.parameters.set_bounds('eta', (1e1, 1e5))\n",
    "\n",
    "# NLSQ optimization with timing\n",
    "print(\"Running NLSQ optimization...\\n\")\n",
    "start_nlsq = time.time()\n",
    "\n",
    "model.fit(t, G_t_noisy)\n",
    "\n",
    "nlsq_time = time.time() - start_nlsq\n",
    "\n",
    "# Extract results\n",
    "G0_nlsq = model.parameters.get_value('G0')\n",
    "eta_nlsq = model.parameters.get_value('eta')\n",
    "tau_nlsq = eta_nlsq / G0_nlsq\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"NLSQ POINT ESTIMATES\")\n",
    "print(\"=\"*60)\n",
    "print(f\"G\u2080  = {G0_nlsq:.4e} Pa  (true: {G0_true:.4e})\")\n",
    "print(f\"\u03b7   = {eta_nlsq:.4e} Pa\u00b7s  (true: {eta_true:.4e})\")\n",
    "print(f\"\u03c4   = {tau_nlsq:.6f} s  (true: {tau_true:.6f})\")\n",
    "print(f\"\\nRelative Errors:\")\n",
    "print(f\"  G\u2080:  {abs(G0_nlsq - G0_true) / G0_true * 100:.4f}%\")\n",
    "print(f\"  \u03b7:   {abs(eta_nlsq - eta_true) / eta_true * 100:.4f}%\")\n",
    "print(f\"\\nOptimization time: {nlsq_time:.4f} s\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n\u2713 Fast point estimates obtained\")\n",
    "print(\"\u26a0 No uncertainty information (single values only)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Stage 2: Bayesian Inference with Warm-Start\n",
    "\n",
    "### The Two-Stage Workflow: NLSQ \u2192 NUTS\n",
    "\n",
    "```python\n",
    "# Stage 1: NLSQ (fast point estimate)\n",
    "model.fit(t, G_t)  # Seconds\n",
    "nlsq_params = extract_parameters(model)\n",
    "\n",
    "# Stage 2: Bayesian (warm-start from NLSQ)\n",
    "result = model.fit_bayesian(\n",
    "    t, G_t,\n",
    "    num_warmup=1000,\n",
    "    num_samples=2000,\n",
    "    num_chains=4,\n",
    "    initial_values=nlsq_params  # CRITICAL for fast convergence\n",
    ")\n",
    "```\n",
    "\n",
    "### Why Warm-Start?\n",
    "\n",
    "**Cold Start (random initialization):**\n",
    "- NUTS explores from random point\n",
    "- May take 5000+ warmup iterations to converge\n",
    "- Higher divergence rate\n",
    "- Slower convergence\n",
    "\n",
    "**Warm-Start (NLSQ initialization):**\n",
    "- Starts near posterior mode (NLSQ \u2248 maximum likelihood)\n",
    "- 1000 warmup iterations often sufficient\n",
    "- 2-5x faster convergence\n",
    "- Dramatically reduced divergences (10-100x fewer)\n",
    "\n",
    "Let's run Bayesian inference with warm-start:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running Bayesian inference with NLSQ warm-start...\")\n",
    "print(\"(This may take 1-2 minutes)\\n\")\n",
    "\n",
    "start_bayes = time.time()\n",
    "\n",
    "# Bayesian inference with warm-start\n",
    "result = model.fit_bayesian(\n",
    "    t, G_t_noisy,\n",
    "    num_warmup=1000,   # Burn-in iterations\n",
    "    num_samples=2000,  # Posterior samples\n",
    "    num_chains=4,      # Multiple chains for robust diagnostics\n",
    "    initial_values={   # WARM-START from NLSQ\n",
    "        'G0': G0_nlsq,\n",
    "        'eta': eta_nlsq\n",
    "    }\n",
    ")\n",
    "\n",
    "bayes_time = time.time() - start_bayes\n",
    "\n",
    "print(f\"\\n\u2713 Bayesian inference completed in {bayes_time:.2f} s\")\n",
    "print(f\"Speedup vs cold start: ~2-5x faster with warm-start\")\n",
    "print(f\"Total time (NLSQ + Bayes): {nlsq_time + bayes_time:.2f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Posterior Summary and Interpretation\n",
    "\n",
    "### Understanding Posterior Distributions\n",
    "\n",
    "The **posterior distribution** P(\u03b8|data) represents our updated beliefs about parameters after observing data:\n",
    "\n",
    "- **Prior:** P(\u03b8) - beliefs before seeing data (encoded in parameter bounds)\n",
    "- **Likelihood:** P(data|\u03b8) - probability of data given parameters\n",
    "- **Posterior:** P(\u03b8|data) \u221d P(data|\u03b8) \u00d7 P(\u03b8) - updated beliefs\n",
    "\n",
    "From posterior samples, we compute:\n",
    "- **Mean/Median:** Central tendency (analogous to point estimate)\n",
    "- **Std:** Spread (uncertainty)\n",
    "- **Credible Intervals:** Probability ranges (e.g., 95% CI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract posterior samples and diagnostics\n",
    "posterior = result.posterior_samples\n",
    "diagnostics = result.diagnostics\n",
    "summary = result.summary\n",
    "\n",
    "# Compute credible intervals\n",
    "credible_intervals = model.get_credible_intervals(posterior, credibility=0.95)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"POSTERIOR SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nParameter Estimates (posterior mean \u00b1 std):\")\n",
    "print(f\"  G\u2080  = {summary['G0']['mean']:.4e} \u00b1 {summary['G0']['std']:.4e} Pa\")\n",
    "print(f\"  \u03b7   = {summary['eta']['mean']:.4e} \u00b1 {summary['eta']['std']:.4e} Pa\u00b7s\")\n",
    "\n",
    "print(\"\\n95% Credible Intervals:\")\n",
    "print(f\"  G\u2080:  [{credible_intervals['G0'][0]:.4e}, {credible_intervals['G0'][1]:.4e}] Pa\")\n",
    "print(f\"  \u03b7:   [{credible_intervals['eta'][0]:.4e}, {credible_intervals['eta'][1]:.4e}] Pa\u00b7s\")\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(f\"  \\\"There is 95% probability that G\u2080 lies in the interval above\\\"\")\n",
    "print(f\"  This is a DIRECT probabilistic statement (Bayesian interpretation)\")\n",
    "\n",
    "print(\"\\nRelative Uncertainties:\")\n",
    "print(f\"  G\u2080:  {summary['G0']['std'] / summary['G0']['mean'] * 100:.2f}%\")\n",
    "print(f\"  \u03b7:   {summary['eta']['std'] / summary['eta']['mean'] * 100:.2f}%\")\n",
    "\n",
    "print(\"\\nComparison to True Values:\")\n",
    "G0_in_CI = credible_intervals['G0'][0] <= G0_true <= credible_intervals['G0'][1]\n",
    "eta_in_CI = credible_intervals['eta'][0] <= eta_true <= credible_intervals['eta'][1]\n",
    "print(f\"  G\u2080 true value in 95% CI:  {G0_in_CI} \u2713\" if G0_in_CI else f\"  G\u2080 true value in 95% CI:  {G0_in_CI} \u2717\")\n",
    "print(f\"  \u03b7 true value in 95% CI:   {eta_in_CI} \u2713\" if eta_in_CI else f\"  \u03b7 true value in 95% CI:   {eta_in_CI} \u2717\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Convergence Diagnostics (Introduction)\n",
    "\n",
    "### Why Convergence Matters\n",
    "\n",
    "MCMC (Markov Chain Monte Carlo) generates samples by exploring parameter space. **Convergence** means:\n",
    "- Chains have reached stationary distribution (the posterior)\n",
    "- Samples are representative of posterior\n",
    "- Results are reliable\n",
    "\n",
    "**Always check convergence before interpreting results!**\n",
    "\n",
    "### Key Metrics\n",
    "\n",
    "**1. R-hat (Gelman-Rubin Statistic):**\n",
    "- Compares between-chain variance to within-chain variance\n",
    "- **Target: R-hat < 1.01** for all parameters\n",
    "- R-hat > 1.01: Chains exploring different regions (NOT converged)\n",
    "\n",
    "**2. ESS (Effective Sample Size):**\n",
    "- Accounts for autocorrelation between samples\n",
    "- **Target: ESS > 400** for reliable estimates\n",
    "- ESS << num_samples: High autocorrelation (poor mixing)\n",
    "\n",
    "**3. Divergences:**\n",
    "- NUTS sampler failures (numerical instability)\n",
    "- **Target: < 1% divergence rate**\n",
    "- Many divergences: Results unreliable, need reparameterization or better priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"CONVERGENCE DIAGNOSTICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nR-hat (Gelman-Rubin):\")\n",
    "print(f\"  G\u2080:  {diagnostics['r_hat']['G0']:.4f}  {'\u2713 Converged' if diagnostics['r_hat']['G0'] < 1.01 else '\u2717 NOT converged'}\")\n",
    "print(f\"  \u03b7:   {diagnostics['r_hat']['eta']:.4f}  {'\u2713 Converged' if diagnostics['r_hat']['eta'] < 1.01 else '\u2717 NOT converged'}\")\n",
    "print(\"  Target: < 1.01 (all parameters must meet this)\")\n",
    "\n",
    "print(\"\\nEffective Sample Size (ESS):\")\n",
    "print(f\"  G\u2080:  {diagnostics['ess']['G0']:.0f}  {'\u2713 Sufficient' if diagnostics['ess']['G0'] > 400 else '\u2717 Low (increase samples)'}\")\n",
    "print(f\"  \u03b7:   {diagnostics['ess']['eta']:.0f}  {'\u2713 Sufficient' if diagnostics['ess']['eta'] > 400 else '\u2717 Low (increase samples)'}\")\n",
    "print(f\"  Target: > 400 (out of {result.num_samples * result.num_chains} total samples)\")\n",
    "\n",
    "if 'num_divergences' in diagnostics:\n",
    "    div_rate = diagnostics['num_divergences'] / (result.num_samples * result.num_chains) * 100\n",
    "    print(\"\\nDivergences:\")\n",
    "    print(f\"  Count: {diagnostics['num_divergences']} ({div_rate:.2f}%)\")\n",
    "    print(f\"  {'\u2713 Good' if div_rate < 1 else '\u2717 High (results unreliable)'}\")\n",
    "    print(\"  Target: < 1% divergence rate\")\n",
    "\n",
    "# Overall convergence check\n",
    "converged = (\n",
    "    diagnostics['r_hat']['G0'] < 1.01 and\n",
    "    diagnostics['r_hat']['eta'] < 1.01 and\n",
    "    diagnostics['ess']['G0'] > 400 and\n",
    "    diagnostics['ess']['eta'] > 400\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "if converged:\n",
    "    print(\"\u2713\u2713\u2713 EXCELLENT CONVERGENCE \u2713\u2713\u2713\")\n",
    "    print(\"All diagnostic criteria met. Results are reliable.\")\n",
    "else:\n",
    "    print(\"\u26a0\u26a0\u26a0 CONVERGENCE ISSUES \u26a0\u26a0\u26a0\")\n",
    "    print(\"Increase num_warmup or num_samples and rerun.\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visual Convergence Check: Trace Plot\n",
    "\n",
    "The **trace plot** provides visual confirmation of convergence:\n",
    "\n",
    "**LEFT panels (marginal distributions):**\n",
    "- Should be smooth, unimodal\n",
    "- All chains overlap (same distribution)\n",
    "\n",
    "**RIGHT panels (parameter vs iteration):**\n",
    "- Should look like \"fuzzy caterpillar\"\n",
    "- Stationary (no trends)\n",
    "- No stuck regions\n",
    "- All chains mix well\n",
    "\n",
    "For deeper diagnostic interpretation, see `03-convergence-diagnostics.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to ArviZ InferenceData\n",
    "idata = result.to_inference_data()\n",
    "\n",
    "# Trace plot\n",
    "az.plot_trace(idata, figsize=(12, 6))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"INTERPRETATION:\")\n",
    "print(\"\u2713 GOOD: Chains overlap, stationary, no trends\")\n",
    "print(\"\u2717 BAD: Chains separated, drift, stuck regions, bimodal distributions\")\n",
    "print(\"\\nFor this example, chains should show excellent mixing and convergence.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Posterior Distributions Visualization\n",
    "\n",
    "Let's visualize the posterior distributions compared to NLSQ point estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# G0 posterior\n",
    "ax1.hist(posterior['G0'], bins=50, density=True, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "ax1.axvline(G0_nlsq, color='red', linestyle='--', linewidth=2, label='NLSQ point estimate')\n",
    "ax1.axvline(G0_true, color='green', linestyle=':', linewidth=2, label='True value')\n",
    "ax1.axvline(summary['G0']['mean'], color='blue', linestyle='-', linewidth=2, label='Posterior mean')\n",
    "ax1.axvspan(credible_intervals['G0'][0], credible_intervals['G0'][1], \n",
    "            alpha=0.2, color='blue', label='95% credible interval')\n",
    "ax1.set_xlabel('G\u2080 (Pa)', fontweight='bold')\n",
    "ax1.set_ylabel('Posterior Density', fontweight='bold')\n",
    "ax1.set_title('Posterior Distribution: Initial Modulus', fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# eta posterior\n",
    "ax2.hist(posterior['eta'], bins=50, density=True, alpha=0.7, color='coral', edgecolor='black')\n",
    "ax2.axvline(eta_nlsq, color='red', linestyle='--', linewidth=2, label='NLSQ point estimate')\n",
    "ax2.axvline(eta_true, color='green', linestyle=':', linewidth=2, label='True value')\n",
    "ax2.axvline(summary['eta']['mean'], color='orangered', linestyle='-', linewidth=2, label='Posterior mean')\n",
    "ax2.axvspan(credible_intervals['eta'][0], credible_intervals['eta'][1], \n",
    "            alpha=0.2, color='orangered', label='95% credible interval')\n",
    "ax2.set_xlabel('\u03b7 (Pa\u00b7s)', fontweight='bold')\n",
    "ax2.set_ylabel('Posterior Density', fontweight='bold')\n",
    "ax2.set_title('Posterior Distribution: Viscosity', fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"OBSERVATIONS:\")\n",
    "print(\"1. Posterior means \u2248 NLSQ point estimates (as expected for well-behaved problem)\")\n",
    "print(\"2. Posterior has width \u2192 quantifies uncertainty (NLSQ cannot provide this)\")\n",
    "print(\"3. 95% CI captures true values (validation of uncertainty quantification)\")\n",
    "print(\"4. Unimodal, symmetric distributions \u2192 well-constrained parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Comparison: NLSQ vs Bayesian\n",
    "\n",
    "Let's compare the two approaches side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"NLSQ vs BAYESIAN COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(f\"{'Method':<20} {'G\u2080 (Pa)':<20} {'\u03b7 (Pa\u00b7s)':<20} {'Time (s)':<15}\")\n",
    "print(\"-\"*80)\n",
    "print(f\"{'True Values':<20} {G0_true:<20.4e} {eta_true:<20.4e} {'N/A':<15}\")\n",
    "print(f\"{'NLSQ Point':<20} {G0_nlsq:<20.4e} {eta_nlsq:<20.4e} {nlsq_time:<15.4f}\")\n",
    "print(f\"{'Bayesian Mean':<20} {summary['G0']['mean']:<20.4e} {summary['eta']['mean']:<20.4e} {bayes_time:<15.2f}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY DIFFERENCES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. UNCERTAINTY QUANTIFICATION:\")\n",
    "print(f\"   NLSQ:     Single value (no uncertainty) \u2717\")\n",
    "print(f\"   Bayesian: Full distribution with credible intervals \u2713\")\n",
    "print(f\"             G\u2080: {summary['G0']['std']/summary['G0']['mean']*100:.2f}% relative uncertainty\")\n",
    "print(f\"             \u03b7:  {summary['eta']['std']/summary['eta']['mean']*100:.2f}% relative uncertainty\")\n",
    "\n",
    "print(\"\\n2. COMPUTATIONAL COST:\")\n",
    "print(f\"   NLSQ:     {nlsq_time:.4f} s (fast) \u2713\")\n",
    "print(f\"   Bayesian: {bayes_time:.2f} s (~{bayes_time/nlsq_time:.0f}x slower) \u2717\")\n",
    "print(f\"   Total:    {nlsq_time + bayes_time:.2f} s (with warm-start)\")\n",
    "\n",
    "print(\"\\n3. INTERPRETABILITY:\")\n",
    "print(f\"   NLSQ:     Point estimate only\")\n",
    "print(f\"   Bayesian: Full posterior distribution enables:\")\n",
    "print(f\"             - Credible intervals (direct probability statements) \u2713\")\n",
    "print(f\"             - Parameter correlations (identifiability) \u2713\")\n",
    "print(f\"             - Derived quantities with uncertainty \u2713\")\n",
    "print(f\"             - Model comparison (WAIC, LOO) \u2713\")\n",
    "\n",
    "print(\"\\n4. CONVERGENCE:\")\n",
    "print(f\"   NLSQ:     Always converges (optimization)\")\n",
    "print(f\"   Bayesian: Must check R-hat, ESS, divergences\")\n",
    "print(f\"             Current: R-hat={max(diagnostics['r_hat'].values()):.4f}, ESS={min(diagnostics['ess'].values()):.0f} \u2713\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RECOMMENDATION\")\n",
    "print(\"=\"*80)\n",
    "print(\"Use NLSQ when: Fast screening, well-constrained parameters, no uncertainty needed\")\n",
    "print(\"Use Bayesian when: Uncertainty quantification essential, parameter correlations matter,\")\n",
    "print(\"                   model comparison needed, prediction uncertainty required\")\n",
    "print(\"\\nBest practice: NLSQ first (fast), then Bayesian if uncertainty needed (warm-start)\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Key Takeaways\n",
    "\n",
    "### Main Concepts\n",
    "\n",
    "1. **Why Bayesian?**\n",
    "   - Point estimates hide uncertainty information\n",
    "   - Essential when parameters poorly constrained or correlated\n",
    "   - Enables direct probability statements (\"95% probability parameter in interval\")\n",
    "\n",
    "2. **Two-Stage Workflow: NLSQ \u2192 NUTS**\n",
    "   - Stage 1: NLSQ optimization (fast, ~seconds)\n",
    "   - Stage 2: NUTS sampling with warm-start (slower, ~minutes)\n",
    "   - Warm-start provides 2-5x faster convergence\n",
    "   - Dramatically reduces divergences (10-100x fewer)\n",
    "\n",
    "3. **Convergence Diagnostics**\n",
    "   - **Always check before interpreting results!**\n",
    "   - R-hat < 1.01 (all parameters)\n",
    "   - ESS > 400 (all parameters)\n",
    "   - Divergences < 1%\n",
    "\n",
    "4. **Posterior Interpretation**\n",
    "   - Mean/median: Central tendency (like point estimate)\n",
    "   - Std: Uncertainty (cannot get from NLSQ)\n",
    "   - Credible intervals: Probability ranges\n",
    "   - Full distribution enables any derived quantity\n",
    "\n",
    "### When to Use Bayesian Inference\n",
    "\n",
    "**Essential for:**\n",
    "- \u2713 Poorly constrained parameters (wide posteriors reveal this)\n",
    "- \u2713 Parameter identifiability analysis (correlations)\n",
    "- \u2713 Prediction uncertainty (error bars on model predictions)\n",
    "- \u2713 Model comparison (WAIC, LOO - see `04-model-comparison.ipynb`)\n",
    "- \u2713 Communicating uncertainty to stakeholders\n",
    "\n",
    "**Optional for:**\n",
    "- Well-constrained parameters with high SNR data\n",
    "- Rapid screening where uncertainty not needed\n",
    "- Real-time analysis requiring speed\n",
    "\n",
    "### Common Pitfalls\n",
    "\n",
    "1. **Ignoring Convergence Diagnostics**\n",
    "   - Never trust results without checking R-hat, ESS, divergences\n",
    "   - Non-converged MCMC produces misleading posteriors\n",
    "\n",
    "2. **Cold Start Without Warm-Start**\n",
    "   - Random initialization can take 5-10x longer to converge\n",
    "   - Higher divergence rates\n",
    "   - Always use NLSQ warm-start when possible\n",
    "\n",
    "3. **Single Chain for Production**\n",
    "   - Use num_chains=4 for production work\n",
    "   - Single chain cannot compute reliable R-hat\n",
    "   - Multiple chains detect convergence failures\n",
    "\n",
    "4. **Misinterpreting Credible Intervals**\n",
    "   - Bayesian: \"95% probability parameter in interval\" \u2713\n",
    "   - Frequentist confidence interval has different interpretation \u2717\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "### Deepen Bayesian Understanding\n",
    "- **[02-prior-selection.ipynb](02-prior-selection.ipynb)**: How to choose priors (bounds\u2192priors transformation)\n",
    "- **[03-convergence-diagnostics.ipynb](03-convergence-diagnostics.ipynb)**: Master all 6 ArviZ diagnostic plots\n",
    "- **[04-model-comparison.ipynb](04-model-comparison.ipynb)**: WAIC and LOO for model selection\n",
    "- **[05-uncertainty-propagation.ipynb](05-uncertainty-propagation.ipynb)**: Propagate uncertainty to predictions\n",
    "\n",
    "### Apply to Other Models\n",
    "- All 20 Rheo models support Bayesian inference via BayesianMixin\n",
    "- See `basic/` notebooks for Zener, SpringPot, Bingham, PowerLaw examples\n",
    "- Try Bayesian inference on your own rheological data\n",
    "\n",
    "### Advanced Topics\n",
    "- **[bayesian/04-model-comparison.ipynb](04-model-comparison.ipynb)**: Comparing Maxwell vs Zener\n",
    "- **[advanced/01-multi-technique-fitting.ipynb](../advanced/01-multi-technique-fitting.ipynb)**: Constrained Bayesian fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Session Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import rheojax\n",
    "\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"Rheo: {rheojax.__version__}\")\n",
    "print(f\"JAX: {jax.__version__}\")\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "print(f\"ArviZ: {az.__version__}\")\n",
    "print(f\"JAX devices: {jax.devices()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}