{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Bayesian Inference Basics: From Point Estimates to Uncertainty Quantification\n\n> **Handbook:** See [Bayesian Inference](../../docs/source/user_guide/03_advanced_topics/bayesian_inference.rst) for theoretical background and [Bayesian Workflow](../../docs/source/_includes/bayesian_workflow.rst) for best practices.\n\nThis notebook introduces Bayesian inference in rheological modeling, demonstrating the NLSQ→NUTS two-stage workflow that combines fast optimization with comprehensive uncertainty quantification.\n\n## Learning Objectives\n\nAfter completing this notebook, you will be able to:\n- Understand when Bayesian inference is essential vs optional\n- Implement the NLSQ→NUTS two-stage workflow with warm-start\n- Interpret posterior distributions and credible intervals\n- Verify convergence using R-hat and ESS diagnostics\n- Appreciate warm-start benefits (2-5x faster convergence)\n- Compare Bayesian credible intervals vs frequentist confidence intervals\n\n## Prerequisites\n\n- Basic understanding of Bayesian probability\n- Familiarity with Maxwell model (see `basic/01-maxwell-fitting.ipynb`)\n- Basic rheological concepts (relaxation, modulus)\n\n**Estimated Time:** 30 minutes"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T21:16:25.039994Z",
     "iopub.status.busy": "2026-02-09T21:16:25.039882Z",
     "iopub.status.idle": "2026-02-09T21:16:25.043715Z",
     "shell.execute_reply": "2026-02-09T21:16:25.043234Z"
    }
   },
   "outputs": [],
   "source": [
    "# Google Colab Setup - Run this cell first!\n",
    "# Skip if running locally with rheojax already installed\n",
    "\n",
    "import sys\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Install rheojax and dependencies\n",
    "    !pip install -q rheojax\n",
    "    \n",
    "    # Colab uses float32 by default - we need float64 for numerical stability\n",
    "    # This MUST be set before importing JAX\n",
    "    import os\n",
    "    os.environ['JAX_ENABLE_X64'] = 'true'\n",
    "    \n",
    "    print(\"✓ RheoJAX installed successfully!\")\n",
    "    print(\"✓ Float64 precision enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction: Why Bayesian?\n",
    "\n",
    "### The Limitation of Point Estimates\n",
    "\n",
    "Traditional optimization (NLSQ, scipy.curve_fit) provides **point estimates** - single values for each parameter. However, these point estimates hide critical information:\n",
    "\n",
    "**Scenario:** Two datasets yield identical NLSQ fits (same G₀, same η), but:\n",
    "- Dataset A: High signal-to-noise ratio → parameters well-constrained\n",
    "- Dataset B: Low signal-to-noise ratio → parameters poorly constrained\n",
    "\n",
    "**Point estimates can't distinguish between these cases!**\n",
    "\n",
    "### Three Scenarios Where Bayesian is Essential\n",
    "\n",
    "1. **Poorly Constrained Parameters:** Wide posterior distributions reveal when data insufficient\n",
    "2. **Parameter Correlations:** Joint distributions show when parameters co-vary (identifiability issues)\n",
    "3. **Prediction Uncertainty:** Propagate parameter uncertainty to model predictions (error bars)\n",
    "\n",
    "### Bayesian vs Frequentist Interpretation\n",
    "\n",
    "**Frequentist Confidence Interval (95%):**\n",
    "- \"If we repeat experiment many times, 95% of intervals contain true value\"\n",
    "- Cannot say: \"95% probability parameter in this interval\" (frequentist philosophy)\n",
    "\n",
    "**Bayesian Credible Interval (95%):**\n",
    "- **\"95% probability parameter lies in this interval\"**\n",
    "- Direct probabilistic statement about parameter\n",
    "- More intuitive for scientific interpretation\n",
    "\n",
    "### Posterior Samples Enable Any Derived Quantity\n",
    "\n",
    "Once you have posterior samples, you can compute uncertainty for:\n",
    "- Any function of parameters (e.g., relaxation time τ = η/G₀)\n",
    "- Correlations between parameters\n",
    "- Quantiles, moments, or any statistical summary\n",
    "- Model predictions with uncertainty bands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T21:16:25.045221Z",
     "iopub.status.busy": "2026-02-09T21:16:25.045121Z",
     "iopub.status.idle": "2026-02-09T21:16:26.600878Z",
     "shell.execute_reply": "2026-02-09T21:16:26.600320Z"
    }
   },
   "outputs": [],
   "source": [
    "# Configure matplotlib for inline plotting in VS Code/Jupyter\n",
    "# MUST come before importing matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# ArviZ for Bayesian diagnostics\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "\n",
    "from rheojax.core.jax_config import safe_import_jax\n",
    "\n",
    "# RheoJAX imports\n",
    "from rheojax.models import Maxwell\n",
    "\n",
    "# Safe JAX import\n",
    "jax, jnp = safe_import_jax()\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Plotting configuration\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"✓ Imports successful\")\n",
    "print(f\"JAX float64 enabled: {jnp.array([1.0]).dtype == jnp.float64}\")\n",
    "\n",
    "# Suppress matplotlib inline backend warning\n",
    "# This warning is harmless - plots display correctly with %matplotlib inline\n",
    "warnings.filterwarnings('ignore', message='.*non-interactive.*')\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, os.path.dirname(os.path.abspath(\"\")))\n",
    "from utils.plotting_utils import (\n",
    "    display_arviz_diagnostics,\n",
    "    plot_nlsq_fit,\n",
    "    plot_posterior_predictive,\n",
    ")\n",
    "\n",
    "FAST_MODE = os.environ.get(\"FAST_MODE\", \"1\") == \"1\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Synthetic Relaxation Data\n",
    "\n",
    "We create Maxwell relaxation data with known parameters to validate the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T21:16:26.602454Z",
     "iopub.status.busy": "2026-02-09T21:16:26.602257Z",
     "iopub.status.idle": "2026-02-09T21:16:26.863130Z",
     "shell.execute_reply": "2026-02-09T21:16:26.862529Z"
    }
   },
   "outputs": [],
   "source": [
    "# True parameters\n",
    "G0_true = 1e5  # Pa\n",
    "eta_true = 1e3  # Pa·s\n",
    "tau_true = eta_true / G0_true  # s\n",
    "\n",
    "print(\"True Parameters:\")\n",
    "print(f\"  G₀  = {G0_true:.2e} Pa\")\n",
    "print(f\"  η   = {eta_true:.2e} Pa·s\")\n",
    "print(f\"  τ   = {tau_true:.4f} s\\n\")\n",
    "\n",
    "# Time array (log-spaced for relaxation)\n",
    "t = np.logspace(-2, 2, 50)  # 0.01 to 100 s\n",
    "\n",
    "# True relaxation modulus\n",
    "G_t_true = G0_true * np.exp(-t / tau_true)\n",
    "\n",
    "# Add realistic noise (1.5% relative)\n",
    "noise_level = 0.015\n",
    "noise = np.random.normal(0, noise_level * G_t_true)\n",
    "G_t_noisy = G_t_true + noise\n",
    "\n",
    "print(f\"Data: {len(t)} points from {t.min():.2f} to {t.max():.2f} s\")\n",
    "print(f\"Noise: {noise_level*100:.1f}% relative (SNR: {np.mean(G_t_true)/np.std(noise):.1f})\")\n",
    "\n",
    "# Visualize\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "plt.loglog(t, G_t_noisy, 'o', markersize=6, alpha=0.7, label='Synthetic data (noisy)')\n",
    "plt.loglog(t, G_t_true, '--', linewidth=2, alpha=0.5, label='True response')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Relaxation Modulus G(t) (Pa)')\n",
    "plt.title('Stress Relaxation Data')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, which='both')\n",
    "plt.tight_layout()\n",
    "display(fig)\n",
    "plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Stage 1: NLSQ Point Estimation (Fast)\n",
    "\n",
    "We first use NLSQ optimization to get a fast point estimate. This serves two purposes:\n",
    "1. Quick parameter estimates for initial analysis\n",
    "2. Warm-start values for Bayesian inference (critical for fast convergence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T21:16:26.864663Z",
     "iopub.status.busy": "2026-02-09T21:16:26.864546Z",
     "iopub.status.idle": "2026-02-09T21:16:28.172282Z",
     "shell.execute_reply": "2026-02-09T21:16:28.171724Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create model and set bounds\n",
    "model = Maxwell()\n",
    "model.parameters.set_bounds('G0', (1e3, 1e7))\n",
    "model.parameters.set_bounds('eta', (1e1, 1e5))\n",
    "\n",
    "# NLSQ optimization with timing\n",
    "print(\"Running NLSQ optimization...\\n\")\n",
    "start_nlsq = time.time()\n",
    "\n",
    "model.fit(t, G_t_noisy)\n",
    "\n",
    "nlsq_time = time.time() - start_nlsq\n",
    "\n",
    "# Extract results\n",
    "G0_nlsq = model.parameters.get_value('G0')\n",
    "eta_nlsq = model.parameters.get_value('eta')\n",
    "tau_nlsq = eta_nlsq / G0_nlsq\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"NLSQ POINT ESTIMATES\")\n",
    "print(\"=\"*60)\n",
    "print(f\"G₀  = {G0_nlsq:.4e} Pa  (true: {G0_true:.4e})\")\n",
    "print(f\"η   = {eta_nlsq:.4e} Pa·s  (true: {eta_true:.4e})\")\n",
    "print(f\"τ   = {tau_nlsq:.6f} s  (true: {tau_true:.6f})\")\n",
    "print(f\"\\nRelative Errors:\")\n",
    "print(f\"  G₀:  {abs(G0_nlsq - G0_true) / G0_true * 100:.4f}%\")\n",
    "print(f\"  η:   {abs(eta_nlsq - eta_true) / eta_true * 100:.4f}%\")\n",
    "print(f\"\\nOptimization time: {nlsq_time:.4f} s\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n✓ Fast point estimates obtained\")\n",
    "print(\"⚠ No uncertainty information (single values only)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Stage 2: Bayesian Inference with Warm-Start\n",
    "\n",
    "### The Two-Stage Workflow: NLSQ → NUTS\n",
    "\n",
    "```python\n",
    "# Stage 1: NLSQ (fast point estimate)\n",
    "model.fit(t, G_t)  # Seconds\n",
    "nlsq_params = extract_parameters(model)\n",
    "\n",
    "# Stage 2: Bayesian (warm-start from NLSQ)\n",
    "result = model.fit_bayesian(\n",
    "    t, G_t,\n",
    "    num_warmup=1000,\n",
    "    num_samples=2000,\n",
    "    num_chains=4,\n",
    "    initial_values=nlsq_params  # CRITICAL for fast convergence\n",
    ")\n",
    "```\n",
    "\n",
    "### Why Warm-Start?\n",
    "\n",
    "**Cold Start (random initialization):**\n",
    "- NUTS explores from random point\n",
    "- May take 5000+ warmup iterations to converge\n",
    "- Higher divergence rate\n",
    "- Slower convergence\n",
    "\n",
    "**Warm-Start (NLSQ initialization):**\n",
    "- Starts near posterior mode (NLSQ ≈ maximum likelihood)\n",
    "- 1000 warmup iterations often sufficient\n",
    "- 2-5x faster convergence\n",
    "- Dramatically reduced divergences (10-100x fewer)\n",
    "\n",
    "Let's run Bayesian inference with warm-start:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T21:16:28.174198Z",
     "iopub.status.busy": "2026-02-09T21:16:28.174069Z",
     "iopub.status.idle": "2026-02-09T21:16:35.800128Z",
     "shell.execute_reply": "2026-02-09T21:16:35.799600Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Running Bayesian inference with NLSQ warm-start...\")\n",
    "print(\"(This may take 1-2 minutes)\\n\")\n",
    "\n",
    "start_bayes = time.time()\n",
    "\n",
    "# Bayesian inference with warm-start\n",
    "result = model.fit_bayesian(\n",
    "    t, G_t_noisy,\n",
    "    num_warmup=1000,   # Burn-in iterations\n",
    "    num_samples=2000,  # Posterior samples\n",
    "    num_chains=4,      # Multiple chains for robust diagnostics\n",
    "    initial_values={   # WARM-START from NLSQ\n",
    "        'G0': G0_nlsq,\n",
    "        'eta': eta_nlsq\n",
    "    }\n",
    ")\n",
    "\n",
    "bayes_time = time.time() - start_bayes\n",
    "\n",
    "print(f\"\\n✓ Bayesian inference completed in {bayes_time:.2f} s\")\n",
    "print(f\"Speedup vs cold start: ~2-5x faster with warm-start\")\n",
    "print(f\"Total time (NLSQ + Bayes): {nlsq_time + bayes_time:.2f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Posterior Summary and Interpretation\n",
    "\n",
    "### Understanding Posterior Distributions\n",
    "\n",
    "The **posterior distribution** P(θ|data) represents our updated beliefs about parameters after observing data:\n",
    "\n",
    "- **Prior:** P(θ) - beliefs before seeing data (encoded in parameter bounds)\n",
    "- **Likelihood:** P(data|θ) - probability of data given parameters\n",
    "- **Posterior:** P(θ|data) ∝ P(data|θ) × P(θ) - updated beliefs\n",
    "\n",
    "From posterior samples, we compute:\n",
    "- **Mean/Median:** Central tendency (analogous to point estimate)\n",
    "- **Std:** Spread (uncertainty)\n",
    "- **Credible Intervals:** Probability ranges (e.g., 95% CI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T21:16:35.801485Z",
     "iopub.status.busy": "2026-02-09T21:16:35.801378Z",
     "iopub.status.idle": "2026-02-09T21:16:35.807003Z",
     "shell.execute_reply": "2026-02-09T21:16:35.806623Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extract posterior samples and diagnostics\n",
    "posterior = result.posterior_samples\n",
    "diagnostics = result.diagnostics\n",
    "summary = result.summary\n",
    "\n",
    "# Compute credible intervals\n",
    "credible_intervals = model.get_credible_intervals(posterior, credibility=0.95)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"POSTERIOR SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nParameter Estimates (posterior mean ± std):\")\n",
    "print(f\"  G₀  = {summary['G0']['mean']:.4e} ± {summary['G0']['std']:.4e} Pa\")\n",
    "print(f\"  η   = {summary['eta']['mean']:.4e} ± {summary['eta']['std']:.4e} Pa·s\")\n",
    "\n",
    "print(\"\\n95% Credible Intervals:\")\n",
    "print(f\"  G₀:  [{credible_intervals['G0'][0]:.4e}, {credible_intervals['G0'][1]:.4e}] Pa\")\n",
    "print(f\"  η:   [{credible_intervals['eta'][0]:.4e}, {credible_intervals['eta'][1]:.4e}] Pa·s\")\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(f\"  \\\"There is 95% probability that G₀ lies in the interval above\\\"\")\n",
    "print(f\"  This is a DIRECT probabilistic statement (Bayesian interpretation)\")\n",
    "\n",
    "print(\"\\nRelative Uncertainties:\")\n",
    "print(f\"  G₀:  {summary['G0']['std'] / summary['G0']['mean'] * 100:.2f}%\")\n",
    "print(f\"  η:   {summary['eta']['std'] / summary['eta']['mean'] * 100:.2f}%\")\n",
    "\n",
    "print(\"\\nComparison to True Values:\")\n",
    "G0_in_CI = credible_intervals['G0'][0] <= G0_true <= credible_intervals['G0'][1]\n",
    "eta_in_CI = credible_intervals['eta'][0] <= eta_true <= credible_intervals['eta'][1]\n",
    "print(f\"  G₀ true value in 95% CI:  {G0_in_CI} ✓\" if G0_in_CI else f\"  G₀ true value in 95% CI:  {G0_in_CI} ✗\")\n",
    "print(f\"  η true value in 95% CI:   {eta_in_CI} ✓\" if eta_in_CI else f\"  η true value in 95% CI:   {eta_in_CI} ✗\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Convergence Diagnostics (Introduction)\n\n### Why Convergence Matters\n\nMCMC (Markov Chain Monte Carlo) generates samples by exploring parameter space. **Convergence** means:\n- Chains have reached stationary distribution (the posterior)\n- Samples are representative of posterior\n- Results are reliable\n\n**Always check convergence before interpreting results!**\n\n### Key Metrics\n\n**1. R-hat (Gelman-Rubin Statistic):**\n- Compares between-chain variance to within-chain variance\n- **Target: R-hat < 1.01** for all parameters\n- R-hat > 1.01: Chains exploring different regions (NOT converged)\n\n**2. ESS (Effective Sample Size):**\n- Accounts for autocorrelation between samples\n- **Target: ESS > 400** for reliable estimates\n- ESS << num_samples: High autocorrelation (poor mixing)\n\n**3. Divergences:**\n- NUTS sampler failures (numerical instability)\n- **Target: < 1% divergence rate**\n- Many divergences: Results unreliable, need reparameterization or better priors\n\n### Convergence Diagnostic Targets\n\n| Diagnostic | Target | Interpretation | Action if Failed |\n|------------|--------|----------------|------------------|\n| **R-hat** | < 1.01 | All chains converged to same posterior | Increase `num_warmup` |\n| **ESS** | > 400 | Sufficient independent samples | Increase `num_samples` |\n| **Divergences** | < 1% | No sampling pathologies | Increase `target_accept_prob` or tighten priors |\n\n### Residual Analysis\n\n**Residual pattern diagnosis** is critical for model validation:\n- **Random scatter**: Model captures data structure (good)\n- **Systematic patterns**: Model misspecification (underfitting)\n- **Heteroscedasticity**: Non-constant variance (consider weighted least squares)\n\nThe residual plots throughout this notebook reveal whether the Maxwell model adequately describes the relaxation data."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T21:16:35.808398Z",
     "iopub.status.busy": "2026-02-09T21:16:35.808284Z",
     "iopub.status.idle": "2026-02-09T21:16:35.811866Z",
     "shell.execute_reply": "2026-02-09T21:16:35.811457Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"CONVERGENCE DIAGNOSTICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nR-hat (Gelman-Rubin):\")\n",
    "print(f\"  G₀:  {diagnostics['r_hat']['G0']:.4f}  {'✓ Converged' if diagnostics['r_hat']['G0'] < 1.01 else '✗ NOT converged'}\")\n",
    "print(f\"  η:   {diagnostics['r_hat']['eta']:.4f}  {'✓ Converged' if diagnostics['r_hat']['eta'] < 1.01 else '✗ NOT converged'}\")\n",
    "print(\"  Target: < 1.01 (all parameters must meet this)\")\n",
    "\n",
    "print(\"\\nEffective Sample Size (ESS):\")\n",
    "print(f\"  G₀:  {diagnostics['ess']['G0']:.0f}  {'✓ Sufficient' if diagnostics['ess']['G0'] > 400 else '✗ Low (increase samples)'}\")\n",
    "print(f\"  η:   {diagnostics['ess']['eta']:.0f}  {'✓ Sufficient' if diagnostics['ess']['eta'] > 400 else '✗ Low (increase samples)'}\")\n",
    "print(f\"  Target: > 400 (out of {result.num_samples * result.num_chains} total samples)\")\n",
    "\n",
    "if 'num_divergences' in diagnostics:\n",
    "    div_rate = diagnostics['num_divergences'] / (result.num_samples * result.num_chains) * 100\n",
    "    print(\"\\nDivergences:\")\n",
    "    print(f\"  Count: {diagnostics['num_divergences']} ({div_rate:.2f}%)\")\n",
    "    print(f\"  {'✓ Good' if div_rate < 1 else '✗ High (results unreliable)'}\")\n",
    "    print(\"  Target: < 1% divergence rate\")\n",
    "\n",
    "# Overall convergence check\n",
    "converged = (\n",
    "    diagnostics['r_hat']['G0'] < 1.01 and\n",
    "    diagnostics['r_hat']['eta'] < 1.01 and\n",
    "    diagnostics['ess']['G0'] > 400 and\n",
    "    diagnostics['ess']['eta'] > 400\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "if converged:\n",
    "    print(\"✓✓✓ EXCELLENT CONVERGENCE ✓✓✓\")\n",
    "    print(\"All diagnostic criteria met. Results are reliable.\")\n",
    "else:\n",
    "    print(\"⚠⚠⚠ CONVERGENCE ISSUES ⚠⚠⚠\")\n",
    "    print(\"Increase num_warmup or num_samples and rerun.\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T21:16:35.813112Z",
     "iopub.status.busy": "2026-02-09T21:16:35.813027Z",
     "iopub.status.idle": "2026-02-09T21:16:37.433050Z",
     "shell.execute_reply": "2026-02-09T21:16:37.432574Z"
    }
   },
   "outputs": [],
   "source": [
    "# ArviZ diagnostic plots (trace, pair, forest, energy, autocorr, rank)\n",
    "display_arviz_diagnostics(result, ['G0', 'eta'], fast_mode=FAST_MODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Posterior Distributions Visualization\n",
    "\n",
    "Let's visualize the posterior distributions compared to NLSQ point estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T21:16:37.434337Z",
     "iopub.status.busy": "2026-02-09T21:16:37.434132Z",
     "iopub.status.idle": "2026-02-09T21:16:37.596522Z",
     "shell.execute_reply": "2026-02-09T21:16:37.595805Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# G0 posterior\n",
    "ax1.hist(posterior['G0'], bins=50, density=True, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "ax1.axvline(G0_nlsq, color='red', linestyle='--', linewidth=2, label='NLSQ point estimate')\n",
    "ax1.axvline(G0_true, color='green', linestyle=':', linewidth=2, label='True value')\n",
    "ax1.axvline(summary['G0']['mean'], color='blue', linestyle='-', linewidth=2, label='Posterior mean')\n",
    "ax1.axvspan(credible_intervals['G0'][0], credible_intervals['G0'][1], \n",
    "            alpha=0.2, color='blue', label='95% credible interval')\n",
    "ax1.set_xlabel('G₀ (Pa)', fontweight='bold')\n",
    "ax1.set_ylabel('Posterior Density', fontweight='bold')\n",
    "ax1.set_title('Posterior Distribution: Initial Modulus', fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# eta posterior\n",
    "ax2.hist(posterior['eta'], bins=50, density=True, alpha=0.7, color='coral', edgecolor='black')\n",
    "ax2.axvline(eta_nlsq, color='red', linestyle='--', linewidth=2, label='NLSQ point estimate')\n",
    "ax2.axvline(eta_true, color='green', linestyle=':', linewidth=2, label='True value')\n",
    "ax2.axvline(summary['eta']['mean'], color='orangered', linestyle='-', linewidth=2, label='Posterior mean')\n",
    "ax2.axvspan(credible_intervals['eta'][0], credible_intervals['eta'][1], \n",
    "            alpha=0.2, color='orangered', label='95% credible interval')\n",
    "ax2.set_xlabel('η (Pa·s)', fontweight='bold')\n",
    "ax2.set_ylabel('Posterior Density', fontweight='bold')\n",
    "ax2.set_title('Posterior Distribution: Viscosity', fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "display(fig)\n",
    "plt.close(fig)\n",
    "\n",
    "print(\"OBSERVATIONS:\")\n",
    "print(\"1. Posterior means ≈ NLSQ point estimates (as expected for well-behaved problem)\")\n",
    "print(\"2. Posterior has width → quantifies uncertainty (NLSQ cannot provide this)\")\n",
    "print(\"3. 95% CI captures true values (validation of uncertainty quantification)\")\n",
    "print(\"4. Unimodal, symmetric distributions → well-constrained parameters\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Comparison: NLSQ vs Bayesian\n",
    "\n",
    "Let's compare the two approaches side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T21:16:37.598097Z",
     "iopub.status.busy": "2026-02-09T21:16:37.597999Z",
     "iopub.status.idle": "2026-02-09T21:16:37.602831Z",
     "shell.execute_reply": "2026-02-09T21:16:37.602300Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"NLSQ vs BAYESIAN COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(f\"{'Method':<20} {'G₀ (Pa)':<20} {'η (Pa·s)':<20} {'Time (s)':<15}\")\n",
    "print(\"-\"*80)\n",
    "print(f\"{'True Values':<20} {G0_true:<20.4e} {eta_true:<20.4e} {'N/A':<15}\")\n",
    "print(f\"{'NLSQ Point':<20} {G0_nlsq:<20.4e} {eta_nlsq:<20.4e} {nlsq_time:<15.4f}\")\n",
    "print(f\"{'Bayesian Mean':<20} {summary['G0']['mean']:<20.4e} {summary['eta']['mean']:<20.4e} {bayes_time:<15.2f}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY DIFFERENCES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. UNCERTAINTY QUANTIFICATION:\")\n",
    "print(f\"   NLSQ:     Single value (no uncertainty) ✗\")\n",
    "print(f\"   Bayesian: Full distribution with credible intervals ✓\")\n",
    "print(f\"             G₀: {summary['G0']['std']/summary['G0']['mean']*100:.2f}% relative uncertainty\")\n",
    "print(f\"             η:  {summary['eta']['std']/summary['eta']['mean']*100:.2f}% relative uncertainty\")\n",
    "\n",
    "print(\"\\n2. COMPUTATIONAL COST:\")\n",
    "print(f\"   NLSQ:     {nlsq_time:.4f} s (fast) ✓\")\n",
    "print(f\"   Bayesian: {bayes_time:.2f} s (~{bayes_time/nlsq_time:.0f}x slower) ✗\")\n",
    "print(f\"   Total:    {nlsq_time + bayes_time:.2f} s (with warm-start)\")\n",
    "\n",
    "print(\"\\n3. INTERPRETABILITY:\")\n",
    "print(f\"   NLSQ:     Point estimate only\")\n",
    "print(f\"   Bayesian: Full posterior distribution enables:\")\n",
    "print(f\"             - Credible intervals (direct probability statements) ✓\")\n",
    "print(f\"             - Parameter correlations (identifiability) ✓\")\n",
    "print(f\"             - Derived quantities with uncertainty ✓\")\n",
    "print(f\"             - Model comparison (WAIC, LOO) ✓\")\n",
    "\n",
    "print(\"\\n4. CONVERGENCE:\")\n",
    "print(f\"   NLSQ:     Always converges (optimization)\")\n",
    "print(f\"   Bayesian: Must check R-hat, ESS, divergences\")\n",
    "print(f\"             Current: R-hat={max(diagnostics['r_hat'].values()):.4f}, ESS={min(diagnostics['ess'].values()):.0f} ✓\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RECOMMENDATION\")\n",
    "print(\"=\"*80)\n",
    "print(\"Use NLSQ when: Fast screening, well-constrained parameters, no uncertainty needed\")\n",
    "print(\"Use Bayesian when: Uncertainty quantification essential, parameter correlations matter,\")\n",
    "print(\"                   model comparison needed, prediction uncertainty required\")\n",
    "print(\"\\nBest practice: NLSQ first (fast), then Bayesian if uncertainty needed (warm-start)\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "- **ArviZ Documentation**: [Convergence Diagnostics](https://python.arviz.org/en/stable/api/diagnostics.html) — Detailed explanations of R-hat, ESS, and other metrics\n",
    "- **NumPyro MCMC Guide**: [NUTS Sampler](https://num.pyro.ai/en/stable/mcmc.html#numpyro.infer.NUTS) — Understanding the No-U-Turn Sampler\n",
    "- **Betancourt (2018)**: [\"Calibrating Model-Based Inferences and Decisions\"](https://arxiv.org/abs/1803.08393) — Bayesian workflow best practices\n",
    "- **RheoJAX Bayesian Workflow**: [Documentation](../../docs/source/_includes/bayesian_workflow.rst) — Implementation details and advanced features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- **[02-prior-selection.ipynb](02-prior-selection.ipynb)**: Learn how to choose and tune priors for rheological models\n",
    "- **[03-convergence-diagnostics.ipynb](03-convergence-diagnostics.ipynb)**: Deep dive into diagnosing and fixing convergence issues\n",
    "- **[04-model-comparison.ipynb](04-model-comparison.ipynb)**: Compare competing models using WAIC and LOO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Apply to Other Models\n- All 53 RheoJAX models support Bayesian inference via BayesianMixin\n- See `basic/` notebooks for Zener, SpringPot, Bingham, PowerLaw examples\n- Try Bayesian inference on your own rheological data\n\n### Advanced Topics\n- **[06-bayesian_workflow_demo.ipynb](06-bayesian_workflow_demo.ipynb)**: Complete pipeline demonstration\n- **[07-gmm_bayesian_workflow.ipynb](07-gmm_bayesian_workflow.ipynb)**: Multi-mode Generalized Maxwell\n- **[08-spp-laos.ipynb](08-spp-laos.ipynb)** & **[09-spp-rheojax-workflow.ipynb](09-spp-rheojax-workflow.ipynb)**: Large Amplitude Oscillatory Shear analysis\n\n### Key References\n\n- **McElreath, R. (2020).** *Statistical Rethinking: A Bayesian Course with Examples in R and Stan*. 2nd ed. CRC Press. [Comprehensive introduction to Bayesian modeling]\n- **Gelman, A. et al. (2013).** *Bayesian Data Analysis*. 3rd ed. CRC Press. [Standard textbook on Bayesian methods]\n- **Betancourt, M. (2017).** \"A Conceptual Introduction to Hamiltonian Monte Carlo.\" arXiv:1701.02434. [Understanding NUTS sampler]"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Session Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T21:16:37.604324Z",
     "iopub.status.busy": "2026-02-09T21:16:37.604224Z",
     "iopub.status.idle": "2026-02-09T21:16:37.606586Z",
     "shell.execute_reply": "2026-02-09T21:16:37.606118Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import arviz as az\n",
    "\n",
    "import rheojax\n",
    "\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"RheoJAX: {rheojax.__version__}\")\n",
    "print(f\"JAX: {jax.__version__}\")\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "print(f\"ArviZ: {az.__version__}\")\n",
    "print(f\"JAX devices: {jax.devices()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rheojax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}