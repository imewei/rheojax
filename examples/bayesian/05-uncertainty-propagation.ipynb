{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": "# Uncertainty Propagation: From Posterior Samples to Predictions\n\n> **Handbook:** See [Uncertainty Propagation](../../docs/source/user_guide/03_advanced_topics/bayesian_inference.rst#uncertainty-propagation) for theoretical framework and [Posterior Predictive Checks](../../docs/source/_includes/bayesian_workflow.rst#posterior-predictive) for validation strategies.\n\nThis notebook demonstrates how to propagate parameter uncertainty through to predictions, derived quantities, and credible intervals using Bayesian posterior samples.\n\n## Learning Objectives\n\nAfter completing this notebook, you will be able to:\n- Compute credible intervals for derived rheological quantities\n- Propagate uncertainty from parameters to model predictions\n- Diagnose parameter identifiability using correlations\n- Analyze uncertainty in complex quantities (relaxation time, loss tangent)\n- Generate prediction bands with quantified uncertainty\n- Use posterior samples for uncertainty quantification in any transformation\n- Apply uncertainty propagation to rheological decision-making\n\n## Prerequisites\n\n- Understanding of Bayesian inference (`01-bayesian-basics.ipynb`)\n- Prior selection concepts (`02-prior-selection.ipynb`)\n- Convergence diagnostics (`03-convergence-diagnostics.ipynb`)\n- Familiarity with Zener model (4 parameters: Ge, Gm, eta)\n\n**Estimated Time:** 40-45 minutes"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T21:17:58.117042Z",
     "iopub.status.busy": "2026-02-09T21:17:58.116846Z",
     "iopub.status.idle": "2026-02-09T21:17:58.122760Z",
     "shell.execute_reply": "2026-02-09T21:17:58.121921Z"
    }
   },
   "outputs": [],
   "source": [
    "# Google Colab Setup - Run this cell first!\n",
    "# Skip if running locally with rheojax already installed\n",
    "\n",
    "import sys\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Install rheojax and dependencies\n",
    "    !pip install -q rheojax\n",
    "    \n",
    "    # Colab uses float32 by default - we need float64 for numerical stability\n",
    "    # This MUST be set before importing JAX\n",
    "    import os\n",
    "    os.environ['JAX_ENABLE_X64'] = 'true'\n",
    "    \n",
    "    print(\"✓ RheoJAX installed successfully!\")\n",
    "    print(\"✓ Float64 precision enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "## 1. Introduction: Beyond Point Estimates\n",
    "\n",
    "### The Uncertainty Propagation Problem\n",
    "\n",
    "After Bayesian inference, we have posterior distributions for model parameters. But rheologists need more:\n",
    "- **Relaxation time τ = η/G₀**: What is its uncertainty?\n",
    "- **Predictions**: What are credible intervals for G(t) at new time points?\n",
    "- **Derived quantities**: How uncertain is the loss tangent tan(δ)?\n",
    "\n",
    "### Point Estimates Are Insufficient\n",
    "\n",
    "**NLSQ Approach (Inadequate):**\n",
    "```python\n",
    "# NLSQ fit\n",
    "model.fit(t, G_t)\n",
    "G0_fit = model.parameters.get_value('G0')  # Single value\n",
    "eta_fit = model.parameters.get_value('eta')  # Single value\n",
    "tau_fit = eta_fit / G0_fit  # Single value - but how certain?\n",
    "```\n",
    "\n",
    "**Problem:** τ uncertainty depends on both G₀ and η uncertainties AND their correlation.\n",
    "\n",
    "**Bayesian Approach (Complete):**\n",
    "```python\n",
    "# Bayesian inference\n",
    "result = model.fit_bayesian(t, G_t, ...)\n",
    "G0_samples = result.posterior_samples['G0']  # 2000 samples\n",
    "eta_samples = result.posterior_samples['eta']  # 2000 samples\n",
    "\n",
    "# Propagate uncertainty to derived quantity\n",
    "tau_samples = eta_samples / G0_samples  # 2000 samples\n",
    "\n",
    "# Full distribution, not just point estimate\n",
    "tau_mean = tau_samples.mean()\n",
    "tau_ci = np.percentile(tau_samples, [2.5, 97.5])\n",
    "```\n",
    "\n",
    "### Uncertainty Propagation Principle\n",
    "\n",
    "For any function f(θ) of parameters θ:\n",
    "1. Extract posterior samples: θ₁, θ₂, ..., θₙ\n",
    "2. Apply function: f(θ₁), f(θ₂), ..., f(θₙ)\n",
    "3. Analyze distribution of f(θ)\n",
    "\n",
    "**Monte Carlo propagation automatically accounts for:**\n",
    "- Parameter uncertainties\n",
    "- Parameter correlations\n",
    "- Nonlinear transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## 2. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T21:17:58.125472Z",
     "iopub.status.busy": "2026-02-09T21:17:58.125281Z",
     "iopub.status.idle": "2026-02-09T21:18:00.116626Z",
     "shell.execute_reply": "2026-02-09T21:18:00.116180Z"
    }
   },
   "outputs": [],
   "source": [
    "# Configure matplotlib for inline plotting in VS Code/Jupyter\n",
    "# Configure matplotlib for inline plotting in VS Code/Jupyter\n",
    "# MUST come before importing matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "\n",
    "# ArviZ for diagnostics\n",
    "import arviz as az\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "from rheojax.core.jax_config import safe_import_jax\n",
    "\n",
    "# RheoJAX imports\n",
    "from rheojax.models import Zener\n",
    "\n",
    "# Safe JAX import\n",
    "jax, jnp = safe_import_jax()\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Plotting\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"✓ Imports successful\")\n",
    "\n",
    "# Suppress matplotlib backend warning in VS Code\n",
    "warnings.filterwarnings('ignore', message='.*non-interactive.*')\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, os.path.dirname(os.path.abspath(\"\")))\n",
    "from utils.plotting_utils import (\n",
    "    display_arviz_diagnostics,\n",
    "    plot_nlsq_fit,\n",
    "    plot_posterior_predictive,\n",
    ")\n",
    "\n",
    "FAST_MODE = os.environ.get(\"FAST_MODE\", \"1\") == \"1\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-intro",
   "metadata": {},
   "source": [
    "## 3. Generate Synthetic Data: Zener Model with Correlations\n",
    "\n",
    "We generate data from a **Zener model** (3 parameters) to demonstrate:\n",
    "- Parameter correlations (Gm and η affect same timescale)\n",
    "- Derived quantity uncertainty (relaxation time τ = η/Gm)\n",
    "- Prediction uncertainty bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate-data",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T21:18:00.117982Z",
     "iopub.status.busy": "2026-02-09T21:18:00.117809Z",
     "iopub.status.idle": "2026-02-09T21:18:00.328431Z",
     "shell.execute_reply": "2026-02-09T21:18:00.327880Z"
    }
   },
   "outputs": [],
   "source": [
    "# True Zener parameters\n",
    "Ge_true = 1e4    # Equilibrium modulus (Pa)\n",
    "Gm_true = 5e4    # Maxwell arm modulus (Pa)\n",
    "eta_true = 1e3   # Viscosity (Pa·s)\n",
    "tau_true = eta_true / Gm_true  # Relaxation time (s)\n",
    "\n",
    "print(\"True Zener Parameters:\")\n",
    "print(f\"  Ge  = {Ge_true:.2e} Pa (equilibrium modulus)\")\n",
    "print(f\"  Gm  = {Gm_true:.2e} Pa (Maxwell arm)\")\n",
    "print(f\"  η   = {eta_true:.2e} Pa·s\")\n",
    "print(f\"  τ   = {tau_true:.4f} s (characteristic time)\\n\")\n",
    "\n",
    "# Time array\n",
    "t = np.logspace(-2, 2, 60)  # 0.01 to 100 s\n",
    "\n",
    "# True Zener relaxation modulus\n",
    "# G(t) = Ge + Gm * exp(-t / tau)\n",
    "G_t_true = Ge_true + Gm_true * np.exp(-t / tau_true)\n",
    "\n",
    "# Add realistic noise (3%)\n",
    "noise_level = 0.03\n",
    "noise = np.random.normal(0, noise_level * G_t_true)\n",
    "G_t_noisy = G_t_true + noise\n",
    "\n",
    "print(f\"Data: {len(t)} points from {t.min():.2f} to {t.max():.2f} s\")\n",
    "print(f\"Noise: {noise_level*100:.0f}% relative\")\n",
    "\n",
    "# Visualize\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "plt.loglog(t, G_t_noisy, 'o', markersize=6, alpha=0.7, label='Synthetic data (Zener + noise)', color='black')\n",
    "plt.loglog(t, G_t_true, '--', linewidth=2, alpha=0.5, label='True Zener response', color='red')\n",
    "plt.axhline(Ge_true, color='blue', linestyle=':', linewidth=1.5, alpha=0.5, \n",
    "            label=f'Equilibrium modulus Ge = {Ge_true:.0e} Pa')\n",
    "plt.axvline(tau_true, color='green', linestyle=':', linewidth=1.5, alpha=0.5,\n",
    "            label=f'Relaxation time τ = {tau_true:.4f} s')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Relaxation Modulus G(t) (Pa)')\n",
    "plt.title('Stress Relaxation Data (Zener Model)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, which='both')\n",
    "plt.tight_layout()\n",
    "display(fig)\n",
    "plt.close(fig)\n",
    "\n",
    "print(\"\\nKey features:\")\n",
    "print(\"  - Exponential decay from (Ge + Gm) to Ge\")\n",
    "print(\"  - Characteristic time τ where G(τ) ≈ Ge + Gm/e\")\n",
    "print(\"  - Finite equilibrium modulus (viscoelastic solid)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fit-intro",
   "metadata": {},
   "source": [
    "## 4. Fit Zener Model: NLSQ → NUTS Workflow\n",
    "\n",
    "We perform two-stage fitting:\n",
    "1. **NLSQ**: Fast point estimate with warm-start values\n",
    "2. **Bayesian NUTS**: Full posterior with uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fit-nlsq",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T21:18:00.329977Z",
     "iopub.status.busy": "2026-02-09T21:18:00.329881Z",
     "iopub.status.idle": "2026-02-09T21:18:04.500776Z",
     "shell.execute_reply": "2026-02-09T21:18:04.500119Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"STAGE 1: NLSQ OPTIMIZATION (POINT ESTIMATES)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create and configure Zener model\n",
    "model = Zener()\n",
    "model.parameters.set_bounds('Ge', (1e2, 1e6))\n",
    "model.parameters.set_bounds('Gm', (1e3, 1e7))\n",
    "model.parameters.set_bounds('eta', (1e1, 1e5))\n",
    "\n",
    "# NLSQ fit\n",
    "import time\n",
    "\n",
    "start_nlsq = time.time()\n",
    "model.fit(t, G_t_noisy)\n",
    "time_nlsq = time.time() - start_nlsq\n",
    "\n",
    "# Extract point estimates\n",
    "Ge_nlsq = model.parameters.get_value('Ge')\n",
    "Gm_nlsq = model.parameters.get_value('Gm')\n",
    "eta_nlsq = model.parameters.get_value('eta')\n",
    "tau_nlsq = eta_nlsq / Gm_nlsq\n",
    "\n",
    "print(f\"\\nNLSQ Point Estimates (converged in {time_nlsq:.3f}s):\")\n",
    "print(f\"  Ge  = {Ge_nlsq:.3e} Pa\")\n",
    "print(f\"  Gm  = {Gm_nlsq:.3e} Pa\")\n",
    "print(f\"  η   = {eta_nlsq:.3e} Pa·s\")\n",
    "print(f\"  τ   = {tau_nlsq:.4f} s (derived)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STAGE 2: BAYESIAN INFERENCE (UNCERTAINTY QUANTIFICATION)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Bayesian inference with warm-start\n",
    "print(\"\\nRunning NUTS sampling with warm-start from NLSQ...\")\n",
    "start_bayes = time.time()\n",
    "result = model.fit_bayesian(\n",
    "    t, G_t_noisy,\n",
    "    num_warmup=1000,\n",
    "    num_samples=2000,\n",
    "    num_chains=1,\n",
    "    initial_values={\n",
    "        'Ge': Ge_nlsq,\n",
    "        'Gm': Gm_nlsq,\n",
    "        'eta': eta_nlsq\n",
    "    }\n",
    ")\n",
    "time_bayes = time.time() - start_bayes\n",
    "\n",
    "print(f\"\\n✓ Bayesian inference complete ({time_bayes:.2f}s)\")\n",
    "print(f\"  Speedup from warm-start: ~{time_bayes/time_nlsq:.1f}x slower than NLSQ (but with full uncertainty)\")\n",
    "print(f\"  R-hat: {max(result.diagnostics['r_hat'].values()):.4f} < 1.01 ✓\")\n",
    "print(f\"  ESS:   {min(result.diagnostics['ess'].values()):.0f} > 400 ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acae54e37e7d407bbb7b55eff062a284",
   "metadata": {},
   "source": [
    "### Convergence Diagnostic Interpretation\n",
    "\n",
    "| Metric | Target | Meaning |\n",
    "|--------|--------|---------|",
    "| **R-hat < 1.01** | Chains converged | Multiple chains agree on posterior |",
    "| **ESS > 400** | Sufficient samples | Independent information content |",
    "| **Divergences < 1%** | Well-behaved sampler | No numerical issues in posterior geometry |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "derived-intro",
   "metadata": {},
   "source": [
    "## 5. Derived Quantity: Relaxation Time τ\n",
    "\n",
    "### Computing Uncertainty in τ = η/Gm\n",
    "\n",
    "The relaxation time τ is a derived quantity that depends on two parameters. We propagate uncertainty through this transformation using posterior samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compute-tau",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T21:18:04.502323Z",
     "iopub.status.busy": "2026-02-09T21:18:04.502196Z",
     "iopub.status.idle": "2026-02-09T21:18:04.639123Z",
     "shell.execute_reply": "2026-02-09T21:18:04.638600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extract posterior samples\n",
    "Ge_samples = result.posterior_samples['Ge']\n",
    "Gm_samples = result.posterior_samples['Gm']\n",
    "eta_samples = result.posterior_samples['eta']\n",
    "\n",
    "# Compute derived quantity: relaxation time τ = η/Gm\n",
    "tau_samples = eta_samples / Gm_samples\n",
    "\n",
    "# Summary statistics\n",
    "tau_mean = tau_samples.mean()\n",
    "tau_std = tau_samples.std()\n",
    "tau_median = np.median(tau_samples)\n",
    "tau_ci_68 = np.percentile(tau_samples, [16, 84])  # 68% (1σ)\n",
    "tau_ci_95 = np.percentile(tau_samples, [2.5, 97.5])  # 95% (2σ)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"RELAXATION TIME τ = η/Gm (DERIVED QUANTITY)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nPoint estimate (NLSQ):\")\n",
    "print(f\"  τ = {tau_nlsq:.4f} s\")\n",
    "print(f\"\\nBayesian uncertainty quantification:\")\n",
    "print(f\"  Mean:     {tau_mean:.4f} s\")\n",
    "print(f\"  Median:   {tau_median:.4f} s\")\n",
    "print(f\"  Std:      {tau_std:.4f} s\")\n",
    "print(f\"  68% CI:   [{tau_ci_68[0]:.4f}, {tau_ci_68[1]:.4f}] s\")\n",
    "print(f\"  95% CI:   [{tau_ci_95[0]:.4f}, {tau_ci_95[1]:.4f}] s\")\n",
    "print(f\"\\nTrue value: {tau_true:.4f} s (within 95% CI? {tau_ci_95[0] < tau_true < tau_ci_95[1]})\")\n",
    "\n",
    "# Visualize posterior distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Histogram with statistics\n",
    "axes[0].hist(tau_samples, bins=50, density=True, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "axes[0].axvline(tau_mean, color='red', linestyle='--', linewidth=2, label=f'Mean = {tau_mean:.4f} s')\n",
    "axes[0].axvline(tau_ci_95[0], color='black', linestyle=':', linewidth=1.5, label=f'95% CI')\n",
    "axes[0].axvline(tau_ci_95[1], color='black', linestyle=':', linewidth=1.5)\n",
    "axes[0].axvline(tau_true, color='green', linestyle='-', linewidth=2, label=f'True = {tau_true:.4f} s')\n",
    "axes[0].fill_between([tau_ci_95[0], tau_ci_95[1]], 0, axes[0].get_ylim()[1], \n",
    "                      alpha=0.2, color='gray', label='95% credible region')\n",
    "axes[0].set_xlabel('Relaxation time τ (s)', fontweight='bold')\n",
    "axes[0].set_ylabel('Posterior density', fontweight='bold')\n",
    "axes[0].set_title('Posterior Distribution of τ = η/Gm', fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Comparison with NLSQ point estimate\n",
    "axes[1].violinplot([tau_samples], positions=[0], widths=0.7, showmeans=True, showmedians=True)\n",
    "axes[1].scatter([0], [tau_nlsq], s=200, c='orange', marker='D', \n",
    "               edgecolors='black', linewidths=2, label='NLSQ point estimate', zorder=5)\n",
    "axes[1].scatter([0], [tau_true], s=200, c='green', marker='*', \n",
    "               edgecolors='black', linewidths=2, label='True value', zorder=5)\n",
    "axes[1].set_xlim([-0.5, 0.5])\n",
    "axes[1].set_xticks([0])\n",
    "axes[1].set_xticklabels(['τ'])\n",
    "axes[1].set_ylabel('Relaxation time τ (s)', fontweight='bold')\n",
    "axes[1].set_title('NLSQ vs Bayesian Uncertainty', fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "display(fig)\n",
    "plt.close(fig)\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"  - NLSQ gives single value (orange diamond)\")\n",
    "print(\"  - Bayesian gives full distribution (blue violin)\")\n",
    "print(f\"  - Relative uncertainty: {tau_std/tau_mean*100:.1f}%\")\n",
    "print(\"  - 95% CI captures true value ✓\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "correlation-intro",
   "metadata": {},
   "source": [
    "## 6. Parameter Identifiability: Correlation Analysis\n",
    "\n",
    "### Why Correlations Matter\n",
    "\n",
    "Strong parameter correlations indicate **identifiability issues**:\n",
    "- Data cannot independently constrain both parameters\n",
    "- Multiple parameter combinations fit data equally well\n",
    "- Derived quantities may still be well-constrained\n",
    "\n",
    "### Interpreting Correlations\n",
    "\n",
    "- **|ρ| < 0.3**: Well-identified (data constrains parameters independently)\n",
    "- **0.3 < |ρ| < 0.7**: Moderate correlation (acceptable for most applications)\n",
    "- **|ρ| > 0.7**: Strong correlation (identifiability issue, may need more/different data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correlations",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T21:18:04.640753Z",
     "iopub.status.busy": "2026-02-09T21:18:04.640626Z",
     "iopub.status.idle": "2026-02-09T21:18:05.108394Z",
     "shell.execute_reply": "2026-02-09T21:18:05.107760Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"PARAMETER CORRELATION ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Compute correlation matrix\n",
    "param_df = pd.DataFrame({\n",
    "    'Ge': Ge_samples,\n",
    "    'Gm': Gm_samples,\n",
    "    'eta': eta_samples\n",
    "})\n",
    "correlation = param_df.corr()\n",
    "\n",
    "print(\"\\nCorrelation matrix:\")\n",
    "print(correlation.to_string())\n",
    "\n",
    "# Identify strong correlations\n",
    "print(\"\\nCorrelation interpretation:\")\n",
    "for i, param_i in enumerate(['Ge', 'Gm', 'eta']):\n",
    "    for j, param_j in enumerate(['Ge', 'Gm', 'eta']):\n",
    "        if i < j:\n",
    "            rho = correlation.loc[param_i, param_j]\n",
    "            if abs(rho) < 0.3:\n",
    "                status = \"✓ Well-identified (weak correlation)\"\n",
    "            elif abs(rho) < 0.7:\n",
    "                status = \"⚠ Moderate correlation (acceptable)\"\n",
    "            else:\n",
    "                status = \"✗ Strong correlation (identifiability issue)\"\n",
    "            print(f\"  {param_i} - {param_j}: ρ = {rho:+.3f}  {status}\")\n",
    "\n",
    "# Visualize correlations with ArviZ pair plot\n",
    "print(\"\\nGenerating pair plot (joint distributions)...\")\n",
    "idata = result.to_inference_data()\n",
    "az.plot_pair(\n",
    "    idata, \n",
    "    var_names=['Ge', 'Gm', 'eta'],\n",
    "    kind='scatter',\n",
    "    marginals=True,\n",
    "    figsize=(10, 8)\n",
    ")\n",
    "plt.suptitle('Parameter Correlations (Pair Plot)', fontweight='bold', y=1.01)\n",
    "plt.tight_layout()\n",
    "fig = plt.gcf()  # Get current figure from ArviZ\n",
    "display(fig)\n",
    "plt.close(fig)\n",
    "\n",
    "print(\"\\nPhysical interpretation:\")\n",
    "print(\"  - Gm and η correlation expected (both control relaxation time τ = η/Gm)\")\n",
    "print(\"  - Ge weakly correlated (independent: long-time plateau)\")\n",
    "print(\"  - Moderate correlations are normal for viscoelastic models\")\n",
    "print(\"\\nWhen to worry:\")\n",
    "print(\"  - |ρ| > 0.9: Likely need more data or different test mode\")\n",
    "print(\"  - Diagonal lines in pair plot: Parameters not independently constrained\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prediction-intro",
   "metadata": {},
   "source": "## 7. Prediction Uncertainty Bands\n\n### Propagating Uncertainty to Predictions\n\nGenerate model predictions G(t) using each posterior sample, creating a distribution of predictions at each time point.\n\n**Two types of uncertainty:**\n1. **Parameter uncertainty**: From posterior distribution\n2. **Observation noise**: From σ (noise variance) samples\n\nWe visualize both components.\n\n### Residual Diagnostic Table\n\n| Residual Pattern | Interpretation | Uncertainty Band Check |\n|------------------|----------------|------------------------|\n| **Random scatter** | Model captures structure | Prediction bands should cover ~95% of data |\n| **Systematic trends** | Model misspecification | Bands narrow but miss data systematically |\n| **Heteroscedasticity** | Non-constant variance | Band width should vary appropriately |\n| **Outliers** | Data quality or model limitation | Check if outliers outside bands are spurious |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "predictions",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T21:18:05.109904Z",
     "iopub.status.busy": "2026-02-09T21:18:05.109799Z",
     "iopub.status.idle": "2026-02-09T21:18:05.474913Z",
     "shell.execute_reply": "2026-02-09T21:18:05.474486Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"PREDICTION UNCERTAINTY BANDS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Generate predictions for each posterior sample\n",
    "print(\"\\nGenerating predictions from 2000 posterior samples...\")\n",
    "t_pred = np.logspace(-2, 2, 200)  # Fine grid for smooth curves\n",
    "\n",
    "# Compute true response on prediction grid\n",
    "G_t_true_pred = Ge_true + Gm_true * np.exp(-t_pred / tau_true)\n",
    "\n",
    "# Vectorized prediction (efficient)\n",
    "n_samples = len(Ge_samples)\n",
    "G_pred_samples = np.zeros((n_samples, len(t_pred)))\n",
    "\n",
    "for i in range(n_samples):\n",
    "    # Set parameters for this sample\n",
    "    Ge_i = Ge_samples[i]\n",
    "    Gm_i = Gm_samples[i]\n",
    "    eta_i = eta_samples[i]\n",
    "    tau_i = eta_i / Gm_i\n",
    "    \n",
    "    # Zener prediction: G(t) = Ge + Gm * exp(-t/tau)\n",
    "    G_pred_samples[i, :] = Ge_i + Gm_i * np.exp(-t_pred / tau_i)\n",
    "\n",
    "# Compute uncertainty bands\n",
    "G_pred_mean = G_pred_samples.mean(axis=0)\n",
    "G_pred_median = np.median(G_pred_samples, axis=0)\n",
    "G_pred_68 = np.percentile(G_pred_samples, [16, 84], axis=0)  # 68% CI\n",
    "G_pred_95 = np.percentile(G_pred_samples, [2.5, 97.5], axis=0)  # 95% CI\n",
    "\n",
    "print(\"✓ Predictions computed\")\n",
    "print(f\"  Mean relative uncertainty: {np.mean((G_pred_95[1] - G_pred_95[0]) / G_pred_mean) * 100:.1f}%\")\n",
    "\n",
    "# Plot prediction uncertainty bands\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Left: Full view with data\n",
    "axes[0].loglog(t, G_t_noisy, 'o', markersize=6, alpha=0.7, label='Observed data', color='black', zorder=5)\n",
    "axes[0].loglog(t_pred, G_pred_mean, '-', linewidth=2.5, color='red', label='Posterior mean prediction', zorder=4)\n",
    "axes[0].fill_between(t_pred, G_pred_95[0], G_pred_95[1], alpha=0.3, color='red', \n",
    "                      label='95% credible band', zorder=3)\n",
    "axes[0].fill_between(t_pred, G_pred_68[0], G_pred_68[1], alpha=0.5, color='red', \n",
    "                      label='68% credible band', zorder=2)\n",
    "axes[0].loglog(t_pred, G_t_true_pred, '--', linewidth=2, alpha=0.5, color='green', \n",
    "              label='True response', zorder=1)\n",
    "axes[0].set_xlabel('Time (s)', fontweight='bold')\n",
    "axes[0].set_ylabel('Relaxation Modulus G(t) (Pa)', fontweight='bold')\n",
    "axes[0].set_title('Prediction Uncertainty Bands', fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3, which='both')\n",
    "\n",
    "# Right: Relative uncertainty vs time\n",
    "rel_uncertainty = (G_pred_95[1] - G_pred_95[0]) / G_pred_mean * 100\n",
    "axes[1].semilogx(t_pred, rel_uncertainty, linewidth=2, color='steelblue')\n",
    "axes[1].axhline(rel_uncertainty.mean(), color='red', linestyle='--', \n",
    "               label=f'Mean = {rel_uncertainty.mean():.1f}%')\n",
    "axes[1].set_xlabel('Time (s)', fontweight='bold')\n",
    "axes[1].set_ylabel('Relative Uncertainty (95% CI width / mean) [%]', fontweight='bold')\n",
    "axes[1].set_title('Prediction Uncertainty vs Time', fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "display(fig)\n",
    "plt.close(fig)\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"  - Credible bands quantify prediction uncertainty\")\n",
    "print(\"  - Narrower bands where data is dense\")\n",
    "print(\"  - Wider bands in extrapolation regions\")\n",
    "print(\"  - True response (green) within 95% band ✓\")\n",
    "print(\"\\nUse cases:\")\n",
    "print(f\"  - Report predictions with uncertainty: G(t=1s) = {G_pred_mean[np.argmin(np.abs(t_pred - 1.0))]:.1e} ± {(G_pred_95[1] - G_pred_95[0])[np.argmin(np.abs(t_pred - 1.0))] / 2:.1e} Pa (95% CI)\")\n",
    "print(\"  - Quality control: flag measurements outside bands\")\n",
    "print(\"  - Design decisions: use conservative estimates (lower CI bound)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "components-intro",
   "metadata": {},
   "source": [
    "## 8. Uncertainty Components: Parameter vs Observation Noise\n",
    "\n",
    "### Decomposing Total Uncertainty\n",
    "\n",
    "Total prediction uncertainty has two sources:\n",
    "1. **Epistemic (parameter) uncertainty**: From imperfect parameter knowledge\n",
    "2. **Aleatoric (observation) uncertainty**: From inherent measurement noise\n",
    "\n",
    "Understanding this decomposition informs data collection strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uncertainty-decomposition",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T21:18:05.476571Z",
     "iopub.status.busy": "2026-02-09T21:18:05.476461Z",
     "iopub.status.idle": "2026-02-09T21:18:05.745978Z",
     "shell.execute_reply": "2026-02-09T21:18:05.745228Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"UNCERTAINTY DECOMPOSITION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Extract observation noise (sigma) samples from MCMC\n",
    "# Note: sigma is not in posterior_samples (which only contains model parameters)\n",
    "# but it's available in the full MCMC samples\n",
    "mcmc_samples = result.mcmc.get_samples()\n",
    "sigma_samples = np.asarray(mcmc_samples['sigma'], dtype=np.float64)\n",
    "sigma_mean = sigma_samples.mean()\n",
    "\n",
    "print(f\"\\nObservation noise σ:\")\n",
    "print(f\"  Mean: {sigma_mean:.2e} Pa\")\n",
    "print(f\"  95% CI: [{np.percentile(sigma_samples, 2.5):.2e}, {np.percentile(sigma_samples, 97.5):.2e}] Pa\")\n",
    "\n",
    "# Component 1: Parameter uncertainty only (mean of sigma)\n",
    "G_param_width = G_pred_95[1] - G_pred_95[0]\n",
    "\n",
    "# Component 2: Add observation noise\n",
    "print(\"\\nGenerating predictions with observation noise...\")\n",
    "G_pred_with_noise = np.zeros((n_samples, len(t_pred)))\n",
    "for i in range(n_samples):\n",
    "    G_pred_with_noise[i, :] = G_pred_samples[i, :] + np.random.normal(0, sigma_samples[i], len(t_pred))\n",
    "\n",
    "G_total_95 = np.percentile(G_pred_with_noise, [2.5, 97.5], axis=0)\n",
    "G_total_width = G_total_95[1] - G_total_95[0]\n",
    "\n",
    "# Visualize decomposition\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Left: Both uncertainty components\n",
    "axes[0].loglog(t, G_t_noisy, 'o', markersize=6, alpha=0.7, label='Observed data', color='black', zorder=5)\n",
    "axes[0].loglog(t_pred, G_pred_mean, '-', linewidth=2.5, color='red', label='Posterior mean', zorder=4)\n",
    "axes[0].fill_between(t_pred, G_pred_95[0], G_pred_95[1], alpha=0.4, color='blue', \n",
    "                      label='Parameter uncertainty only', zorder=3)\n",
    "axes[0].fill_between(t_pred, G_total_95[0], G_total_95[1], alpha=0.2, color='orange', \n",
    "                      label='Total (parameter + noise)', zorder=2)\n",
    "axes[0].set_xlabel('Time (s)', fontweight='bold')\n",
    "axes[0].set_ylabel('Relaxation Modulus G(t) (Pa)', fontweight='bold')\n",
    "axes[0].set_title('Uncertainty Decomposition', fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3, which='both')\n",
    "\n",
    "# Right: Relative contribution\n",
    "param_contribution = G_param_width / G_total_width * 100\n",
    "noise_contribution = (G_total_width - G_param_width) / G_total_width * 100\n",
    "\n",
    "axes[1].semilogx(t_pred, param_contribution, linewidth=2, color='blue', label='Parameter uncertainty')\n",
    "axes[1].semilogx(t_pred, noise_contribution, linewidth=2, color='orange', label='Observation noise')\n",
    "axes[1].fill_between(t_pred, 0, param_contribution, alpha=0.3, color='blue')\n",
    "axes[1].fill_between(t_pred, param_contribution, 100, alpha=0.3, color='orange')\n",
    "axes[1].set_xlabel('Time (s)', fontweight='bold')\n",
    "axes[1].set_ylabel('Contribution to Total Uncertainty [%]', fontweight='bold')\n",
    "axes[1].set_title('Uncertainty Source Contribution', fontweight='bold')\n",
    "axes[1].set_ylim([0, 100])\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "display(fig)\n",
    "plt.close(fig)\n",
    "\n",
    "print(\"\\nMean contributions:\")\n",
    "print(f\"  Parameter uncertainty: {param_contribution.mean():.1f}%\")\n",
    "print(f\"  Observation noise:     {noise_contribution.mean():.1f}%\")\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "if param_contribution.mean() > 70:\n",
    "    print(\"  - Parameter uncertainty dominates\")\n",
    "    print(\"  - More data would significantly reduce uncertainty\")\n",
    "    print(\"  - Consider: Additional experiments, different test modes\")\n",
    "elif noise_contribution.mean() > 70:\n",
    "    print(\"  - Observation noise dominates\")\n",
    "    print(\"  - Parameters are well-constrained\")\n",
    "    print(\"  - Consider: Better instrumentation, signal processing\")\n",
    "else:\n",
    "    print(\"  - Balanced contributions from both sources\")\n",
    "    print(\"  - Both more data and better measurements would help\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complex-intro",
   "metadata": {},
   "source": [
    "## 9. Complex Rheological Quantities\n",
    "\n",
    "### Propagating Uncertainty to Timescale Ratios\n",
    "\n",
    "Many rheological decisions depend on timescale comparisons:\n",
    "- **Process time vs material relaxation time**: Is material elastic or viscous during processing?\n",
    "- **Measurement time vs relaxation time**: Is steady state reached?\n",
    "\n",
    "We demonstrate uncertainty propagation to these critical ratios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complex-quantities",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T21:18:05.747518Z",
     "iopub.status.busy": "2026-02-09T21:18:05.747402Z",
     "iopub.status.idle": "2026-02-09T21:18:05.900624Z",
     "shell.execute_reply": "2026-02-09T21:18:05.900061Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"COMPLEX DERIVED QUANTITIES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Scenario: Processing at t_process = 0.1 s\n",
    "t_process = 0.1  # seconds\n",
    "\n",
    "# Compute Deborah number: De = tau / t_process\n",
    "# De >> 1: Elastic behavior\n",
    "# De << 1: Viscous behavior\n",
    "# De ~ 1: Viscoelastic transition\n",
    "De_samples = tau_samples / t_process\n",
    "\n",
    "De_mean = De_samples.mean()\n",
    "De_ci = np.percentile(De_samples, [2.5, 97.5])\n",
    "\n",
    "print(f\"\\nDeborah Number (De = τ / t_process) at t_process = {t_process} s:\")\n",
    "print(f\"  Mean: {De_mean:.3f}\")\n",
    "print(f\"  95% CI: [{De_ci[0]:.3f}, {De_ci[1]:.3f}]\")\n",
    "\n",
    "# Classify behavior\n",
    "if De_ci[0] > 1.5:\n",
    "    behavior = \"ELASTIC (De > 1.5 with 95% confidence)\"\n",
    "elif De_ci[1] < 0.5:\n",
    "    behavior = \"VISCOUS (De < 0.5 with 95% confidence)\"\n",
    "else:\n",
    "    behavior = \"VISCOELASTIC (De ~ 1, uncertain classification)\"\n",
    "\n",
    "print(f\"\\nMaterial behavior during processing: {behavior}\")\n",
    "\n",
    "# Compute modulus ratio: Gm/Ge (elasticity contrast)\n",
    "# High ratio: Strong solid-like component\n",
    "# Low ratio: Weak relaxation\n",
    "modulus_ratio_samples = Gm_samples / Ge_samples\n",
    "modulus_ratio_mean = modulus_ratio_samples.mean()\n",
    "modulus_ratio_ci = np.percentile(modulus_ratio_samples, [2.5, 97.5])\n",
    "\n",
    "print(f\"\\nModulus Ratio (Gm/Ge):\")\n",
    "print(f\"  Mean: {modulus_ratio_mean:.2f}\")\n",
    "print(f\"  95% CI: [{modulus_ratio_ci[0]:.2f}, {modulus_ratio_ci[1]:.2f}]\")\n",
    "print(f\"  Interpretation: Maxwell arm is {modulus_ratio_mean:.1f}x stronger than equilibrium modulus\")\n",
    "\n",
    "# Visualize distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Deborah number\n",
    "axes[0].hist(De_samples, bins=50, density=True, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "axes[0].axvline(De_mean, color='red', linestyle='--', linewidth=2, label=f'Mean = {De_mean:.3f}')\n",
    "axes[0].axvline(De_ci[0], color='black', linestyle=':', linewidth=1.5)\n",
    "axes[0].axvline(De_ci[1], color='black', linestyle=':', linewidth=1.5, label='95% CI')\n",
    "axes[0].axvline(1.0, color='green', linestyle='-', linewidth=2, alpha=0.5, label='De = 1 (transition)')\n",
    "axes[0].fill_betweenx([0, axes[0].get_ylim()[1]], 0.5, 1.5, alpha=0.2, color='yellow', \n",
    "                       label='Viscoelastic region')\n",
    "axes[0].set_xlabel('Deborah Number De = τ / t_process', fontweight='bold')\n",
    "axes[0].set_ylabel('Posterior density', fontweight='bold')\n",
    "axes[0].set_title('Deborah Number Distribution', fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Modulus ratio\n",
    "axes[1].hist(modulus_ratio_samples, bins=50, density=True, alpha=0.7, color='coral', edgecolor='black')\n",
    "axes[1].axvline(modulus_ratio_mean, color='red', linestyle='--', linewidth=2, \n",
    "               label=f'Mean = {modulus_ratio_mean:.2f}')\n",
    "axes[1].axvline(modulus_ratio_ci[0], color='black', linestyle=':', linewidth=1.5)\n",
    "axes[1].axvline(modulus_ratio_ci[1], color='black', linestyle=':', linewidth=1.5, label='95% CI')\n",
    "axes[1].set_xlabel('Modulus Ratio Gm/Ge', fontweight='bold')\n",
    "axes[1].set_ylabel('Posterior density', fontweight='bold')\n",
    "axes[1].set_title('Elasticity Contrast Distribution', fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "display(fig)\n",
    "plt.close(fig)\n",
    "\n",
    "print(\"\\nDecision-making with uncertainty:\")\n",
    "print(f\"  - Can we confidently say material is elastic at t={t_process}s?\")\n",
    "if De_ci[0] > 1.0:\n",
    "    print(f\"    YES: De > 1 with 97.5% confidence\")\n",
    "elif De_ci[1] < 1.0:\n",
    "    print(f\"    NO: De < 1 with 97.5% confidence\")\n",
    "else:\n",
    "    print(f\"    UNCERTAIN: 95% CI spans De=1 transition\")\n",
    "    print(f\"    Recommendation: Collect more data or test at different timescales\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reporting-intro",
   "metadata": {},
   "source": [
    "## 10. Reporting Guidelines: Communicating Uncertainty\n",
    "\n",
    "### Best Practices for Scientific Communication\n",
    "\n",
    "Always report parameter estimates with uncertainty quantification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reporting",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T21:18:05.901831Z",
     "iopub.status.busy": "2026-02-09T21:18:05.901731Z",
     "iopub.status.idle": "2026-02-09T21:18:05.909860Z",
     "shell.execute_reply": "2026-02-09T21:18:05.909413Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"PARAMETER SUMMARY TABLE (FOR PUBLICATION)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create summary table\n",
    "summary_data = []\n",
    "for param_name in ['Ge', 'Gm', 'eta']:\n",
    "    samples = result.posterior_samples[param_name]\n",
    "    mean = samples.mean()\n",
    "    std = samples.std()\n",
    "    median = np.median(samples)\n",
    "    ci_95 = np.percentile(samples, [2.5, 97.5])\n",
    "    \n",
    "    # Determine units\n",
    "    if param_name in ['Ge', 'Gm']:\n",
    "        unit = 'Pa'\n",
    "    elif param_name == 'eta':\n",
    "        unit = 'Pa·s'\n",
    "    else:\n",
    "        unit = ''\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Parameter': param_name,\n",
    "        'Mean': f\"{mean:.3e}\",\n",
    "        'Std': f\"{std:.3e}\",\n",
    "        'Median': f\"{median:.3e}\",\n",
    "        '95% CI Lower': f\"{ci_95[0]:.3e}\",\n",
    "        '95% CI Upper': f\"{ci_95[1]:.3e}\",\n",
    "        'Units': unit\n",
    "    })\n",
    "\n",
    "# Add derived quantity\n",
    "summary_data.append({\n",
    "    'Parameter': 'τ (derived)',\n",
    "    'Mean': f\"{tau_mean:.4f}\",\n",
    "    'Std': f\"{tau_std:.4f}\",\n",
    "    'Median': f\"{tau_median:.4f}\",\n",
    "    '95% CI Lower': f\"{tau_ci_95[0]:.4f}\",\n",
    "    '95% CI Upper': f\"{tau_ci_95[1]:.4f}\",\n",
    "    'Units': 's'\n",
    "})\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\n\" + summary_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CONVERGENCE DIAGNOSTICS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nMCMC Settings:\")\n",
    "print(f\"  Warmup samples: 1000\")\n",
    "print(f\"  Sampling iterations: 2000\")\n",
    "print(f\"  Number of chains: 1\")\n",
    "print(f\"\\nConvergence metrics:\")\n",
    "for param in ['Ge', 'Gm', 'eta']:\n",
    "    print(f\"  {param}: R-hat = {result.diagnostics['r_hat'][param]:.4f}, ESS = {result.diagnostics['ess'][param]:.0f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RECOMMENDED REPORTING FORMAT\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nExample text for publication:\")\n",
    "print('\"\"\"')\n",
    "print(\"We performed Bayesian inference using No-U-Turn Sampler (NUTS) with 1000 warmup\")\n",
    "print(\"and 2000 sampling iterations. All parameters exhibited convergence (R-hat < 1.01)\")\n",
    "print(\"and sufficient effective sample size (ESS > 400). The fitted Zener model parameters\")\n",
    "print(\"are reported as posterior means with 95% credible intervals:\")\n",
    "print(\"\")\n",
    "print(f\"  - Equilibrium modulus: Ge = {Ge_samples.mean():.2e} Pa \")\n",
    "print(f\"    (95% CI: [{np.percentile(Ge_samples, 2.5):.2e}, {np.percentile(Ge_samples, 97.5):.2e}] Pa)\")\n",
    "print(f\"  - Maxwell arm modulus: Gm = {Gm_samples.mean():.2e} Pa \")\n",
    "print(f\"    (95% CI: [{np.percentile(Gm_samples, 2.5):.2e}, {np.percentile(Gm_samples, 97.5):.2e}] Pa)\")\n",
    "print(f\"  - Viscosity: η = {eta_samples.mean():.2e} Pa·s \")\n",
    "print(f\"    (95% CI: [{np.percentile(eta_samples, 2.5):.2e}, {np.percentile(eta_samples, 97.5):.2e}] Pa·s)\")\n",
    "print(\"\")\n",
    "print(f\"The characteristic relaxation time τ = η/Gm was {tau_mean:.4f} s \")\n",
    "print(f\"(95% CI: [{tau_ci_95[0]:.4f}, {tau_ci_95[1]:.4f}] s).\")\n",
    "print('\"\"\"')\n",
    "\n",
    "print(\"\\nVisualization recommendations:\")\n",
    "print(\"  - Always show prediction uncertainty bands (95% CI)\")\n",
    "print(\"  - Use transparent shading for credible regions\")\n",
    "print(\"  - Report both point estimates and intervals in captions\")\n",
    "print(\"  - Include sample size and convergence metrics in methods\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a63283cbaf04dbcab1f6479b197f3a8",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "- **Gelman et al. (2013)**: [*Bayesian Data Analysis*](http://www.stat.columbia.edu/~gelman/book/) — Chapter 7 on posterior predictive checks\n",
    "- **McElreath (2020)**: *Statistical Rethinking* — Chapter 4 on uncertainty propagation through transformations\n",
    "- **ArviZ Posterior Predictive**: [Documentation](https://python.arviz.org/en/stable/api/plots.html#arviz.plot_ppc) — Tools for posterior predictive visualization\n",
    "- **NumPyro Predictive**: [API Reference](https://num.pyro.ai/en/stable/utilities.html#numpyro.infer.Predictive) — Posterior sampling utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd0d8092fe74a7c96281538738b07e2",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- **[06-bayesian_workflow_demo.ipynb](06-bayesian_workflow_demo.ipynb)**: Complete end-to-end Bayesian workflow demonstration\n",
    "- **[07-gmm_bayesian_workflow.ipynb](07-gmm_bayesian_workflow.ipynb)**: Uncertainty propagation for complex multi-mode models\n",
    "- Apply posterior predictive checks to validate your own model predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "key-takeaways",
   "metadata": {},
   "source": "- **[../basic/03-springpot-fitting.ipynb](../basic/03-springpot-fitting.ipynb)**: Fractional derivatives with uncertainty\n\n### Key References\n\n- **McElreath, R. (2020).** *Statistical Rethinking*. 2nd ed. CRC Press. [Chapter 3: Sampling from the posterior - Monte Carlo uncertainty propagation]\n- **Gelman, A. et al. (2013).** *Bayesian Data Analysis*. 3rd ed. CRC Press. [Chapter 6: Model checking and posterior predictive inference]\n- **Kruschke, J.K. (2014).** *Doing Bayesian Data Analysis*. 2nd ed. Academic Press. [Chapter 25: Reporting Bayesian results with uncertainty]"
  },
  {
   "cell_type": "markdown",
   "id": "session-info",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Session Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "session-info-code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T21:18:05.911271Z",
     "iopub.status.busy": "2026-02-09T21:18:05.911194Z",
     "iopub.status.idle": "2026-02-09T21:18:05.913415Z",
     "shell.execute_reply": "2026-02-09T21:18:05.912947Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import rheojax\n",
    "\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"RheoJAX: {rheojax.__version__}\")\n",
    "print(f\"JAX: {jax.__version__}\")\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "print(f\"ArviZ: {az.__version__}\")\n",
    "print(f\"JAX devices: {jax.devices()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rheojax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}