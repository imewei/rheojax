{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# Uncertainty Propagation: From Posterior Samples to Predictions\n",
    "\n",
    "This notebook demonstrates how to propagate parameter uncertainty through to predictions, derived quantities, and credible intervals using Bayesian posterior samples.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "After completing this notebook, you will be able to:\n",
    "- Compute credible intervals for derived rheological quantities\n",
    "- Propagate uncertainty from parameters to model predictions\n",
    "- Diagnose parameter identifiability using correlations\n",
    "- Analyze uncertainty in complex quantities (relaxation time, loss tangent)\n",
    "- Generate prediction bands with quantified uncertainty\n",
    "- Use posterior samples for uncertainty quantification in any transformation\n",
    "- Apply uncertainty propagation to rheological decision-making\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Understanding of Bayesian inference (`01-bayesian-basics.ipynb`)\n",
    "- Prior selection concepts (`02-prior-selection.ipynb`)\n",
    "- Convergence diagnostics (`03-convergence-diagnostics.ipynb`)\n",
    "- Familiarity with Zener model (4 parameters: Ge, Gm, eta)\n",
    "\n",
    "**Estimated Time:** 40-45 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "## 1. Introduction: Beyond Point Estimates\n",
    "\n",
    "### The Uncertainty Propagation Problem\n",
    "\n",
    "After Bayesian inference, we have posterior distributions for model parameters. But rheologists need more:\n",
    "- **Relaxation time τ = η/G₀**: What is its uncertainty?\n",
    "- **Predictions**: What are credible intervals for G(t) at new time points?\n",
    "- **Derived quantities**: How uncertain is the loss tangent tan(δ)?\n",
    "\n",
    "### Point Estimates Are Insufficient\n",
    "\n",
    "**NLSQ Approach (Inadequate):**\n",
    "```python\n",
    "# NLSQ fit\n",
    "model.fit(t, G_t)\n",
    "G0_fit = model.parameters.get_value('G0')  # Single value\n",
    "eta_fit = model.parameters.get_value('eta')  # Single value\n",
    "tau_fit = eta_fit / G0_fit  # Single value - but how certain?\n",
    "```\n",
    "\n",
    "**Problem:** τ uncertainty depends on both G₀ and η uncertainties AND their correlation.\n",
    "\n",
    "**Bayesian Approach (Complete):**\n",
    "```python\n",
    "# Bayesian inference\n",
    "result = model.fit_bayesian(t, G_t, ...)\n",
    "G0_samples = result.posterior_samples['G0']  # 2000 samples\n",
    "eta_samples = result.posterior_samples['eta']  # 2000 samples\n",
    "\n",
    "# Propagate uncertainty to derived quantity\n",
    "tau_samples = eta_samples / G0_samples  # 2000 samples\n",
    "\n",
    "# Full distribution, not just point estimate\n",
    "tau_mean = tau_samples.mean()\n",
    "tau_ci = np.percentile(tau_samples, [2.5, 97.5])\n",
    "```\n",
    "\n",
    "### Uncertainty Propagation Principle\n",
    "\n",
    "For any function f(θ) of parameters θ:\n",
    "1. Extract posterior samples: θ₁, θ₂, ..., θₙ\n",
    "2. Apply function: f(θ₁), f(θ₂), ..., f(θₙ)\n",
    "3. Analyze distribution of f(θ)\n",
    "\n",
    "**Monte Carlo propagation automatically accounts for:**\n",
    "- Parameter uncertainties\n",
    "- Parameter correlations\n",
    "- Nonlinear transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## 2. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Rheo imports\n",
    "from rheojax.models.zener import Zener\n",
    "from rheojax.core.jax_config import safe_import_jax\n",
    "\n",
    "# ArviZ for diagnostics\n",
    "import arviz as az\n",
    "\n",
    "# Safe JAX import\n",
    "jax, jnp = safe_import_jax()\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Plotting\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-intro",
   "metadata": {},
   "source": [
    "## 3. Generate Synthetic Data: Zener Model with Correlations\n",
    "\n",
    "We generate data from a **Zener model** (3 parameters) to demonstrate:\n",
    "- Parameter correlations (Gm and η affect same timescale)\n",
    "- Derived quantity uncertainty (relaxation time τ = η/Gm)\n",
    "- Prediction uncertainty bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# True Zener parameters\n",
    "Ge_true = 1e4    # Equilibrium modulus (Pa)\n",
    "Gm_true = 5e4    # Maxwell arm modulus (Pa)\n",
    "eta_true = 1e3   # Viscosity (Pa·s)\n",
    "tau_true = eta_true / Gm_true  # Relaxation time (s)\n",
    "\n",
    "print(\"True Zener Parameters:\")\n",
    "print(f\"  Ge  = {Ge_true:.2e} Pa (equilibrium modulus)\")\n",
    "print(f\"  Gm  = {Gm_true:.2e} Pa (Maxwell arm)\")\n",
    "print(f\"  η   = {eta_true:.2e} Pa·s\")\n",
    "print(f\"  τ   = {tau_true:.4f} s (characteristic time)\\n\")\n",
    "\n",
    "# Time array\n",
    "t = np.logspace(-2, 2, 60)  # 0.01 to 100 s\n",
    "\n",
    "# True Zener relaxation modulus\n",
    "# G(t) = Ge + Gm * exp(-t / tau)\n",
    "G_t_true = Ge_true + Gm_true * np.exp(-t / tau_true)\n",
    "\n",
    "# Add realistic noise (3%)\n",
    "noise_level = 0.03\n",
    "noise = np.random.normal(0, noise_level * G_t_true)\n",
    "G_t_noisy = G_t_true + noise\n",
    "\n",
    "print(f\"Data: {len(t)} points from {t.min():.2f} to {t.max():.2f} s\")\n",
    "print(f\"Noise: {noise_level*100:.0f}% relative\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.loglog(t, G_t_noisy, 'o', markersize=6, alpha=0.7, label='Synthetic data (Zener + noise)', color='black')\n",
    "plt.loglog(t, G_t_true, '--', linewidth=2, alpha=0.5, label='True Zener response', color='red')\n",
    "plt.axhline(Ge_true, color='blue', linestyle=':', linewidth=1.5, alpha=0.5, \n",
    "            label=f'Equilibrium modulus Ge = {Ge_true:.0e} Pa')\n",
    "plt.axvline(tau_true, color='green', linestyle=':', linewidth=1.5, alpha=0.5,\n",
    "            label=f'Relaxation time τ = {tau_true:.4f} s')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Relaxation Modulus G(t) (Pa)')\n",
    "plt.title('Stress Relaxation Data (Zener Model)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, which='both')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey features:\")\n",
    "print(\"  - Exponential decay from (Ge + Gm) to Ge\")\n",
    "print(\"  - Characteristic time τ where G(τ) ≈ Ge + Gm/e\")\n",
    "print(\"  - Finite equilibrium modulus (viscoelastic solid)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fit-intro",
   "metadata": {},
   "source": [
    "## 4. Fit Zener Model: NLSQ → NUTS Workflow\n",
    "\n",
    "We perform two-stage fitting:\n",
    "1. **NLSQ**: Fast point estimate with warm-start values\n",
    "2. **Bayesian NUTS**: Full posterior with uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fit-nlsq",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"STAGE 1: NLSQ OPTIMIZATION (POINT ESTIMATES)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create and configure Zener model\n",
    "model = Zener()\n",
    "model.parameters.set_bounds('Ge', (1e2, 1e6))\n",
    "model.parameters.set_bounds('Gm', (1e3, 1e7))\n",
    "model.parameters.set_bounds('eta', (1e1, 1e5))\n",
    "\n",
    "# NLSQ fit\n",
    "import time\n",
    "start_nlsq = time.time()\n",
    "model.fit(t, G_t_noisy)\n",
    "time_nlsq = time.time() - start_nlsq\n",
    "\n",
    "# Extract point estimates\n",
    "Ge_nlsq = model.parameters.get_value('Ge')\n",
    "Gm_nlsq = model.parameters.get_value('Gm')\n",
    "eta_nlsq = model.parameters.get_value('eta')\n",
    "tau_nlsq = eta_nlsq / Gm_nlsq\n",
    "\n",
    "print(f\"\\nNLSQ Point Estimates (converged in {time_nlsq:.3f}s):\")\n",
    "print(f\"  Ge  = {Ge_nlsq:.3e} Pa\")\n",
    "print(f\"  Gm  = {Gm_nlsq:.3e} Pa\")\n",
    "print(f\"  η   = {eta_nlsq:.3e} Pa·s\")\n",
    "print(f\"  τ   = {tau_nlsq:.4f} s (derived)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STAGE 2: BAYESIAN INFERENCE (UNCERTAINTY QUANTIFICATION)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Bayesian inference with warm-start\n",
    "print(\"\\nRunning NUTS sampling with warm-start from NLSQ...\")\n",
    "start_bayes = time.time()\n",
    "result = model.fit_bayesian(\n",
    "    t, G_t_noisy,\n",
    "    num_warmup=1000,\n",
    "    num_samples=2000,\n",
    "    num_chains=1,\n",
    "    initial_values={\n",
    "        'Ge': Ge_nlsq,\n",
    "        'Gm': Gm_nlsq,\n",
    "        'eta': eta_nlsq\n",
    "    }\n",
    ")\n",
    "time_bayes = time.time() - start_bayes\n",
    "\n",
    "print(f\"\\n✓ Bayesian inference complete ({time_bayes:.2f}s)\")\n",
    "print(f\"  Speedup from warm-start: ~{time_bayes/time_nlsq:.1f}x slower than NLSQ (but with full uncertainty)\")\n",
    "print(f\"  R-hat: {max(result.diagnostics['r_hat'].values()):.4f} < 1.01 ✓\")\n",
    "print(f\"  ESS:   {min(result.diagnostics['ess'].values()):.0f} > 400 ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "derived-intro",
   "metadata": {},
   "source": [
    "## 5. Derived Quantity: Relaxation Time τ\n",
    "\n",
    "### Computing Uncertainty in τ = η/Gm\n",
    "\n",
    "The relaxation time τ is a derived quantity that depends on two parameters. We propagate uncertainty through this transformation using posterior samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compute-tau",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract posterior samples\n",
    "Ge_samples = result.posterior_samples['Ge']\n",
    "Gm_samples = result.posterior_samples['Gm']\n",
    "eta_samples = result.posterior_samples['eta']\n",
    "\n",
    "# Compute derived quantity: relaxation time τ = η/Gm\n",
    "tau_samples = eta_samples / Gm_samples\n",
    "\n",
    "# Summary statistics\n",
    "tau_mean = tau_samples.mean()\n",
    "tau_std = tau_samples.std()\n",
    "tau_median = np.median(tau_samples)\n",
    "tau_ci_68 = np.percentile(tau_samples, [16, 84])  # 68% (1σ)\n",
    "tau_ci_95 = np.percentile(tau_samples, [2.5, 97.5])  # 95% (2σ)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"RELAXATION TIME τ = η/Gm (DERIVED QUANTITY)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nPoint estimate (NLSQ):\")\n",
    "print(f\"  τ = {tau_nlsq:.4f} s\")\n",
    "print(f\"\\nBayesian uncertainty quantification:\")\n",
    "print(f\"  Mean:     {tau_mean:.4f} s\")\n",
    "print(f\"  Median:   {tau_median:.4f} s\")\n",
    "print(f\"  Std:      {tau_std:.4f} s\")\n",
    "print(f\"  68% CI:   [{tau_ci_68[0]:.4f}, {tau_ci_68[1]:.4f}] s\")\n",
    "print(f\"  95% CI:   [{tau_ci_95[0]:.4f}, {tau_ci_95[1]:.4f}] s\")\n",
    "print(f\"\\nTrue value: {tau_true:.4f} s (within 95% CI? {tau_ci_95[0] < tau_true < tau_ci_95[1]})\")\n",
    "\n",
    "# Visualize posterior distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Histogram with statistics\n",
    "axes[0].hist(tau_samples, bins=50, density=True, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "axes[0].axvline(tau_mean, color='red', linestyle='--', linewidth=2, label=f'Mean = {tau_mean:.4f} s')\n",
    "axes[0].axvline(tau_ci_95[0], color='black', linestyle=':', linewidth=1.5, label=f'95% CI')\n",
    "axes[0].axvline(tau_ci_95[1], color='black', linestyle=':', linewidth=1.5)\n",
    "axes[0].axvline(tau_true, color='green', linestyle='-', linewidth=2, label=f'True = {tau_true:.4f} s')\n",
    "axes[0].fill_between([tau_ci_95[0], tau_ci_95[1]], 0, axes[0].get_ylim()[1], \n",
    "                      alpha=0.2, color='gray', label='95% credible region')\n",
    "axes[0].set_xlabel('Relaxation time τ (s)', fontweight='bold')\n",
    "axes[0].set_ylabel('Posterior density', fontweight='bold')\n",
    "axes[0].set_title('Posterior Distribution of τ = η/Gm', fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Comparison with NLSQ point estimate\n",
    "axes[1].violinplot([tau_samples], positions=[0], widths=0.7, showmeans=True, showmedians=True)\n",
    "axes[1].scatter([0], [tau_nlsq], s=200, c='orange', marker='D', \n",
    "               edgecolors='black', linewidths=2, label='NLSQ point estimate', zorder=5)\n",
    "axes[1].scatter([0], [tau_true], s=200, c='green', marker='*', \n",
    "               edgecolors='black', linewidths=2, label='True value', zorder=5)\n",
    "axes[1].set_xlim([-0.5, 0.5])\n",
    "axes[1].set_xticks([0])\n",
    "axes[1].set_xticklabels(['τ'])\n",
    "axes[1].set_ylabel('Relaxation time τ (s)', fontweight='bold')\n",
    "axes[1].set_title('NLSQ vs Bayesian Uncertainty', fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"  - NLSQ gives single value (orange diamond)\")\n",
    "print(\"  - Bayesian gives full distribution (blue violin)\")\n",
    "print(f\"  - Relative uncertainty: {tau_std/tau_mean*100:.1f}%\")\n",
    "print(\"  - 95% CI captures true value ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "correlation-intro",
   "metadata": {},
   "source": [
    "## 6. Parameter Identifiability: Correlation Analysis\n",
    "\n",
    "### Why Correlations Matter\n",
    "\n",
    "Strong parameter correlations indicate **identifiability issues**:\n",
    "- Data cannot independently constrain both parameters\n",
    "- Multiple parameter combinations fit data equally well\n",
    "- Derived quantities may still be well-constrained\n",
    "\n",
    "### Interpreting Correlations\n",
    "\n",
    "- **|ρ| < 0.3**: Well-identified (data constrains parameters independently)\n",
    "- **0.3 < |ρ| < 0.7**: Moderate correlation (acceptable for most applications)\n",
    "- **|ρ| > 0.7**: Strong correlation (identifiability issue, may need more/different data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correlations",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"PARAMETER CORRELATION ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Compute correlation matrix\n",
    "param_df = pd.DataFrame({\n",
    "    'Ge': Ge_samples,\n",
    "    'Gm': Gm_samples,\n",
    "    'eta': eta_samples\n",
    "})\n",
    "correlation = param_df.corr()\n",
    "\n",
    "print(\"\\nCorrelation matrix:\")\n",
    "print(correlation.to_string())\n",
    "\n",
    "# Identify strong correlations\n",
    "print(\"\\nCorrelation interpretation:\")\n",
    "for i, param_i in enumerate(['Ge', 'Gm', 'eta']):\n",
    "    for j, param_j in enumerate(['Ge', 'Gm', 'eta']):\n",
    "        if i < j:\n",
    "            rho = correlation.loc[param_i, param_j]\n",
    "            if abs(rho) < 0.3:\n",
    "                status = \"✓ Well-identified (weak correlation)\"\n",
    "            elif abs(rho) < 0.7:\n",
    "                status = \"⚠ Moderate correlation (acceptable)\"\n",
    "            else:\n",
    "                status = \"✗ Strong correlation (identifiability issue)\"\n",
    "            print(f\"  {param_i} - {param_j}: ρ = {rho:+.3f}  {status}\")\n",
    "\n",
    "# Visualize correlations with ArviZ pair plot\n",
    "print(\"\\nGenerating pair plot (joint distributions)...\")\n",
    "idata = result.to_inference_data()\n",
    "az.plot_pair(\n",
    "    idata, \n",
    "    var_names=['Ge', 'Gm', 'eta'],\n",
    "    kind='scatter',\n",
    "    marginals=True,\n",
    "    figsize=(10, 8)\n",
    ")\n",
    "plt.suptitle('Parameter Correlations (Pair Plot)', fontweight='bold', y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPhysical interpretation:\")\n",
    "print(\"  - Gm and η correlation expected (both control relaxation time τ = η/Gm)\")\n",
    "print(\"  - Ge weakly correlated (independent: long-time plateau)\")\n",
    "print(\"  - Moderate correlations are normal for viscoelastic models\")\n",
    "print(\"\\nWhen to worry:\")\n",
    "print(\"  - |ρ| > 0.9: Likely need more data or different test mode\")\n",
    "print(\"  - Diagonal lines in pair plot: Parameters not independently constrained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prediction-intro",
   "metadata": {},
   "source": [
    "## 7. Prediction Uncertainty Bands\n",
    "\n",
    "### Propagating Uncertainty to Predictions\n",
    "\n",
    "Generate model predictions G(t) using each posterior sample, creating a distribution of predictions at each time point.\n",
    "\n",
    "**Two types of uncertainty:**\n",
    "1. **Parameter uncertainty**: From posterior distribution\n",
    "2. **Observation noise**: From σ (noise variance) samples\n",
    "\n",
    "We visualize both components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "predictions",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"PREDICTION UNCERTAINTY BANDS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Generate predictions for each posterior sample\n",
    "print(\"\\nGenerating predictions from 2000 posterior samples...\")\n",
    "t_pred = np.logspace(-2, 2, 200)  # Fine grid for smooth curves\n",
    "\n",
    "# Vectorized prediction (efficient)\n",
    "n_samples = len(Ge_samples)\n",
    "G_pred_samples = np.zeros((n_samples, len(t_pred)))\n",
    "\n",
    "for i in range(n_samples):\n",
    "    # Set parameters for this sample\n",
    "    Ge_i = Ge_samples[i]\n",
    "    Gm_i = Gm_samples[i]\n",
    "    eta_i = eta_samples[i]\n",
    "    tau_i = eta_i / Gm_i\n",
    "    \n",
    "    # Zener prediction: G(t) = Ge + Gm * exp(-t/tau)\n",
    "    G_pred_samples[i, :] = Ge_i + Gm_i * np.exp(-t_pred / tau_i)\n",
    "\n",
    "# Compute uncertainty bands\n",
    "G_pred_mean = G_pred_samples.mean(axis=0)\n",
    "G_pred_median = np.median(G_pred_samples, axis=0)\n",
    "G_pred_68 = np.percentile(G_pred_samples, [16, 84], axis=0)  # 68% CI\n",
    "G_pred_95 = np.percentile(G_pred_samples, [2.5, 97.5], axis=0)  # 95% CI\n",
    "\n",
    "print(\"✓ Predictions computed\")\n",
    "print(f\"  Mean relative uncertainty: {np.mean((G_pred_95[1] - G_pred_95[0]) / G_pred_mean) * 100:.1f}%\")\n",
    "\n",
    "# Plot prediction uncertainty bands\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Left: Full view with data\n",
    "axes[0].loglog(t, G_t_noisy, 'o', markersize=6, alpha=0.7, label='Observed data', color='black', zorder=5)\n",
    "axes[0].loglog(t_pred, G_pred_mean, '-', linewidth=2.5, color='red', label='Posterior mean prediction', zorder=4)\n",
    "axes[0].fill_between(t_pred, G_pred_95[0], G_pred_95[1], alpha=0.3, color='red', \n",
    "                      label='95% credible band', zorder=3)\n",
    "axes[0].fill_between(t_pred, G_pred_68[0], G_pred_68[1], alpha=0.5, color='red', \n",
    "                      label='68% credible band', zorder=2)\n",
    "axes[0].loglog(t_pred, G_t_true, '--', linewidth=2, alpha=0.5, color='green', \n",
    "              label='True response', zorder=1)\n",
    "axes[0].set_xlabel('Time (s)', fontweight='bold')\n",
    "axes[0].set_ylabel('Relaxation Modulus G(t) (Pa)', fontweight='bold')\n",
    "axes[0].set_title('Prediction Uncertainty Bands', fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3, which='both')\n",
    "\n",
    "# Right: Relative uncertainty vs time\n",
    "rel_uncertainty = (G_pred_95[1] - G_pred_95[0]) / G_pred_mean * 100\n",
    "axes[1].semilogx(t_pred, rel_uncertainty, linewidth=2, color='steelblue')\n",
    "axes[1].axhline(rel_uncertainty.mean(), color='red', linestyle='--', \n",
    "               label=f'Mean = {rel_uncertainty.mean():.1f}%')\n",
    "axes[1].set_xlabel('Time (s)', fontweight='bold')\n",
    "axes[1].set_ylabel('Relative Uncertainty (95% CI width / mean) [%]', fontweight='bold')\n",
    "axes[1].set_title('Prediction Uncertainty vs Time', fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"  - Credible bands quantify prediction uncertainty\")\n",
    "print(\"  - Narrower bands where data is dense\")\n",
    "print(\"  - Wider bands in extrapolation regions\")\n",
    "print(\"  - True response (green) within 95% band ✓\")\n",
    "print(\"\\nUse cases:\")\n",
    "print(\"  - Report predictions with uncertainty: G(t=1s) = {:.1e} ± {:.1e} Pa (95% CI)\".format(\n",
    "    G_pred_mean[np.argmin(np.abs(t_pred - 1.0))],\n",
    "    (G_pred_95[1] - G_pred_95[0])[np.argmin(np.abs(t_pred - 1.0))] / 2\n",
    "))\n",
    "print(\"  - Quality control: flag measurements outside bands\")\n",
    "print(\"  - Design decisions: use conservative estimates (lower CI bound)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "components-intro",
   "metadata": {},
   "source": [
    "## 8. Uncertainty Components: Parameter vs Observation Noise\n",
    "\n",
    "### Decomposing Total Uncertainty\n",
    "\n",
    "Total prediction uncertainty has two sources:\n",
    "1. **Epistemic (parameter) uncertainty**: From imperfect parameter knowledge\n",
    "2. **Aleatoric (observation) uncertainty**: From inherent measurement noise\n",
    "\n",
    "Understanding this decomposition informs data collection strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uncertainty-decomposition",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"UNCERTAINTY DECOMPOSITION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Extract observation noise (sigma) samples\n",
    "sigma_samples = result.posterior_samples['sigma']\n",
    "sigma_mean = sigma_samples.mean()\n",
    "\n",
    "print(f\"\\nObservation noise σ:\")\n",
    "print(f\"  Mean: {sigma_mean:.2e} Pa\")\n",
    "print(f\"  95% CI: [{np.percentile(sigma_samples, 2.5):.2e}, {np.percentile(sigma_samples, 97.5):.2e}] Pa\")\n",
    "\n",
    "# Component 1: Parameter uncertainty only (mean of sigma)\n",
    "G_param_width = G_pred_95[1] - G_pred_95[0]\n",
    "\n",
    "# Component 2: Add observation noise\n",
    "print(\"\\nGenerating predictions with observation noise...\")\n",
    "G_pred_with_noise = np.zeros((n_samples, len(t_pred)))\n",
    "for i in range(n_samples):\n",
    "    G_pred_with_noise[i, :] = G_pred_samples[i, :] + np.random.normal(0, sigma_samples[i], len(t_pred))\n",
    "\n",
    "G_total_95 = np.percentile(G_pred_with_noise, [2.5, 97.5], axis=0)\n",
    "G_total_width = G_total_95[1] - G_total_95[0]\n",
    "\n",
    "# Visualize decomposition\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Left: Both uncertainty components\n",
    "axes[0].loglog(t, G_t_noisy, 'o', markersize=6, alpha=0.7, label='Observed data', color='black', zorder=5)\n",
    "axes[0].loglog(t_pred, G_pred_mean, '-', linewidth=2.5, color='red', label='Posterior mean', zorder=4)\n",
    "axes[0].fill_between(t_pred, G_pred_95[0], G_pred_95[1], alpha=0.4, color='blue', \n",
    "                      label='Parameter uncertainty only', zorder=3)\n",
    "axes[0].fill_between(t_pred, G_total_95[0], G_total_95[1], alpha=0.2, color='orange', \n",
    "                      label='Total (parameter + noise)', zorder=2)\n",
    "axes[0].set_xlabel('Time (s)', fontweight='bold')\n",
    "axes[0].set_ylabel('Relaxation Modulus G(t) (Pa)', fontweight='bold')\n",
    "axes[0].set_title('Uncertainty Decomposition', fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3, which='both')\n",
    "\n",
    "# Right: Relative contribution\n",
    "param_contribution = G_param_width / G_total_width * 100\n",
    "noise_contribution = (G_total_width - G_param_width) / G_total_width * 100\n",
    "\n",
    "axes[1].semilogx(t_pred, param_contribution, linewidth=2, color='blue', label='Parameter uncertainty')\n",
    "axes[1].semilogx(t_pred, noise_contribution, linewidth=2, color='orange', label='Observation noise')\n",
    "axes[1].fill_between(t_pred, 0, param_contribution, alpha=0.3, color='blue')\n",
    "axes[1].fill_between(t_pred, param_contribution, 100, alpha=0.3, color='orange')\n",
    "axes[1].set_xlabel('Time (s)', fontweight='bold')\n",
    "axes[1].set_ylabel('Contribution to Total Uncertainty [%]', fontweight='bold')\n",
    "axes[1].set_title('Uncertainty Source Contribution', fontweight='bold')\n",
    "axes[1].set_ylim([0, 100])\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nMean contributions:\")\n",
    "print(f\"  Parameter uncertainty: {param_contribution.mean():.1f}%\")\n",
    "print(f\"  Observation noise:     {noise_contribution.mean():.1f}%\")\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "if param_contribution.mean() > 70:\n",
    "    print(\"  - Parameter uncertainty dominates\")\n",
    "    print(\"  - More data would significantly reduce uncertainty\")\n",
    "    print(\"  - Consider: Additional experiments, different test modes\")\n",
    "elif noise_contribution.mean() > 70:\n",
    "    print(\"  - Observation noise dominates\")\n",
    "    print(\"  - Parameters are well-constrained\")\n",
    "    print(\"  - Consider: Better instrumentation, signal processing\")\n",
    "else:\n",
    "    print(\"  - Balanced contributions from both sources\")\n",
    "    print(\"  - Both more data and better measurements would help\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complex-intro",
   "metadata": {},
   "source": [
    "## 9. Complex Rheological Quantities\n",
    "\n",
    "### Propagating Uncertainty to Timescale Ratios\n",
    "\n",
    "Many rheological decisions depend on timescale comparisons:\n",
    "- **Process time vs material relaxation time**: Is material elastic or viscous during processing?\n",
    "- **Measurement time vs relaxation time**: Is steady state reached?\n",
    "\n",
    "We demonstrate uncertainty propagation to these critical ratios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complex-quantities",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"COMPLEX DERIVED QUANTITIES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Scenario: Processing at t_process = 0.1 s\n",
    "t_process = 0.1  # seconds\n",
    "\n",
    "# Compute Deborah number: De = tau / t_process\n",
    "# De >> 1: Elastic behavior\n",
    "# De << 1: Viscous behavior\n",
    "# De ~ 1: Viscoelastic transition\n",
    "De_samples = tau_samples / t_process\n",
    "\n",
    "De_mean = De_samples.mean()\n",
    "De_ci = np.percentile(De_samples, [2.5, 97.5])\n",
    "\n",
    "print(f\"\\nDeborah Number (De = τ / t_process) at t_process = {t_process} s:\")\n",
    "print(f\"  Mean: {De_mean:.3f}\")\n",
    "print(f\"  95% CI: [{De_ci[0]:.3f}, {De_ci[1]:.3f}]\")\n",
    "\n",
    "# Classify behavior\n",
    "if De_ci[0] > 1.5:\n",
    "    behavior = \"ELASTIC (De > 1.5 with 95% confidence)\"\n",
    "elif De_ci[1] < 0.5:\n",
    "    behavior = \"VISCOUS (De < 0.5 with 95% confidence)\"\n",
    "else:\n",
    "    behavior = \"VISCOELASTIC (De ~ 1, uncertain classification)\"\n",
    "\n",
    "print(f\"\\nMaterial behavior during processing: {behavior}\")\n",
    "\n",
    "# Compute modulus ratio: Gm/Ge (elasticity contrast)\n",
    "# High ratio: Strong solid-like component\n",
    "# Low ratio: Weak relaxation\n",
    "modulus_ratio_samples = Gm_samples / Ge_samples\n",
    "modulus_ratio_mean = modulus_ratio_samples.mean()\n",
    "modulus_ratio_ci = np.percentile(modulus_ratio_samples, [2.5, 97.5])\n",
    "\n",
    "print(f\"\\nModulus Ratio (Gm/Ge):\")\n",
    "print(f\"  Mean: {modulus_ratio_mean:.2f}\")\n",
    "print(f\"  95% CI: [{modulus_ratio_ci[0]:.2f}, {modulus_ratio_ci[1]:.2f}]\")\n",
    "print(f\"  Interpretation: Maxwell arm is {modulus_ratio_mean:.1f}x stronger than equilibrium modulus\")\n",
    "\n",
    "# Visualize distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Deborah number\n",
    "axes[0].hist(De_samples, bins=50, density=True, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "axes[0].axvline(De_mean, color='red', linestyle='--', linewidth=2, label=f'Mean = {De_mean:.3f}')\n",
    "axes[0].axvline(De_ci[0], color='black', linestyle=':', linewidth=1.5)\n",
    "axes[0].axvline(De_ci[1], color='black', linestyle=':', linewidth=1.5, label='95% CI')\n",
    "axes[0].axvline(1.0, color='green', linestyle='-', linewidth=2, alpha=0.5, label='De = 1 (transition)')\n",
    "axes[0].fill_betweenx([0, axes[0].get_ylim()[1]], 0.5, 1.5, alpha=0.2, color='yellow', \n",
    "                       label='Viscoelastic region')\n",
    "axes[0].set_xlabel('Deborah Number De = τ / t_process', fontweight='bold')\n",
    "axes[0].set_ylabel('Posterior density', fontweight='bold')\n",
    "axes[0].set_title('Deborah Number Distribution', fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Modulus ratio\n",
    "axes[1].hist(modulus_ratio_samples, bins=50, density=True, alpha=0.7, color='coral', edgecolor='black')\n",
    "axes[1].axvline(modulus_ratio_mean, color='red', linestyle='--', linewidth=2, \n",
    "               label=f'Mean = {modulus_ratio_mean:.2f}')\n",
    "axes[1].axvline(modulus_ratio_ci[0], color='black', linestyle=':', linewidth=1.5)\n",
    "axes[1].axvline(modulus_ratio_ci[1], color='black', linestyle=':', linewidth=1.5, label='95% CI')\n",
    "axes[1].set_xlabel('Modulus Ratio Gm/Ge', fontweight='bold')\n",
    "axes[1].set_ylabel('Posterior density', fontweight='bold')\n",
    "axes[1].set_title('Elasticity Contrast Distribution', fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nDecision-making with uncertainty:\")\n",
    "print(f\"  - Can we confidently say material is elastic at t={t_process}s?\")\n",
    "if De_ci[0] > 1.0:\n",
    "    print(f\"    YES: De > 1 with 97.5% confidence\")\n",
    "elif De_ci[1] < 1.0:\n",
    "    print(f\"    NO: De < 1 with 97.5% confidence\")\n",
    "else:\n",
    "    print(f\"    UNCERTAIN: 95% CI spans De=1 transition\")\n",
    "    print(f\"    Recommendation: Collect more data or test at different timescales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reporting-intro",
   "metadata": {},
   "source": [
    "## 10. Reporting Guidelines: Communicating Uncertainty\n",
    "\n",
    "### Best Practices for Scientific Communication\n",
    "\n",
    "Always report parameter estimates with uncertainty quantification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reporting",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"PARAMETER SUMMARY TABLE (FOR PUBLICATION)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create summary table\n",
    "summary_data = []\n",
    "for param_name in ['Ge', 'Gm', 'eta']:\n",
    "    samples = result.posterior_samples[param_name]\n",
    "    mean = samples.mean()\n",
    "    std = samples.std()\n",
    "    median = np.median(samples)\n",
    "    ci_95 = np.percentile(samples, [2.5, 97.5])\n",
    "    \n",
    "    # Determine units\n",
    "    if param_name in ['Ge', 'Gm']:\n",
    "        unit = 'Pa'\n",
    "    elif param_name == 'eta':\n",
    "        unit = 'Pa·s'\n",
    "    else:\n",
    "        unit = ''\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Parameter': param_name,\n",
    "        'Mean': f\"{mean:.3e}\",\n",
    "        'Std': f\"{std:.3e}\",\n",
    "        'Median': f\"{median:.3e}\",\n",
    "        '95% CI Lower': f\"{ci_95[0]:.3e}\",\n",
    "        '95% CI Upper': f\"{ci_95[1]:.3e}\",\n",
    "        'Units': unit\n",
    "    })\n",
    "\n",
    "# Add derived quantity\n",
    "summary_data.append({\n",
    "    'Parameter': 'τ (derived)',\n",
    "    'Mean': f\"{tau_mean:.4f}\",\n",
    "    'Std': f\"{tau_std:.4f}\",\n",
    "    'Median': f\"{tau_median:.4f}\",\n",
    "    '95% CI Lower': f\"{tau_ci_95[0]:.4f}\",\n",
    "    '95% CI Upper': f\"{tau_ci_95[1]:.4f}\",\n",
    "    'Units': 's'\n",
    "})\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\n\" + summary_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CONVERGENCE DIAGNOSTICS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nMCMC Settings:\")\n",
    "print(f\"  Warmup samples: 1000\")\n",
    "print(f\"  Sampling iterations: 2000\")\n",
    "print(f\"  Number of chains: 1\")\n",
    "print(f\"\\nConvergence metrics:\")\n",
    "for param in ['Ge', 'Gm', 'eta']:\n",
    "    print(f\"  {param}: R-hat = {result.diagnostics['r_hat'][param]:.4f}, ESS = {result.diagnostics['ess'][param]:.0f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RECOMMENDED REPORTING FORMAT\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nExample text for publication:\")\n",
    "print('\"\"\"')\n",
    "print(\"We performed Bayesian inference using No-U-Turn Sampler (NUTS) with 1000 warmup\")\n",
    "print(\"and 2000 sampling iterations. All parameters exhibited convergence (R-hat < 1.01)\")\n",
    "print(\"and sufficient effective sample size (ESS > 400). The fitted Zener model parameters\")\n",
    "print(\"are reported as posterior means with 95% credible intervals:\")\n",
    "print(\"\")\n",
    "print(f\"  - Equilibrium modulus: Ge = {Ge_samples.mean():.2e} Pa \")\n",
    "print(f\"    (95% CI: [{np.percentile(Ge_samples, 2.5):.2e}, {np.percentile(Ge_samples, 97.5):.2e}] Pa)\")\n",
    "print(f\"  - Maxwell arm modulus: Gm = {Gm_samples.mean():.2e} Pa \")\n",
    "print(f\"    (95% CI: [{np.percentile(Gm_samples, 2.5):.2e}, {np.percentile(Gm_samples, 97.5):.2e}] Pa)\")\n",
    "print(f\"  - Viscosity: η = {eta_samples.mean():.2e} Pa·s \")\n",
    "print(f\"    (95% CI: [{np.percentile(eta_samples, 2.5):.2e}, {np.percentile(eta_samples, 97.5):.2e}] Pa·s)\")\n",
    "print(\"\")\n",
    "print(f\"The characteristic relaxation time τ = η/Gm was {tau_mean:.4f} s \")\n",
    "print(f\"(95% CI: [{tau_ci_95[0]:.4f}, {tau_ci_95[1]:.4f}] s).\")\n",
    "print('\"\"\"')\n",
    "\n",
    "print(\"\\nVisualization recommendations:\")\n",
    "print(\"  - Always show prediction uncertainty bands (95% CI)\")\n",
    "print(\"  - Use transparent shading for credible regions\")\n",
    "print(\"  - Report both point estimates and intervals in captions\")\n",
    "print(\"  - Include sample size and convergence metrics in methods\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "key-takeaways",
   "metadata": {},
   "source": [
    "## 11. Key Takeaways\n",
    "\n",
    "### Main Concepts\n",
    "\n",
    "1. **Uncertainty Propagation via Posterior Samples:**\n",
    "   - For any function f(θ), compute f(θ_i) for all posterior samples θ_i\n",
    "   - Automatically accounts for parameter uncertainties AND correlations\n",
    "   - Handles nonlinear transformations correctly (no linearization needed)\n",
    "\n",
    "2. **Parameter Identifiability:**\n",
    "   - |ρ| < 0.3: Well-identified parameters\n",
    "   - 0.3 < |ρ| < 0.7: Acceptable correlation\n",
    "   - |ρ| > 0.7: Identifiability issue (may need more/different data)\n",
    "   - Pair plots reveal correlation structure visually\n",
    "\n",
    "3. **Prediction Uncertainty Bands:**\n",
    "   - Include parameter uncertainty (epistemic)\n",
    "   - Can add observation noise (aleatoric) for total uncertainty\n",
    "   - Essential for reporting predictions with confidence\n",
    "   - Quantifies where data collection is most needed\n",
    "\n",
    "4. **Derived Quantities:**\n",
    "   - Relaxation time τ = η/Gm with full uncertainty\n",
    "   - Deborah number De = τ/t_process for behavior classification\n",
    "   - Modulus ratios for material characterization\n",
    "   - Any rheological quantity can be uncertainty-quantified\n",
    "\n",
    "5. **Decision-Making Under Uncertainty:**\n",
    "   - Use credible intervals for confident claims\n",
    "   - Report when uncertainty prevents definitive conclusions\n",
    "   - Uncertainty quantification guides data collection strategy\n",
    "   - Conservative estimates use lower CI bound\n",
    "\n",
    "### Practical Workflow\n",
    "\n",
    "```python\n",
    "# 1. Bayesian inference\n",
    "result = model.fit_bayesian(t, y, num_samples=2000, ...)\n",
    "\n",
    "# 2. Extract posterior samples\n",
    "theta_samples = result.posterior_samples['theta']\n",
    "\n",
    "# 3. Compute derived quantity\n",
    "f_samples = my_function(theta_samples)\n",
    "\n",
    "# 4. Summary statistics\n",
    "f_mean = f_samples.mean()\n",
    "f_ci = np.percentile(f_samples, [2.5, 97.5])\n",
    "\n",
    "# 5. Report with uncertainty\n",
    "print(f\"f = {f_mean:.3e} (95% CI: [{f_ci[0]:.3e}, {f_ci[1]:.3e}])\")\n",
    "```\n",
    "\n",
    "### Common Pitfalls\n",
    "\n",
    "1. **Ignoring Correlations:**\n",
    "   - ✗ Propagating uncertainties independently: Δf² = (∂f/∂θ₁)²Δθ₁² + ...\n",
    "   - ✓ Using posterior samples: f(θ₁, θ₂) for all samples preserves correlations\n",
    "\n",
    "2. **Using Only Point Estimates:**\n",
    "   - ✗ Reporting single fitted value without uncertainty\n",
    "   - ✓ Reporting mean/median with credible intervals\n",
    "\n",
    "3. **Forgetting Observation Noise:**\n",
    "   - Parameter uncertainty: Uncertainty about model parameters\n",
    "   - Observation noise: Inherent measurement variability\n",
    "   - Both contribute to total prediction uncertainty\n",
    "\n",
    "4. **Overinterpreting Weak Data:**\n",
    "   - If 95% CI is very wide, data doesn't strongly constrain parameter\n",
    "   - Solution: More data, tighter priors, or different test mode\n",
    "\n",
    "### When Uncertainty Propagation is Essential\n",
    "\n",
    "- ✓ Derived quantities (relaxation times, Deborah numbers)\n",
    "- ✓ Predictions at new conditions\n",
    "- ✓ Model comparison (credible intervals overlap?)\n",
    "- ✓ Quality control (is measurement within expected range?)\n",
    "- ✓ Publication (reviewers expect uncertainty quantification)\n",
    "\n",
    "### Performance Notes\n",
    "\n",
    "- Computing derived quantities is fast (simple transformations)\n",
    "- Generating prediction bands scales with num_samples × num_pred_points\n",
    "- Use vectorization (NumPy/JAX operations) for efficiency\n",
    "- For very complex functions, consider thinning posterior samples\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "### Apply to Your Data\n",
    "- Propagate uncertainty to your critical derived quantities\n",
    "- Generate prediction bands for quality control\n",
    "- Use uncertainty to inform data collection strategy\n",
    "\n",
    "### Advanced Topics\n",
    "- **[advanced/01-multi-technique-fitting.ipynb](../advanced/01-multi-technique-fitting.ipynb)**: Combine multiple test modes with uncertainty\n",
    "- **[advanced/02-batch-processing.ipynb](../advanced/02-batch-processing.ipynb)**: Uncertainty across multiple samples\n",
    "- **[advanced/04-fractional-models-deep-dive.ipynb](../advanced/04-fractional-models-deep-dive.ipynb)**: Uncertainty in fractional parameters\n",
    "\n",
    "### Related Notebooks\n",
    "- **[01-bayesian-basics.ipynb](01-bayesian-basics.ipynb)**: Foundation of Bayesian inference\n",
    "- **[03-convergence-diagnostics.ipynb](03-convergence-diagnostics.ipynb)**: Ensure reliable posteriors\n",
    "- **[04-model-comparison.ipynb](04-model-comparison.ipynb)**: Select best model before propagating uncertainty\n",
    "\n",
    "### Further Reading\n",
    "- McElreath (2020): \"Statistical Rethinking\" - Monte Carlo uncertainty propagation (Chapter 3)\n",
    "- Gelman et al. (2013): \"Bayesian Data Analysis\" - Posterior predictive inference (Chapter 6)\n",
    "- Kruschke (2014): \"Doing Bayesian Data Analysis\" - Reporting Bayesian results (Chapter 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "session-info",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Session Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "session-info-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import rheojax\n",
    "\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"Rheo: {rheo.__version__}\")\n",
    "print(f\"JAX: {jax.__version__}\")\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "print(f\"ArviZ: {az.__version__}\")\n",
    "print(f\"JAX devices: {jax.devices()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
