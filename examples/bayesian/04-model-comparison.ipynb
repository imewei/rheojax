{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# Bayesian Model Comparison: WAIC and LOO for Rheological Models\n",
    "\n",
    "This notebook demonstrates how to use Bayesian model comparison techniques (WAIC and LOO) to objectively select the best rheological model for your data.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "After completing this notebook, you will be able to:\n",
    "- Understand when model comparison is necessary vs optional\n",
    "- Compute WAIC (Widely Applicable Information Criterion) for model selection\n",
    "- Compute LOO (Leave-One-Out cross-validation) via Pareto-smoothed importance sampling\n",
    "- Interpret ELPD (Expected Log Predictive Density) differences\n",
    "- Use `az.compare()` for automated model ranking\n",
    "- Avoid overfitting through penalized likelihood metrics\n",
    "- Make principled model selection decisions with uncertainty quantification\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Understanding of Bayesian inference (`01-bayesian-basics.ipynb`)\n",
    "- Familiarity with Maxwell and Zener models\n",
    "- Basic understanding of likelihood and model fit\n",
    "\n",
    "**Estimated Time:** 35-40 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "## 1. Introduction: The Model Selection Problem\n",
    "\n",
    "### Why Model Comparison?\n",
    "\n",
    "Rheological data can often be fit by multiple models. How do we choose?\n",
    "\n",
    "**Example Scenario:**\n",
    "- Stress relaxation data shows exponential decay\n",
    "- **Maxwell model** (2 parameters: G₀, η) fits with R² = 0.98\n",
    "- **Zener model** (3 parameters: Ge, Gm, η) fits with R² = 0.99\n",
    "- **Generalized Maxwell** (5 parameters) fits with R² = 0.995\n",
    "\n",
    "**Naive approach:** Choose model with highest R²\n",
    "**Problem:** More complex models *always* fit better (overfitting)\n",
    "\n",
    "### The Overfitting-Underfitting Trade-off\n",
    "\n",
    "**Underfitting (too simple):**\n",
    "- Model misses important features in data\n",
    "- Poor prediction on new data\n",
    "- High bias\n",
    "\n",
    "**Overfitting (too complex):**\n",
    "- Model fits noise instead of signal\n",
    "- Poor prediction on new data\n",
    "- High variance\n",
    "\n",
    "**Goal:** Find simplest model that captures true data-generating process\n",
    "\n",
    "### Bayesian Information Criteria\n",
    "\n",
    "Bayesian model comparison uses **penalized likelihood** metrics:\n",
    "- Balance goodness-of-fit with model complexity\n",
    "- Penalize additional parameters\n",
    "- Estimate out-of-sample prediction accuracy\n",
    "\n",
    "**Two gold-standard metrics:**\n",
    "1. **WAIC (Widely Applicable IC):** Asymptotically equivalent to cross-validation\n",
    "2. **LOO (Leave-One-Out CV):** Direct approximation of cross-validation via importance sampling\n",
    "\n",
    "**Lower is better:** Model with lowest WAIC/LOO has best expected predictive accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## 2. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Rheo imports\n",
    "from rheo.models.maxwell import Maxwell\n",
    "from rheo.models.zener import Zener\n",
    "from rheo.models.springpot import SpringPot\n",
    "from rheo.core.jax_config import safe_import_jax\n",
    "\n",
    "# ArviZ for model comparison\n",
    "import arviz as az\n",
    "\n",
    "# Safe JAX import\n",
    "jax, jnp = safe_import_jax()\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Plotting\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-intro",
   "metadata": {},
   "source": [
    "## 3. Generate Synthetic Data from Zener Model\n",
    "\n",
    "We generate data from a **Zener model** (3 parameters) and test whether Bayesian model comparison correctly identifies it against simpler (Maxwell) and more complex (SpringPot) alternatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# True Zener parameters\n",
    "Ge_true = 1e4    # Equilibrium modulus (Pa)\n",
    "Gm_true = 5e4    # Maxwell arm modulus (Pa)\n",
    "eta_true = 1e3   # Viscosity (Pa·s)\n",
    "tau_true = eta_true / Gm_true  # Relaxation time (s)\n",
    "\n",
    "print(\"True Zener Parameters:\")\n",
    "print(f\"  Ge  = {Ge_true:.2e} Pa (equilibrium modulus)\")\n",
    "print(f\"  Gm  = {Gm_true:.2e} Pa (Maxwell arm)\")\n",
    "print(f\"  η   = {eta_true:.2e} Pa·s\")\n",
    "print(f\"  τ   = {tau_true:.4f} s\\n\")\n",
    "\n",
    "# Time array\n",
    "t = np.logspace(-2, 2, 50)  # 0.01 to 100 s\n",
    "\n",
    "# True Zener relaxation modulus\n",
    "# G(t) = Ge + Gm * exp(-t / tau)\n",
    "G_t_true = Ge_true + Gm_true * np.exp(-t / tau_true)\n",
    "\n",
    "# Add realistic noise (2%)\n",
    "noise_level = 0.02\n",
    "noise = np.random.normal(0, noise_level * G_t_true)\n",
    "G_t_noisy = G_t_true + noise\n",
    "\n",
    "print(f\"Data: {len(t)} points from {t.min():.2f} to {t.max():.2f} s\")\n",
    "print(f\"Noise: {noise_level*100:.0f}% relative\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.loglog(t, G_t_noisy, 'o', markersize=6, alpha=0.7, label='Synthetic data (Zener + noise)')\n",
    "plt.loglog(t, G_t_true, '--', linewidth=2, alpha=0.5, label='True Zener response')\n",
    "plt.axhline(Ge_true, color='red', linestyle=':', linewidth=1.5, alpha=0.5, label=f'Equilibrium modulus Ge = {Ge_true:.0e} Pa')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Relaxation Modulus G(t) (Pa)')\n",
    "plt.title('Stress Relaxation Data (Zener Model)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, which='both')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey feature: Finite equilibrium modulus (data plateaus at Ge, does not decay to 0)\")\n",
    "print(\"This is characteristic of Zener model, NOT Maxwell (which decays to 0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fit-intro",
   "metadata": {},
   "source": [
    "## 4. Fit Three Competing Models\n",
    "\n",
    "We fit three models to the data:\n",
    "1. **Maxwell** (2 parameters: G₀, η) - simpler than true model\n",
    "2. **Zener** (3 parameters: Ge, Gm, η) - true model\n",
    "3. **SpringPot** (2 parameters: G₀, α) - different physics (fractional derivative)\n",
    "\n",
    "Each model undergoes full Bayesian inference (NLSQ → NUTS) to obtain posterior samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fit-maxwell",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"FITTING MODEL 1: MAXWELL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create and configure Maxwell model\n",
    "model_maxwell = Maxwell()\n",
    "model_maxwell.parameters.set_bounds('G0', (1e3, 1e7))\n",
    "model_maxwell.parameters.set_bounds('eta', (1e1, 1e5))\n",
    "\n",
    "# NLSQ fit\n",
    "model_maxwell.fit(t, G_t_noisy)\n",
    "\n",
    "# Bayesian inference\n",
    "print(\"Running Bayesian inference (Maxwell)...\")\n",
    "result_maxwell = model_maxwell.fit_bayesian(\n",
    "    t, G_t_noisy,\n",
    "    num_warmup=1000,\n",
    "    num_samples=2000,\n",
    "    num_chains=1,\n",
    "    initial_values={\n",
    "        'G0': model_maxwell.parameters.get_value('G0'),\n",
    "        'eta': model_maxwell.parameters.get_value('eta')\n",
    "    }\n",
    ")\n",
    "\n",
    "# Convert to InferenceData\n",
    "idata_maxwell = result_maxwell.to_inference_data()\n",
    "\n",
    "print(\"\\n✓ Maxwell model fitted\")\n",
    "print(f\"  Converged: R-hat = {max(result_maxwell.diagnostics['r_hat'].values()):.4f}\")\n",
    "print(f\"  ESS = {min(result_maxwell.diagnostics['ess'].values()):.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fit-zener",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FITTING MODEL 2: ZENER (TRUE MODEL)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create and configure Zener model\n",
    "model_zener = Zener()\n",
    "model_zener.parameters.set_bounds('Ge', (1e2, 1e6))\n",
    "model_zener.parameters.set_bounds('Gm', (1e3, 1e7))\n",
    "model_zener.parameters.set_bounds('eta', (1e1, 1e5))\n",
    "\n",
    "# NLSQ fit\n",
    "model_zener.fit(t, G_t_noisy)\n",
    "\n",
    "# Bayesian inference\n",
    "print(\"Running Bayesian inference (Zener)...\")\n",
    "result_zener = model_zener.fit_bayesian(\n",
    "    t, G_t_noisy,\n",
    "    num_warmup=1000,\n",
    "    num_samples=2000,\n",
    "    num_chains=1,\n",
    "    initial_values={\n",
    "        'Ge': model_zener.parameters.get_value('Ge'),\n",
    "        'Gm': model_zener.parameters.get_value('Gm'),\n",
    "        'eta': model_zener.parameters.get_value('eta')\n",
    "    }\n",
    ")\n",
    "\n",
    "# Convert to InferenceData\n",
    "idata_zener = result_zener.to_inference_data()\n",
    "\n",
    "print(\"\\n✓ Zener model fitted\")\n",
    "print(f\"  Converged: R-hat = {max(result_zener.diagnostics['r_hat'].values()):.4f}\")\n",
    "print(f\"  ESS = {min(result_zener.diagnostics['ess'].values()):.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fit-springpot",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FITTING MODEL 3: SPRINGPOT (FRACTIONAL)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create and configure SpringPot model\n",
    "model_springpot = SpringPot()\n",
    "model_springpot.parameters.set_bounds('G0', (1e3, 1e7))\n",
    "model_springpot.parameters.set_bounds('alpha', (0.1, 1.0))\n",
    "\n",
    "# NLSQ fit\n",
    "model_springpot.fit(t, G_t_noisy)\n",
    "\n",
    "# Bayesian inference\n",
    "print(\"Running Bayesian inference (SpringPot)...\")\n",
    "result_springpot = model_springpot.fit_bayesian(\n",
    "    t, G_t_noisy,\n",
    "    num_warmup=1000,\n",
    "    num_samples=2000,\n",
    "    num_chains=1,\n",
    "    initial_values={\n",
    "        'G0': model_springpot.parameters.get_value('G0'),\n",
    "        'alpha': model_springpot.parameters.get_value('alpha')\n",
    "    }\n",
    ")\n",
    "\n",
    "# Convert to InferenceData\n",
    "idata_springpot = result_springpot.to_inference_data()\n",
    "\n",
    "print(\"\\n✓ SpringPot model fitted\")\n",
    "print(f\"  Converged: R-hat = {max(result_springpot.diagnostics['r_hat'].values()):.4f}\")\n",
    "print(f\"  ESS = {min(result_springpot.diagnostics['ess'].values()):.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visual-compare",
   "metadata": {},
   "source": [
    "## 5. Visual Comparison of Model Fits\n",
    "\n",
    "Before computing information criteria, let's visually assess how well each model fits the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-fits",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions from each model\n",
    "t_plot = np.logspace(-2, 2, 200)\n",
    "G_maxwell = model_maxwell.predict(t_plot)\n",
    "G_zener = model_zener.predict(t_plot)\n",
    "G_springpot = model_springpot.predict(t_plot)\n",
    "\n",
    "# Plot comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Left: Model fits\n",
    "ax1.loglog(t, G_t_noisy, 'o', markersize=6, alpha=0.7, label='Data', color='black')\n",
    "ax1.loglog(t, G_t_true, ':', linewidth=2.5, alpha=0.5, label='True (Zener)', color='gray')\n",
    "ax1.loglog(t_plot, G_maxwell, '-', linewidth=2, label='Maxwell (2 params)', color='#1f77b4')\n",
    "ax1.loglog(t_plot, G_zener, '-', linewidth=2, label='Zener (3 params)', color='#ff7f0e')\n",
    "ax1.loglog(t_plot, G_springpot, '-', linewidth=2, label='SpringPot (2 params)', color='#2ca02c')\n",
    "ax1.set_xlabel('Time (s)', fontweight='bold')\n",
    "ax1.set_ylabel('Relaxation Modulus G(t) (Pa)', fontweight='bold')\n",
    "ax1.set_title('Model Comparison', fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3, which='both')\n",
    "\n",
    "# Right: Residuals\n",
    "res_maxwell = G_t_noisy - model_maxwell.predict(t)\n",
    "res_zener = G_t_noisy - model_zener.predict(t)\n",
    "res_springpot = G_t_noisy - model_springpot.predict(t)\n",
    "\n",
    "ax2.semilogx(t, res_maxwell / G_t_noisy * 100, 'o-', markersize=5, alpha=0.7, label='Maxwell')\n",
    "ax2.semilogx(t, res_zener / G_t_noisy * 100, 's-', markersize=5, alpha=0.7, label='Zener')\n",
    "ax2.semilogx(t, res_springpot / G_t_noisy * 100, '^-', markersize=5, alpha=0.7, label='SpringPot')\n",
    "ax2.axhline(0, color='black', linestyle='--', linewidth=1, alpha=0.5)\n",
    "ax2.set_xlabel('Time (s)', fontweight='bold')\n",
    "ax2.set_ylabel('Relative Residual (%)', fontweight='bold')\n",
    "ax2.set_title('Residual Analysis', fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compute RMSE for each model\n",
    "rmse_maxwell = np.sqrt(np.mean(res_maxwell**2))\n",
    "rmse_zener = np.sqrt(np.mean(res_zener**2))\n",
    "rmse_springpot = np.sqrt(np.mean(res_springpot**2))\n",
    "\n",
    "print(\"\\nRoot Mean Square Error (RMSE):\")\n",
    "print(f\"  Maxwell:   {rmse_maxwell:.2e} Pa\")\n",
    "print(f\"  Zener:     {rmse_zener:.2e} Pa  {'✓ Best' if rmse_zener < min(rmse_maxwell, rmse_springpot) else ''}\")\n",
    "print(f\"  SpringPot: {rmse_springpot:.2e} Pa\")\n",
    "print(\"\\nNote: Zener has lowest RMSE, but this doesn't account for model complexity!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "waic-intro",
   "metadata": {},
   "source": [
    "## 6. Model Comparison: WAIC and LOO\n",
    "\n",
    "### Understanding Information Criteria\n",
    "\n",
    "**WAIC (Widely Applicable Information Criterion):**\n",
    "- Approximates leave-one-out cross-validation\n",
    "- Penalizes model complexity via effective number of parameters\n",
    "- Lower WAIC = better expected out-of-sample prediction\n",
    "- Formula: WAIC = -2 × (lppd - p_WAIC)\n",
    "  - lppd: log pointwise predictive density\n",
    "  - p_WAIC: effective number of parameters\n",
    "\n",
    "**LOO (Leave-One-Out Cross-Validation):**\n",
    "- Direct approximation of leave-one-out CV via Pareto-smoothed importance sampling (PSIS)\n",
    "- More robust than WAIC for influential observations\n",
    "- Lower LOO = better expected out-of-sample prediction\n",
    "- Provides Pareto k diagnostic for problematic points\n",
    "\n",
    "**Interpretation:**\n",
    "- ΔWAIC/ΔLOO < 4: Models indistinguishable\n",
    "- ΔWAIC/ΔLOO 4-10: Moderate evidence for better model\n",
    "- ΔWAIC/ΔLOO > 10: Strong evidence for better model\n",
    "\n",
    "### ArviZ Automated Comparison\n",
    "\n",
    "ArviZ provides `az.compare()` for automated model ranking with uncertainty quantification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compute-waic-loo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary of models for comparison\n",
    "models = {\n",
    "    'Maxwell': idata_maxwell,\n",
    "    'Zener': idata_zener,\n",
    "    'SpringPot': idata_springpot\n",
    "}\n",
    "\n",
    "# Compute WAIC and LOO for all models\n",
    "print(\"=\"*70)\n",
    "print(\"BAYESIAN MODEL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nComputing WAIC and LOO for all models...\\n\")\n",
    "\n",
    "comparison = az.compare(models, ic='waic', method='stacking')\n",
    "print(comparison)\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"INTERPRETATION:\")\n",
    "print(\"-\"*70)\n",
    "print(\"Columns:\")\n",
    "print(\"  rank:   Model rank (0 = best)\")\n",
    "print(\"  elpd_*: Expected log pointwise predictive density (higher is better)\")\n",
    "print(\"  p_*:    Effective number of parameters (complexity penalty)\")\n",
    "print(\"  *_ic:   Information criterion (WAIC or LOO, lower is better)\")\n",
    "print(\"  d_*:    Difference from best model (0 for best, >10 for clearly worse)\")\n",
    "print(\"  weight: Stacking weights for model averaging\")\n",
    "print(\"  se:     Standard error of the IC\")\n",
    "print(\"  dse:    Standard error of the difference\")\n",
    "print(\"\\nRules of thumb:\")\n",
    "print(\"  d_waic < 4:  Models indistinguishable\")\n",
    "print(\"  d_waic 4-10: Moderate evidence for better model\")\n",
    "print(\"  d_waic > 10: Strong evidence for better model\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compute-loo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute LOO specifically for detailed diagnostics\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LOO CROSS-VALIDATION WITH PARETO DIAGNOSTICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Compute LOO for each model\n",
    "loo_maxwell = az.loo(idata_maxwell)\n",
    "loo_zener = az.loo(idata_zener)\n",
    "loo_springpot = az.loo(idata_springpot)\n",
    "\n",
    "print(\"\\nMaxwell LOO:\")\n",
    "print(loo_maxwell)\n",
    "\n",
    "print(\"\\nZener LOO:\")\n",
    "print(loo_zener)\n",
    "\n",
    "print(\"\\nSpringPot LOO:\")\n",
    "print(loo_springpot)\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"PARETO K DIAGNOSTIC:\")\n",
    "print(\"-\"*70)\n",
    "print(\"Pareto k estimates reliability of LOO approximation per data point:\")\n",
    "print(\"  k < 0.5:  Good (LOO approximation reliable)\")\n",
    "print(\"  0.5-0.7:  OK (LOO approximation acceptable)\")\n",
    "print(\"  0.7-1.0:  Bad (LOO approximation unreliable)\")\n",
    "print(\"  k > 1.0:  Very bad (LOO approximation fails)\")\n",
    "print(\"\\nIf many points have k > 0.7, consider K-fold CV instead.\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visual-ic",
   "metadata": {},
   "source": [
    "## 7. Visualize Model Comparison\n",
    "\n",
    "ArviZ provides publication-quality visualizations of model comparison results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-compare",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model comparison\n",
    "az.plot_compare(comparison, figsize=(10, 4))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\"\"\n",
    "INTERPRETATION - Comparison Plot:\n",
    "- Models ranked from best (top) to worst (bottom)\n",
    "- CIRCLE: ELPD (expected log predictive density, higher is better)\n",
    "- ERROR BARS: Standard error of ELPD\n",
    "- TRIANGLE: In-sample deviance (not penalized for complexity)\n",
    "\n",
    "What to look for:\n",
    "✓ Non-overlapping error bars → clear winner\n",
    "✗ Overlapping error bars → models statistically equivalent\n",
    "\n",
    "Expected result:\n",
    "- Zener should rank #1 (true model)\n",
    "- Maxwell #2 (underfit, misses equilibrium modulus)\n",
    "- SpringPot #3 (different physics, wrong functional form)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-pareto",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Pareto k diagnostics for best model\n",
    "best_model_name = comparison.index[0]\n",
    "best_idata = models[best_model_name]\n",
    "\n",
    "print(f\"\\nPareto k diagnostic for best model ({best_model_name}):\")\n",
    "az.plot_khat(best_idata, figsize=(10, 4))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\"\"\n",
    "INTERPRETATION - Pareto k Plot:\n",
    "- Each point represents one data observation\n",
    "- Y-axis: Pareto k statistic (reliability of LOO approximation)\n",
    "- Horizontal lines: Thresholds (0.5 good, 0.7 bad)\n",
    "\n",
    "What to look for:\n",
    "✓ All points below 0.5: LOO is reliable\n",
    "⚠ Few points 0.5-0.7: LOO acceptable but check those points\n",
    "✗ Many points > 0.7: LOO unreliable, use K-fold CV instead\n",
    "\n",
    "High Pareto k indicates:\n",
    "- Influential observations (large impact on posterior)\n",
    "- Outliers or poorly fit regions\n",
    "- Potential model misspecification\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weight-intro",
   "metadata": {},
   "source": [
    "## 8. Model Weights and Averaging\n",
    "\n",
    "### Stacking Weights\n",
    "\n",
    "ArviZ computes **stacking weights** - optimal weights for combining model predictions:\n",
    "- Sum to 1.0\n",
    "- Higher weight = model contributes more to ensemble\n",
    "- If one model has weight ≈ 1.0, it's clearly superior\n",
    "- If weights distributed, consider model averaging\n",
    "\n",
    "### When to Use Model Averaging\n",
    "\n",
    "**Use single best model if:**\n",
    "- Clear winner (d_WAIC > 10)\n",
    "- Stacking weight > 0.9\n",
    "- Physical interpretation matters (choose simplest adequate model)\n",
    "\n",
    "**Use model averaging if:**\n",
    "- Models statistically indistinguishable (d_WAIC < 4)\n",
    "- Stacking weights distributed (no model > 0.7)\n",
    "- Prediction accuracy paramount (not interpretation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stacking-weights",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract stacking weights\n",
    "weights = comparison['weight'].values\n",
    "model_names = comparison.index.tolist()\n",
    "\n",
    "# Visualize weights\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.bar(model_names, weights, color=['#1f77b4', '#ff7f0e', '#2ca02c'], alpha=0.7, edgecolor='black')\n",
    "ax.set_ylabel('Stacking Weight', fontweight='bold')\n",
    "ax.set_title('Model Weights for Ensemble Prediction', fontweight='bold')\n",
    "ax.set_ylim([0, 1])\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Annotate weights\n",
    "for i, (name, w) in enumerate(zip(model_names, weights)):\n",
    "    ax.text(i, w + 0.03, f'{w:.3f}', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nStacking Weights Interpretation:\")\n",
    "print(\"-\" * 50)\n",
    "for name, w in zip(model_names, weights):\n",
    "    print(f\"  {name:<12}: {w:.4f}  \", end=\"\")\n",
    "    if w > 0.9:\n",
    "        print(\"(dominant model - use this one) ✓\")\n",
    "    elif w > 0.5:\n",
    "        print(\"(major contributor)\")\n",
    "    elif w > 0.1:\n",
    "        print(\"(minor contributor)\")\n",
    "    else:\n",
    "        print(\"(negligible)\")\n",
    "\n",
    "print(\"\\nRecommendation:\")\n",
    "if weights[0] > 0.9:\n",
    "    print(f\"✓ Use {model_names[0]} (clear winner with weight {weights[0]:.3f})\")\n",
    "elif weights[0] > 0.7:\n",
    "    print(f\"✓ Prefer {model_names[0]} (dominant with weight {weights[0]:.3f})\")\n",
    "else:\n",
    "    print(f\"⚠ Consider model averaging (weights distributed across {sum(w > 0.1 for w in weights)} models)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decision",
   "metadata": {},
   "source": [
    "## 9. Final Model Selection Decision\n",
    "\n",
    "### Decision Framework\n",
    "\n",
    "Combine **statistical evidence** with **physical reasoning**:\n",
    "\n",
    "**Statistical Criteria:**\n",
    "1. WAIC/LOO comparison (d_WAIC, d_LOO)\n",
    "2. Stacking weights\n",
    "3. Pareto k diagnostics\n",
    "\n",
    "**Physical Criteria:**\n",
    "1. Model assumptions match material physics?\n",
    "2. Parameters physically interpretable?\n",
    "3. Simplicity (Occam's Razor)\n",
    "\n",
    "**Example Decision Tree:**\n",
    "```\n",
    "IF d_WAIC > 10 AND weight > 0.9:\n",
    "    → Use clear winner\n",
    "ELIF d_WAIC < 4:\n",
    "    → Models equivalent, choose simpler model (fewer parameters)\n",
    "ELIF weights distributed:\n",
    "    → Consider model averaging for predictions\n",
    "ELSE:\n",
    "    → Balance statistical evidence with physical interpretation\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-decision",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"FINAL MODEL SELECTION DECISION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Extract comparison metrics\n",
    "best_model = comparison.index[0]\n",
    "d_waic_second = comparison.loc[comparison.index[1], 'd_waic']\n",
    "weight_best = comparison.loc[best_model, 'weight']\n",
    "\n",
    "print(f\"\\nBest Model: {best_model}\")\n",
    "print(f\"  WAIC: {comparison.loc[best_model, 'waic']:.2f}\")\n",
    "print(f\"  Δ WAIC from 2nd: {d_waic_second:.2f}\")\n",
    "print(f\"  Stacking weight: {weight_best:.3f}\")\n",
    "print(f\"  Effective parameters: {comparison.loc[best_model, 'p_waic']:.2f}\")\n",
    "\n",
    "print(\"\\nStatistical Evidence:\")\n",
    "if d_waic_second > 10:\n",
    "    print(f\"  ✓ STRONG: Δ WAIC = {d_waic_second:.1f} > 10 (clear winner)\")\n",
    "elif d_waic_second > 4:\n",
    "    print(f\"  ✓ MODERATE: Δ WAIC = {d_waic_second:.1f} in [4, 10] (likely better)\")\n",
    "else:\n",
    "    print(f\"  ⚠ WEAK: Δ WAIC = {d_waic_second:.1f} < 4 (models indistinguishable)\")\n",
    "\n",
    "if weight_best > 0.9:\n",
    "    print(f\"  ✓ DOMINANT: Stacking weight {weight_best:.3f} > 0.9\")\n",
    "elif weight_best > 0.7:\n",
    "    print(f\"  ✓ MAJOR: Stacking weight {weight_best:.3f} > 0.7\")\n",
    "else:\n",
    "    print(f\"  ⚠ DISTRIBUTED: Stacking weight {weight_best:.3f} < 0.7\")\n",
    "\n",
    "print(\"\\nPhysical Reasoning:\")\n",
    "if best_model == 'Zener':\n",
    "    print(\"  ✓ Data exhibits finite equilibrium modulus (plateau at long times)\")\n",
    "    print(\"  ✓ Zener model captures this physics with spring-dashpot parallel arm\")\n",
    "    print(\"  ✓ 3 parameters provide adequate flexibility without overfitting\")\n",
    "elif best_model == 'Maxwell':\n",
    "    print(\"  ⚠ Maxwell assumes complete stress relaxation (G→0 as t→∞)\")\n",
    "    print(\"  ⚠ Data shows equilibrium modulus (G→Ge ≠ 0)\")\n",
    "    print(\"  ⚠ Model misspecification likely\")\n",
    "elif best_model == 'SpringPot':\n",
    "    print(\"  ⚠ SpringPot models power-law relaxation (different physics)\")\n",
    "    print(\"  ⚠ Data is better described by exponential + constant (Zener)\")\n",
    "    print(\"  ⚠ May fit numerically but wrong physical interpretation\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RECOMMENDATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if best_model == 'Zener' and d_waic_second > 4 and weight_best > 0.7:\n",
    "    print(f\"✓✓✓ Use {best_model} model\")\n",
    "    print(\"\\nRationale:\")\n",
    "    print(\"  - Strong statistical evidence (WAIC and stacking weights)\")\n",
    "    print(\"  - Correct physics (finite equilibrium modulus)\")\n",
    "    print(\"  - Parsimonious (3 parameters vs generalized models)\")\n",
    "    print(\"  - All parameters physically interpretable\")\n",
    "else:\n",
    "    print(f\"Use {best_model} model with caveats\")\n",
    "    print(f\"Consider collecting more data or testing additional models\")\n",
    "\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"  1. Validate on independent test data\")\n",
    "print(\"  2. Check parameter physical reasonableness\")\n",
    "print(\"  3. Examine residuals for systematic errors\")\n",
    "print(\"  4. Consider model averaging if weights distributed\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "key-takeaways",
   "metadata": {},
   "source": [
    "## 10. Key Takeaways\n",
    "\n",
    "### Main Concepts\n",
    "\n",
    "1. **Bayesian Model Comparison:**\n",
    "   - Use WAIC/LOO to balance fit quality with model complexity\n",
    "   - Lower WAIC/LOO = better expected out-of-sample prediction\n",
    "   - Automatically penalizes overfitting via effective parameter count\n",
    "\n",
    "2. **Interpretation Guidelines:**\n",
    "   - **Δ WAIC < 4:** Models statistically indistinguishable → choose simpler\n",
    "   - **Δ WAIC 4-10:** Moderate evidence for better model\n",
    "   - **Δ WAIC > 10:** Strong evidence for better model\n",
    "   - **Stacking weight > 0.9:** Clear winner, use single model\n",
    "   - **Weights distributed:** Consider model averaging\n",
    "\n",
    "3. **Pareto k Diagnostics:**\n",
    "   - Assesses reliability of LOO approximation per data point\n",
    "   - k < 0.5: Good (LOO reliable)\n",
    "   - k > 0.7: Bad (LOO unreliable, use K-fold CV)\n",
    "   - High k indicates influential observations or model misspecification\n",
    "\n",
    "4. **Decision Framework:**\n",
    "   - Combine statistical evidence with physical reasoning\n",
    "   - Prefer simplest model that adequately explains data (Occam's Razor)\n",
    "   - Ensure parameters are physically interpretable\n",
    "   - Validate on independent data when possible\n",
    "\n",
    "### When to Use Model Comparison\n",
    "\n",
    "**Essential for:**\n",
    "- ✓ Multiple candidate models with different physics\n",
    "- ✓ Uncertain about appropriate model complexity\n",
    "- ✓ Avoiding overfitting with limited data\n",
    "- ✓ Objective model selection for publication\n",
    "\n",
    "**Optional for:**\n",
    "- Well-established model for specific material class\n",
    "- Only one physically plausible model\n",
    "- Exploratory analysis (NLSQ screening sufficient)\n",
    "\n",
    "### Common Pitfalls\n",
    "\n",
    "1. **Ignoring Physical Constraints:**\n",
    "   - Statistical best model may violate physics\n",
    "   - Always check parameter physical reasonableness\n",
    "   - Simplest adequate model often preferable to complex best fit\n",
    "\n",
    "2. **Overfitting with Small Datasets:**\n",
    "   - Complex models can overfit with few data points\n",
    "   - WAIC/LOO automatically penalize this via p_WAIC\n",
    "   - If weights distributed, likely insufficient data to distinguish\n",
    "\n",
    "3. **Comparing Apples to Oranges:**\n",
    "   - Only compare models fit to **same data**\n",
    "   - Same likelihood formulation (e.g., Gaussian observation model)\n",
    "   - Same response variable (don't compare G(t) vs G'(ω) models)\n",
    "\n",
    "4. **Ignoring Pareto k Warnings:**\n",
    "   - High Pareto k → LOO unreliable\n",
    "   - Investigate influential points before trusting LOO\n",
    "   - May indicate outliers or model misspecification\n",
    "\n",
    "### ArviZ Functions Summary\n",
    "\n",
    "```python\n",
    "# Model comparison\n",
    "comparison = az.compare(models, ic='waic')  # or ic='loo'\n",
    "\n",
    "# Individual metrics\n",
    "waic = az.waic(idata)\n",
    "loo = az.loo(idata)\n",
    "\n",
    "# Visualization\n",
    "az.plot_compare(comparison)\n",
    "az.plot_khat(idata)  # Pareto k diagnostic\n",
    "```\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "### Apply to Your Data\n",
    "- Test multiple rheological models on your experimental data\n",
    "- Use WAIC/LOO to objectively select best model\n",
    "- Validate selection with physical reasoning\n",
    "\n",
    "### Advanced Topics\n",
    "- **[05-uncertainty-propagation.ipynb](05-uncertainty-propagation.ipynb)**: Propagate parameter uncertainty to predictions\n",
    "- **[advanced/01-multi-technique-fitting.ipynb](../advanced/01-multi-technique-fitting.ipynb)**: Compare models across multiple test modes\n",
    "- **[advanced/04-fractional-models-deep-dive.ipynb](../advanced/04-fractional-models-deep-dive.ipynb)**: Compare 11 fractional models\n",
    "\n",
    "### Further Reading\n",
    "- Vehtari et al. (2017): \"Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC\"\n",
    "- McElreath (2020): \"Statistical Rethinking\" (Chapter 7: Model Comparison)\n",
    "- ArviZ documentation: https://arviz-devs.github.io/arviz/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "session-info",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Session Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "session-info-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import rheo\n",
    "\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"Rheo: {rheo.__version__}\")\n",
    "print(f\"JAX: {jax.__version__}\")\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "print(f\"ArviZ: {az.__version__}\")\n",
    "print(f\"JAX devices: {jax.devices()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
