{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# Bayesian Model Comparison: WAIC and LOO for Rheological Models\n",
    "\n",
    "This notebook demonstrates how to use Bayesian model comparison techniques (WAIC and LOO) to objectively select the best rheological model for your data.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "After completing this notebook, you will be able to:\n",
    "- Understand when model comparison is necessary vs optional\n",
    "- Compute WAIC (Widely Applicable Information Criterion) for model selection\n",
    "- Compute LOO (Leave-One-Out cross-validation) via Pareto-smoothed importance sampling\n",
    "- Interpret ELPD (Expected Log Predictive Density) differences\n",
    "- Use `az.compare()` for automated model ranking\n",
    "- Avoid overfitting through penalized likelihood metrics\n",
    "- Make principled model selection decisions with uncertainty quantification\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Understanding of Bayesian inference (`01-bayesian-basics.ipynb`)\n",
    "- Familiarity with Maxwell and Zener models\n",
    "- Basic understanding of likelihood and model fit\n",
    "\n",
    "**Estimated Time:** 35-40 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab Setup - Run this cell first!\n",
    "# Skip if running locally with rheojax already installed\n",
    "\n",
    "import sys\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Install rheojax and dependencies\n",
    "    !pip install -q rheojax\n",
    "    \n",
    "    # Colab uses float32 by default - we need float64 for numerical stability\n",
    "    # This MUST be set before importing JAX\n",
    "    import os\n",
    "    os.environ['JAX_ENABLE_X64'] = 'true'\n",
    "    \n",
    "    print(\"✓ RheoJAX installed successfully!\")\n",
    "    print(\"✓ Float64 precision enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "## 1. Introduction: The Model Selection Problem\n",
    "\n",
    "### Why Model Comparison?\n",
    "\n",
    "Rheological data can often be fit by multiple models. How do we choose?\n",
    "\n",
    "**Example Scenario:**\n",
    "- Stress relaxation data shows exponential decay\n",
    "- **Maxwell model** (2 parameters: G₀, η) fits with R² = 0.98\n",
    "- **Zener model** (3 parameters: Ge, Gm, η) fits with R² = 0.99\n",
    "- **Generalized Maxwell** (5 parameters) fits with R² = 0.995\n",
    "\n",
    "**Naive approach:** Choose model with highest R²\n",
    "**Problem:** More complex models *always* fit better (overfitting)\n",
    "\n",
    "### The Overfitting-Underfitting Trade-off\n",
    "\n",
    "**Underfitting (too simple):**\n",
    "- Model misses important features in data\n",
    "- Poor prediction on new data\n",
    "- High bias\n",
    "\n",
    "**Overfitting (too complex):**\n",
    "- Model fits noise instead of signal\n",
    "- Poor prediction on new data\n",
    "- High variance\n",
    "\n",
    "**Goal:** Find simplest model that captures true data-generating process\n",
    "\n",
    "### Bayesian Information Criteria\n",
    "\n",
    "Bayesian model comparison uses **penalized likelihood** metrics:\n",
    "- Balance goodness-of-fit with model complexity\n",
    "- Penalize additional parameters\n",
    "- Estimate out-of-sample prediction accuracy\n",
    "\n",
    "**Two gold-standard metrics:**\n",
    "1. **WAIC (Widely Applicable IC):** Asymptotically equivalent to cross-validation\n",
    "2. **LOO (Leave-One-Out CV):** Direct approximation of cross-validation via importance sampling\n",
    "\n",
    "**Lower is better:** Model with lowest WAIC/LOO has best expected predictive accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## 2. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure matplotlib for inline plotting in VS Code/Jupyter\n",
    "# Configure matplotlib for inline plotting in VS Code/Jupyter\n",
    "# MUST come before importing matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "\n",
    "# ArviZ for model comparison\n",
    "import arviz as az\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Rheo imports\n",
    "from rheojax.models import Maxwell\n",
    "from rheojax.models import SpringPot\n",
    "from rheojax.models import Zener\n",
    "\n",
    "from rheojax.core.jax_config import safe_import_jax\n",
    "\n",
    "# Safe JAX import\n",
    "jax, jnp = safe_import_jax()\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Plotting\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"✓ Imports successful\")\n",
    "\n",
    "# Suppress matplotlib backend warning in VS Code\n",
    "warnings.filterwarnings('ignore', message='.*non-interactive.*')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-intro",
   "metadata": {},
   "source": [
    "## 3. Generate Synthetic Data from Zener Model\n",
    "\n",
    "We generate data from a **Zener model** (3 parameters) and test whether Bayesian model comparison correctly identifies it against simpler (Maxwell) and more complex (SpringPot) alternatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# True Zener parameters\n",
    "Ge_true = 1e4    # Equilibrium modulus (Pa)\n",
    "Gm_true = 5e4    # Maxwell arm modulus (Pa)\n",
    "eta_true = 1e3   # Viscosity (Pa·s)\n",
    "tau_true = eta_true / Gm_true  # Relaxation time (s)\n",
    "\n",
    "print(\"True Zener Parameters:\")\n",
    "print(f\"  Ge  = {Ge_true:.2e} Pa (equilibrium modulus)\")\n",
    "print(f\"  Gm  = {Gm_true:.2e} Pa (Maxwell arm)\")\n",
    "print(f\"  η   = {eta_true:.2e} Pa·s\")\n",
    "print(f\"  τ   = {tau_true:.4f} s\\n\")\n",
    "\n",
    "# Time array\n",
    "t = np.logspace(-2, 2, 50)  # 0.01 to 100 s\n",
    "\n",
    "# True Zener relaxation modulus\n",
    "# G(t) = Ge + Gm * exp(-t / tau)\n",
    "G_t_true = Ge_true + Gm_true * np.exp(-t / tau_true)\n",
    "\n",
    "# Add realistic noise (2%)\n",
    "noise_level = 0.02\n",
    "noise = np.random.normal(0, noise_level * G_t_true)\n",
    "G_t_noisy = G_t_true + noise\n",
    "\n",
    "print(f\"Data: {len(t)} points from {t.min():.2f} to {t.max():.2f} s\")\n",
    "print(f\"Noise: {noise_level*100:.0f}% relative\")\n",
    "\n",
    "# Visualize\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "plt.loglog(t, G_t_noisy, 'o', markersize=6, alpha=0.7, label='Synthetic data (Zener + noise)')\n",
    "plt.loglog(t, G_t_true, '--', linewidth=2, alpha=0.5, label='True Zener response')\n",
    "plt.axhline(Ge_true, color='red', linestyle=':', linewidth=1.5, alpha=0.5, label=f'Equilibrium modulus Ge = {Ge_true:.0e} Pa')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Relaxation Modulus G(t) (Pa)')\n",
    "plt.title('Stress Relaxation Data (Zener Model)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, which='both')\n",
    "plt.tight_layout()\n",
    "display(fig)\n",
    "plt.close(fig)\n",
    "\n",
    "print(\"\\nKey feature: Finite equilibrium modulus (data plateaus at Ge, does not decay to 0)\")\n",
    "print(\"This is characteristic of Zener model, NOT Maxwell (which decays to 0)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fit-intro",
   "metadata": {},
   "source": [
    "## 4. Fit Three Competing Models\n",
    "\n",
    "We fit three models to the data:\n",
    "1. **Maxwell** (2 parameters: G₀, η) - simpler than true model\n",
    "2. **Zener** (3 parameters: Ge, Gm, η) - true model\n",
    "3. **SpringPot** (2 parameters: G₀, α) - different physics (fractional derivative)\n",
    "\n",
    "Each model undergoes full Bayesian inference (NLSQ → NUTS) to obtain posterior samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fit-maxwell",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"FITTING MODEL 1: MAXWELL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create and configure Maxwell model\n",
    "model_maxwell = Maxwell()\n",
    "model_maxwell.parameters.set_bounds('G0', (1e3, 1e7))\n",
    "model_maxwell.parameters.set_bounds('eta', (1e1, 1e5))\n",
    "\n",
    "# NLSQ fit\n",
    "model_maxwell.fit(t, G_t_noisy)\n",
    "\n",
    "# Bayesian inference\n",
    "print(\"Running Bayesian inference (Maxwell)...\")\n",
    "result_maxwell = model_maxwell.fit_bayesian(\n",
    "    t, G_t_noisy,\n",
    "    num_warmup=1000,\n",
    "    num_samples=2000,\n",
    "    num_chains=1,\n",
    "    initial_values={\n",
    "        'G0': model_maxwell.parameters.get_value('G0'),\n",
    "        'eta': model_maxwell.parameters.get_value('eta')\n",
    "    }\n",
    ")\n",
    "\n",
    "# Convert to InferenceData\n",
    "idata_maxwell = result_maxwell.to_inference_data()\n",
    "\n",
    "print(\"\\n✓ Maxwell model fitted\")\n",
    "print(f\"  Converged: R-hat = {max(result_maxwell.diagnostics['r_hat'].values()):.4f}\")\n",
    "print(f\"  ESS = {min(result_maxwell.diagnostics['ess'].values()):.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fit-zener",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FITTING MODEL 2: ZENER (TRUE MODEL)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create and configure Zener model\n",
    "model_zener = Zener()\n",
    "model_zener.parameters.set_bounds('Ge', (1e2, 1e6))\n",
    "model_zener.parameters.set_bounds('Gm', (1e3, 1e7))\n",
    "model_zener.parameters.set_bounds('eta', (1e1, 1e5))\n",
    "\n",
    "# NLSQ fit\n",
    "model_zener.fit(t, G_t_noisy)\n",
    "\n",
    "# Bayesian inference\n",
    "print(\"Running Bayesian inference (Zener)...\")\n",
    "result_zener = model_zener.fit_bayesian(\n",
    "    t, G_t_noisy,\n",
    "    num_warmup=1000,\n",
    "    num_samples=2000,\n",
    "    num_chains=1,\n",
    "    initial_values={\n",
    "        'Ge': model_zener.parameters.get_value('Ge'),\n",
    "        'Gm': model_zener.parameters.get_value('Gm'),\n",
    "        'eta': model_zener.parameters.get_value('eta')\n",
    "    }\n",
    ")\n",
    "\n",
    "# Convert to InferenceData\n",
    "idata_zener = result_zener.to_inference_data()\n",
    "\n",
    "print(\"\\n✓ Zener model fitted\")\n",
    "print(f\"  Converged: R-hat = {max(result_zener.diagnostics['r_hat'].values()):.4f}\")\n",
    "print(f\"  ESS = {min(result_zener.diagnostics['ess'].values()):.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fit-springpot",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FITTING MODEL 3: SPRINGPOT (FRACTIONAL)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create and configure SpringPot model\n",
    "model_springpot = SpringPot()\n",
    "model_springpot.parameters.set_bounds('c_alpha', (1e-3, 1e9))\n",
    "model_springpot.parameters.set_bounds('alpha', (0.1, 1.0))\n",
    "\n",
    "# NLSQ fit\n",
    "model_springpot.fit(t, G_t_noisy)\n",
    "\n",
    "# Bayesian inference\n",
    "print(\"Running Bayesian inference (SpringPot)...\")\n",
    "result_springpot = model_springpot.fit_bayesian(\n",
    "    t, G_t_noisy,\n",
    "    num_warmup=1000,\n",
    "    num_samples=2000,\n",
    "    num_chains=1,\n",
    "    initial_values={\n",
    "        'c_alpha': model_springpot.parameters.get_value('c_alpha'),\n",
    "        'alpha': model_springpot.parameters.get_value('alpha')\n",
    "    }\n",
    ")\n",
    "\n",
    "# Convert to InferenceData\n",
    "idata_springpot = result_springpot.to_inference_data()\n",
    "\n",
    "print(\"\\n✓ SpringPot model fitted\")\n",
    "print(f\"  Converged: R-hat = {max(result_springpot.diagnostics['r_hat'].values()):.4f}\")\n",
    "print(f\"  ESS = {min(result_springpot.diagnostics['ess'].values()):.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visual-compare",
   "metadata": {},
   "source": [
    "## 5. Visual Comparison of Model Fits\n",
    "\n",
    "Before computing information criteria, let's visually assess how well each model fits the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-fits",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions from each model\n",
    "t_plot = np.logspace(-2, 2, 200)\n",
    "G_maxwell = model_maxwell.predict(t_plot)\n",
    "G_zener = model_zener.predict(t_plot)\n",
    "G_springpot = model_springpot.predict(t_plot)\n",
    "\n",
    "# Plot comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Left: Model fits\n",
    "ax1.loglog(t, G_t_noisy, 'o', markersize=6, alpha=0.7, label='Data', color='black')\n",
    "ax1.loglog(t, G_t_true, ':', linewidth=2.5, alpha=0.5, label='True (Zener)', color='gray')\n",
    "ax1.loglog(t_plot, G_maxwell, '-', linewidth=2, label='Maxwell (2 params)', color='#1f77b4')\n",
    "ax1.loglog(t_plot, G_zener, '-', linewidth=2, label='Zener (3 params)', color='#ff7f0e')\n",
    "ax1.loglog(t_plot, G_springpot, '-', linewidth=2, label='SpringPot (2 params)', color='#2ca02c')\n",
    "ax1.set_xlabel('Time (s)', fontweight='bold')\n",
    "ax1.set_ylabel('Relaxation Modulus G(t) (Pa)', fontweight='bold')\n",
    "ax1.set_title('Model Comparison', fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3, which='both')\n",
    "\n",
    "# Right: Residuals\n",
    "res_maxwell = G_t_noisy - model_maxwell.predict(t)\n",
    "res_zener = G_t_noisy - model_zener.predict(t)\n",
    "res_springpot = G_t_noisy - model_springpot.predict(t)\n",
    "\n",
    "ax2.semilogx(t, res_maxwell / G_t_noisy * 100, 'o-', markersize=5, alpha=0.7, label='Maxwell')\n",
    "ax2.semilogx(t, res_zener / G_t_noisy * 100, 's-', markersize=5, alpha=0.7, label='Zener')\n",
    "ax2.semilogx(t, res_springpot / G_t_noisy * 100, '^-', markersize=5, alpha=0.7, label='SpringPot')\n",
    "ax2.axhline(0, color='black', linestyle='--', linewidth=1, alpha=0.5)\n",
    "ax2.set_xlabel('Time (s)', fontweight='bold')\n",
    "ax2.set_ylabel('Relative Residual (%)', fontweight='bold')\n",
    "ax2.set_title('Residual Analysis', fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "display(fig)\n",
    "plt.close(fig)\n",
    "\n",
    "# Compute RMSE for each model\n",
    "rmse_maxwell = np.sqrt(np.mean(res_maxwell**2))\n",
    "rmse_zener = np.sqrt(np.mean(res_zener**2))\n",
    "rmse_springpot = np.sqrt(np.mean(res_springpot**2))\n",
    "\n",
    "print(\"\\nRoot Mean Square Error (RMSE):\")\n",
    "print(f\"  Maxwell:   {rmse_maxwell:.2e} Pa\")\n",
    "print(f\"  Zener:     {rmse_zener:.2e} Pa  {'✓ Best' if rmse_zener < min(rmse_maxwell, rmse_springpot) else ''}\")\n",
    "print(f\"  SpringPot: {rmse_springpot:.2e} Pa\")\n",
    "print(\"\\nNote: Zener has lowest RMSE, but this doesn't account for model complexity!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "waic-intro",
   "metadata": {},
   "source": [
    "## 6. Model Comparison: WAIC and LOO\n",
    "\n",
    "### Understanding Information Criteria\n",
    "\n",
    "**WAIC (Widely Applicable Information Criterion):**\n",
    "- Approximates leave-one-out cross-validation\n",
    "- Penalizes model complexity via effective number of parameters\n",
    "- Lower WAIC = better expected out-of-sample prediction\n",
    "- Formula: WAIC = -2 × (lppd - p_WAIC)\n",
    "  - lppd: log pointwise predictive density\n",
    "  - p_WAIC: effective number of parameters\n",
    "\n",
    "**LOO (Leave-One-Out Cross-Validation):**\n",
    "- Direct approximation of leave-one-out CV via Pareto-smoothed importance sampling (PSIS)\n",
    "- More robust than WAIC for influential observations\n",
    "- Lower LOO = better expected out-of-sample prediction\n",
    "- Provides Pareto k diagnostic for problematic points\n",
    "\n",
    "**Interpretation:**\n",
    "- Δ ELPD < 4: Models indistinguishable\n",
    "- Δ ELPD 4-10: Moderate evidence for better model\n",
    "- Δ ELPD > 10: Strong evidence for better model\n",
    "\n",
    "**Note on ArviZ Output:**\n",
    "- ArviZ reports ELPD (Expected Log Pointwise Predictive Density)\n",
    "- With `scale='deviance'`: ELPD values are in deviance scale (-2 × log likelihood)\n",
    "- Column `elpd_diff` shows the difference from the best model (equivalent to Δ WAIC)\n",
    "- Traditional WAIC = -2 × ELPD (hence 'deviance' scale)\n",
    "\n",
    "### ArviZ Automated Comparison\n",
    "\n",
    "ArviZ provides `az.compare()` for automated model ranking with uncertainty quantification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compute-waic-loo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary of models for comparison\n",
    "models = {\n",
    "    'Maxwell': idata_maxwell,\n",
    "    'Zener': idata_zener,\n",
    "    'SpringPot': idata_springpot\n",
    "}\n",
    "\n",
    "# Compute WAIC and LOO for all models\n",
    "print(\"=\"*70)\n",
    "print(\"BAYESIAN MODEL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nComputing WAIC and LOO for all models...\\n\")\n",
    "\n",
    "# Suppress WAIC high variance warnings (expected with limited samples)\n",
    "# When posterior variance of log predictive densities > 0.4, WAIC may be unreliable\n",
    "# In such cases, LOO (below) is more robust - we compute both for comparison\n",
    "warnings.filterwarnings('ignore', message='.*posterior variance of the log predictive densities exceeds.*')\n",
    "\n",
    "comparison = az.compare(models, ic='waic', method='stacking', scale='deviance')\n",
    "print(comparison)\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"INTERPRETATION:\")\n",
    "print(\"-\"*70)\n",
    "print(\"Columns:\")\n",
    "print(\"  rank:       Model rank (0 = best)\")\n",
    "print(\"  elpd_waic:  ELPD (deviance scale: lower is better)\")\n",
    "print(\"  p_waic:     Effective number of parameters (complexity penalty)\")\n",
    "print(\"  elpd_diff:  Difference from best model (Δ WAIC in deviance scale)\")\n",
    "print(\"  weight:     Stacking weights for model averaging\")\n",
    "print(\"  se:         Standard error of ELPD\")\n",
    "print(\"  dse:        Standard error of the difference\")\n",
    "print(\"  warning:    WAIC reliability warning (True = high posterior variance)\")\n",
    "print(\"\\nRules of thumb:\")\n",
    "print(\"  elpd_diff < 4:  Models indistinguishable\")\n",
    "print(\"  elpd_diff 4-10: Moderate evidence for better model\")\n",
    "print(\"  elpd_diff > 10: Strong evidence for better model\")\n",
    "print(\"\\nNote: scale='deviance' means ELPD values are -2×log(likelihood)\")\n",
    "print(\"\\nWARNING COLUMN INTERPRETATION:\")\n",
    "print(\"  warning=True: Posterior variance of log predictive densities > 0.4\")\n",
    "print(\"  This can occur with limited samples (2000 samples, 1 chain)\")\n",
    "print(\"  When this occurs, LOO is more robust than WAIC\")\n",
    "print(\"  → See LOO results below for more reliable model comparison\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compute-loo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute LOO specifically for detailed diagnostics\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LOO CROSS-VALIDATION WITH PARETO DIAGNOSTICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Suppress Pareto k warnings (expected for misspecified models like Maxwell)\n",
    "# High Pareto k indicates influential observations or model misspecification\n",
    "# We'll examine the detailed diagnostics below to understand which models are affected\n",
    "warnings.filterwarnings('ignore', message='.*Estimated shape parameter of Pareto distribution is greater than.*')\n",
    "\n",
    "# Compute LOO for each model\n",
    "loo_maxwell = az.loo(idata_maxwell)\n",
    "loo_zener = az.loo(idata_zener)\n",
    "loo_springpot = az.loo(idata_springpot)\n",
    "\n",
    "print(\"\\nMaxwell LOO:\")\n",
    "print(loo_maxwell)\n",
    "\n",
    "print(\"\\nZener LOO:\")\n",
    "print(loo_zener)\n",
    "\n",
    "print(\"\\nSpringPot LOO:\")\n",
    "print(loo_springpot)\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"PARETO K DIAGNOSTIC:\")\n",
    "print(\"-\"*70)\n",
    "print(\"Pareto k estimates reliability of LOO approximation per data point:\")\n",
    "print(\"  k < 0.5:  Good (LOO approximation reliable)\")\n",
    "print(\"  0.5-0.7:  OK (LOO approximation acceptable)\")\n",
    "print(\"  0.7-1.0:  Bad (LOO approximation unreliable)\")\n",
    "print(\"  k > 1.0:  Very bad (LOO approximation fails)\")\n",
    "print(\"\\nIf many points have k > 0.7, consider K-fold CV instead.\")\n",
    "print(\"\\nOBSERVATION:\")\n",
    "print(\"  Maxwell has 1 'very bad' point (k > 1.0) - this is EXPECTED because\")\n",
    "print(\"  Maxwell is misspecified for Zener-generated data (wrong model).\")\n",
    "print(\"  High Pareto k indicates influential observations or poor model fit.\")\n",
    "print(\"  Zener and SpringPot have all good k values (< 0.7).\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visual-ic",
   "metadata": {},
   "source": [
    "## 7. Visualize Model Comparison\n",
    "\n",
    "ArviZ provides publication-quality visualizations of model comparison results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-compare",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model comparison\n",
    "az.plot_compare(comparison, figsize=(10, 4))\n",
    "plt.tight_layout()\n",
    "fig = plt.gcf()  # Get current figure from ArviZ\n",
    "display(fig)\n",
    "plt.close(fig)\n",
    "\n",
    "print(\"\"\"\n",
    "INTERPRETATION - Comparison Plot:\n",
    "- Models ranked from best (top) to worst (bottom)\n",
    "- CIRCLE: ELPD (expected log predictive density, higher is better)\n",
    "- ERROR BARS: Standard error of ELPD\n",
    "- TRIANGLE: In-sample deviance (not penalized for complexity)\n",
    "\n",
    "What to look for:\n",
    "✓ Non-overlapping error bars → clear winner\n",
    "✗ Overlapping error bars → models statistically equivalent\n",
    "\n",
    "Expected result:\n",
    "- Zener should rank #1 (true model)\n",
    "- Maxwell #2 (underfit, misses equilibrium modulus)\n",
    "- SpringPot #3 (different physics, wrong functional form)\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-pareto",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Pareto k diagnostics for best model\n",
    "best_model_name = comparison.index[0]\n",
    "best_idata = models[best_model_name]\n",
    "\n",
    "# Map model name to LOO result (needed for plot_khat)\n",
    "loo_results = {\n",
    "    'Maxwell': loo_maxwell,\n",
    "    'Zener': loo_zener,\n",
    "    'SpringPot': loo_springpot\n",
    "}\n",
    "best_loo = loo_results[best_model_name]\n",
    "\n",
    "print(f\"\\nPareto k diagnostic for best model ({best_model_name}):\")\n",
    "az.plot_khat(best_loo, figsize=(10, 4))\n",
    "plt.tight_layout()\n",
    "fig = plt.gcf()  # Get current figure from ArviZ\n",
    "display(fig)\n",
    "plt.close(fig)\n",
    "\n",
    "print(\"\"\"\n",
    "INTERPRETATION - Pareto k Plot:\n",
    "- Each point represents one data observation\n",
    "- Y-axis: Pareto k statistic (reliability of LOO approximation)\n",
    "- Horizontal lines: Thresholds (0.5 good, 0.7 bad)\n",
    "\n",
    "What to look for:\n",
    "✓ All points below 0.5: LOO is reliable\n",
    "⚠ Few points 0.5-0.7: LOO acceptable but check those points\n",
    "✗ Many points > 0.7: LOO unreliable, use K-fold CV instead\n",
    "\n",
    "High Pareto k indicates:\n",
    "- Influential observations (large impact on posterior)\n",
    "- Outliers or poorly fit regions\n",
    "- Potential model misspecification\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weight-intro",
   "metadata": {},
   "source": [
    "## 8. Model Weights and Averaging\n",
    "\n",
    "### Stacking Weights\n",
    "\n",
    "ArviZ computes **stacking weights** - optimal weights for combining model predictions:\n",
    "- Sum to 1.0\n",
    "- Higher weight = model contributes more to ensemble\n",
    "- If one model has weight ≈ 1.0, it's clearly superior\n",
    "- If weights distributed, consider model averaging\n",
    "\n",
    "### When to Use Model Averaging\n",
    "\n",
    "**Use single best model if:**\n",
    "- Clear winner (elpd_diff > 10)\n",
    "- Stacking weight > 0.9\n",
    "- Physical interpretation matters (choose simplest adequate model)\n",
    "\n",
    "**Use model averaging if:**\n",
    "- Models statistically indistinguishable (elpd_diff < 4)\n",
    "- Stacking weights distributed (no model > 0.7)\n",
    "- Prediction accuracy paramount (not interpretation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stacking-weights",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract stacking weights\n",
    "weights = comparison['weight'].values\n",
    "model_names = comparison.index.tolist()\n",
    "\n",
    "# Visualize weights\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.bar(model_names, weights, color=['#1f77b4', '#ff7f0e', '#2ca02c'], alpha=0.7, edgecolor='black')\n",
    "ax.set_ylabel('Stacking Weight', fontweight='bold')\n",
    "ax.set_title('Model Weights for Ensemble Prediction', fontweight='bold')\n",
    "ax.set_ylim([0, 1])\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Annotate weights\n",
    "for i, (name, w) in enumerate(zip(model_names, weights)):\n",
    "    ax.text(i, w + 0.03, f'{w:.3f}', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "display(fig)\n",
    "plt.close(fig)\n",
    "\n",
    "print(\"\\nStacking Weights Interpretation:\")\n",
    "print(\"-\" * 50)\n",
    "for name, w in zip(model_names, weights):\n",
    "    print(f\"  {name:<12}: {w:.4f}  \", end=\"\")\n",
    "    if w > 0.9:\n",
    "        print(\"(dominant model - use this one) ✓\")\n",
    "    elif w > 0.5:\n",
    "        print(\"(major contributor)\")\n",
    "    elif w > 0.1:\n",
    "        print(\"(minor contributor)\")\n",
    "    else:\n",
    "        print(\"(negligible)\")\n",
    "\n",
    "print(\"\\nRecommendation:\")\n",
    "if weights[0] > 0.9:\n",
    "    print(f\"✓ Use {model_names[0]} (clear winner with weight {weights[0]:.3f})\")\n",
    "elif weights[0] > 0.7:\n",
    "    print(f\"✓ Prefer {model_names[0]} (dominant with weight {weights[0]:.3f})\")\n",
    "else:\n",
    "    print(f\"⚠ Consider model averaging (weights distributed across {sum(w > 0.1 for w in weights)} models)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decision",
   "metadata": {},
   "source": [
    "## 9. Final Model Selection Decision\n",
    "\n",
    "### Decision Framework\n",
    "\n",
    "Combine **statistical evidence** with **physical reasoning**:\n",
    "\n",
    "**Statistical Criteria:**\n",
    "1. ELPD comparison (elpd_diff values)\n",
    "2. Stacking weights\n",
    "3. Pareto k diagnostics\n",
    "\n",
    "**Physical Criteria:**\n",
    "1. Model assumptions match material physics?\n",
    "2. Parameters physically interpretable?\n",
    "3. Simplicity (Occam's Razor)\n",
    "\n",
    "**Example Decision Tree:**\n",
    "```\n",
    "IF elpd_diff > 10 AND weight > 0.9:\n",
    "    → Use clear winner\n",
    "ELIF elpd_diff < 4:\n",
    "    → Models equivalent, choose simpler model (fewer parameters)\n",
    "ELIF weights distributed:\n",
    "    → Consider model averaging for predictions\n",
    "ELSE:\n",
    "    → Balance statistical evidence with physical interpretation\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-decision",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"FINAL MODEL SELECTION DECISION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Extract comparison metrics\n",
    "best_model = comparison.index[0]\n",
    "elpd_diff_second = comparison.loc[comparison.index[1], 'elpd_diff']\n",
    "weight_best = comparison.loc[best_model, 'weight']\n",
    "\n",
    "print(f\"\\nBest Model: {best_model}\")\n",
    "print(f\"  ELPD (deviance): {comparison.loc[best_model, 'elpd_waic']:.2f}\")\n",
    "print(f\"  Δ ELPD from 2nd: {elpd_diff_second:.2f}\")\n",
    "print(f\"  Stacking weight: {weight_best:.3f}\")\n",
    "print(f\"  Effective parameters: {comparison.loc[best_model, 'p_waic']:.2f}\")\n",
    "\n",
    "print(\"\\nStatistical Evidence:\")\n",
    "if elpd_diff_second > 10:\n",
    "    print(f\"  ✓ STRONG: Δ ELPD = {elpd_diff_second:.1f} > 10 (clear winner)\")\n",
    "elif elpd_diff_second > 4:\n",
    "    print(f\"  ✓ MODERATE: Δ ELPD = {elpd_diff_second:.1f} in [4, 10] (likely better)\")\n",
    "else:\n",
    "    print(f\"  ⚠ WEAK: Δ ELPD = {elpd_diff_second:.1f} < 4 (models indistinguishable)\")\n",
    "\n",
    "if weight_best > 0.9:\n",
    "    print(f\"  ✓ DOMINANT: Stacking weight {weight_best:.3f} > 0.9\")\n",
    "elif weight_best > 0.7:\n",
    "    print(f\"  ✓ MAJOR: Stacking weight {weight_best:.3f} > 0.7\")\n",
    "else:\n",
    "    print(f\"  ⚠ DISTRIBUTED: Stacking weight {weight_best:.3f} < 0.7\")\n",
    "\n",
    "print(\"\\nPhysical Reasoning:\")\n",
    "if best_model == 'Zener':\n",
    "    print(\"  ✓ Data exhibits finite equilibrium modulus (plateau at long times)\")\n",
    "    print(\"  ✓ Zener model captures this physics with spring-dashpot parallel arm\")\n",
    "    print(\"  ✓ 3 parameters provide adequate flexibility without overfitting\")\n",
    "elif best_model == 'Maxwell':\n",
    "    print(\"  ⚠ Maxwell assumes complete stress relaxation (G→0 as t→∞)\")\n",
    "    print(\"  ⚠ Data shows equilibrium modulus (G→Ge ≠ 0)\")\n",
    "    print(\"  ⚠ Model misspecification likely\")\n",
    "elif best_model == 'SpringPot':\n",
    "    print(\"  ⚠ SpringPot models power-law relaxation (different physics)\")\n",
    "    print(\"  ⚠ Data is better described by exponential + constant (Zener)\")\n",
    "    print(\"  ⚠ May fit numerically but wrong physical interpretation\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RECOMMENDATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if best_model == 'Zener' and elpd_diff_second > 4 and weight_best > 0.7:\n",
    "    print(f\"✓✓✓ Use {best_model} model\")\n",
    "    print(\"\\nRationale:\")\n",
    "    print(\"  - Strong statistical evidence (ELPD difference and stacking weights)\")\n",
    "    print(\"  - Correct physics (finite equilibrium modulus)\")\n",
    "    print(\"  - Parsimonious (3 parameters vs generalized models)\")\n",
    "    print(\"  - All parameters physically interpretable\")\n",
    "else:\n",
    "    print(f\"Use {best_model} model with caveats\")\n",
    "    print(f\"Consider collecting more data or testing additional models\")\n",
    "\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"  1. Validate on independent test data\")\n",
    "print(\"  2. Check parameter physical reasonableness\")\n",
    "print(\"  3. Examine residuals for systematic errors\")\n",
    "print(\"  4. Consider model averaging if weights distributed\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "key-takeaways",
   "metadata": {},
   "source": [
    "## 10. Key Takeaways\n",
    "\n",
    "### Main Concepts\n",
    "\n",
    "1. **Bayesian Model Comparison:**\n",
    "   - Use WAIC/LOO to balance fit quality with model complexity\n",
    "   - Lower WAIC/LOO = better expected out-of-sample prediction\n",
    "   - Automatically penalizes overfitting via effective parameter count\n",
    "\n",
    "2. **Interpretation Guidelines:**\n",
    "   - **Δ ELPD < 4:** Models statistically indistinguishable → choose simpler\n",
    "   - **Δ ELPD 4-10:** Moderate evidence for better model\n",
    "   - **Δ ELPD > 10:** Strong evidence for better model\n",
    "   - **Stacking weight > 0.9:** Clear winner, use single model\n",
    "   - **Weights distributed:** Consider model averaging\n",
    "\n",
    "3. **Pareto k Diagnostics:**\n",
    "   - Assesses reliability of LOO approximation per data point\n",
    "   - k < 0.5: Good (LOO reliable)\n",
    "   - k > 0.7: Bad (LOO unreliable, use K-fold CV)\n",
    "   - High k indicates influential observations or model misspecification\n",
    "\n",
    "4. **Decision Framework:**\n",
    "   - Combine statistical evidence with physical reasoning\n",
    "   - Prefer simplest model that adequately explains data (Occam's Razor)\n",
    "   - Ensure parameters are physically interpretable\n",
    "   - Validate on independent data when possible\n",
    "\n",
    "### When to Use Model Comparison\n",
    "\n",
    "**Essential for:**\n",
    "- ✓ Multiple candidate models with different physics\n",
    "- ✓ Uncertain about appropriate model complexity\n",
    "- ✓ Avoiding overfitting with limited data\n",
    "- ✓ Objective model selection for publication\n",
    "\n",
    "**Optional for:**\n",
    "- Well-established model for specific material class\n",
    "- Only one physically plausible model\n",
    "- Exploratory analysis (NLSQ screening sufficient)\n",
    "\n",
    "### Common Pitfalls\n",
    "\n",
    "1. **Ignoring Physical Constraints:**\n",
    "   - Statistical best model may violate physics\n",
    "   - Always check parameter physical reasonableness\n",
    "   - Simplest adequate model often preferable to complex best fit\n",
    "\n",
    "2. **Overfitting with Small Datasets:**\n",
    "   - Complex models can overfit with few data points\n",
    "   - WAIC/LOO automatically penalize this via p_WAIC\n",
    "   - If weights distributed, likely insufficient data to distinguish\n",
    "\n",
    "3. **Comparing Apples to Oranges:**\n",
    "   - Only compare models fit to **same data**\n",
    "   - Same likelihood formulation (e.g., Gaussian observation model)\n",
    "   - Same response variable (don't compare G(t) vs G'(ω) models)\n",
    "\n",
    "4. **Ignoring Pareto k Warnings:**\n",
    "   - High Pareto k → LOO unreliable\n",
    "   - Investigate influential points before trusting LOO\n",
    "   - May indicate outliers or model misspecification\n",
    "\n",
    "### ArviZ Functions Summary\n",
    "\n",
    "```python\n",
    "# Model comparison (with deviance scale for traditional IC values)\n",
    "comparison = az.compare(models, ic='waic', scale='deviance')  # or ic='loo'\n",
    "\n",
    "# Individual metrics\n",
    "waic = az.waic(idata)\n",
    "loo = az.loo(idata)\n",
    "\n",
    "# Visualization\n",
    "az.plot_compare(comparison)\n",
    "az.plot_khat(loo)  # Pareto k diagnostic (pass LOO result, not InferenceData)\n",
    "```\n",
    "\n",
    "**Note on ArviZ column names:**\n",
    "- Modern ArviZ (>=0.15.0) uses `elpd_waic`/`elpd_loo` and `elpd_diff`\n",
    "- With `scale='deviance'`: values are in traditional IC scale (-2 × log likelihood)\n",
    "- `elpd_diff` in deviance scale is equivalent to Δ WAIC or Δ LOO\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "### Apply to Your Data\n",
    "- Test multiple rheological models on your experimental data\n",
    "- Use WAIC/LOO to objectively select best model\n",
    "- Validate selection with physical reasoning\n",
    "\n",
    "### Advanced Topics\n",
    "- **[05-uncertainty-propagation.ipynb](05-uncertainty-propagation.ipynb)**: Propagate parameter uncertainty to predictions\n",
    "- **[advanced/01-multi-technique-fitting.ipynb](../advanced/01-multi-technique-fitting.ipynb)**: Compare models across multiple test modes\n",
    "- **[advanced/04-fractional-models-deep-dive.ipynb](../advanced/04-fractional-models-deep-dive.ipynb)**: Compare 11 fractional models\n",
    "\n",
    "### Further Reading\n",
    "- Vehtari et al. (2017): \"Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC\"\n",
    "- McElreath (2020): \"Statistical Rethinking\" (Chapter 7: Model Comparison)\n",
    "- ArviZ documentation: https://arviz-devs.github.io/arviz/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "session-info",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Session Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "session-info-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import rheojax\n",
    "\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"Rheo: {rheojax.__version__}\")\n",
    "print(f\"JAX: {jax.__version__}\")\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "print(f\"ArviZ: {az.__version__}\")\n",
    "print(f\"JAX devices: {jax.devices()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rheojax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
