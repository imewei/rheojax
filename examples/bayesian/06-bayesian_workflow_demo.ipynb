{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Workflow Demo: Complete NLSQ → NUTS → ArviZ Pipeline\n",
    "\n",
    "> **Handbook:** See [Complete Bayesian Workflow](../../docs/source/_includes/bayesian_workflow.rst) for step-by-step implementation guide and [Three-Stage Pipeline](../../docs/source/user_guide/03_advanced_topics/bayesian_inference.rst#workflow) for theoretical background.\n",
    "\n",
    "This notebook demonstrates the recommended three-stage Bayesian workflow:\n",
    "1. **Stage 1**: NLSQ point estimation (fast, ~seconds)\n",
    "2. **Stage 2**: NUTS posterior sampling with warm-start (~minutes)\n",
    "3. **Stage 3**: ArviZ diagnostic plots (visual verification)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "After completing this notebook, you will be able to:\n",
    "- Execute the complete three-stage Bayesian workflow\n",
    "- Understand warm-start benefits for MCMC convergence\n",
    "- Interpret posterior summaries and credible intervals\n",
    "- Use ArviZ for comprehensive MCMC diagnostics\n",
    "- Apply the workflow to any RheoJAX model\n",
    "\n",
    "**Estimated Time:** 15-20 minutes\n",
    "\n",
    "**Expected runtime**: ~30 seconds (depending on hardware)\n",
    "\n",
    "**Requirements**:\n",
    "- rheojax with Bayesian dependencies (numpyro, arviz)\n",
    "- matplotlib for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T02:56:31.304947Z",
     "iopub.status.busy": "2026-02-14T02:56:31.304858Z",
     "iopub.status.idle": "2026-02-14T02:56:31.308500Z",
     "shell.execute_reply": "2026-02-14T02:56:31.307809Z"
    }
   },
   "outputs": [],
   "source": [
    "# Google Colab Setup - Run this cell first!\n",
    "# Skip if running locally with rheojax already installed\n",
    "\n",
    "import sys\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Install rheojax and dependencies\n",
    "    !pip install -q rheojax\n",
    "    \n",
    "    # Colab uses float32 by default - we need float64 for numerical stability\n",
    "    # This MUST be set before importing JAX\n",
    "    import os\n",
    "    os.environ['JAX_ENABLE_X64'] = 'true'\n",
    "    \n",
    "    print(\"✓ RheoJAX installed successfully!\")\n",
    "    print(\"✓ Float64 precision enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T02:56:31.309767Z",
     "iopub.status.busy": "2026-02-14T02:56:31.309674Z",
     "iopub.status.idle": "2026-02-14T02:56:32.592724Z",
     "shell.execute_reply": "2026-02-14T02:56:32.592170Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from rheojax.models import Maxwell\n",
    "\n",
    "from rheojax.core.jax_config import safe_import_jax\n",
    "\n",
    "# Safe JAX import (ensures float64 precision)\n",
    "jax, jnp = safe_import_jax()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"BAYESIAN WORKFLOW DEMONSTRATION\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nThis demo shows the recommended NLSQ → NUTS → ArviZ workflow\")\n",
    "print(\"for uncertainty quantification in rheological modeling.\\n\")\n",
    "\n",
    "import sys, os\n",
    "sys.path.insert(0, os.path.dirname(os.path.abspath(\"\")))\n",
    "from utils.plotting_utils import (\n",
    "    plot_nlsq_fit, display_arviz_diagnostics, plot_posterior_predictive\n",
    ")\n",
    "\n",
    "FAST_MODE = os.environ.get(\"FAST_MODE\", \"1\") == \"1\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Generate Synthetic Relaxation Data\n",
    "\n",
    "We'll create synthetic Maxwell relaxation data with realistic noise to demonstrate the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T02:56:32.594053Z",
     "iopub.status.busy": "2026-02-14T02:56:32.593895Z",
     "iopub.status.idle": "2026-02-14T02:56:32.597112Z",
     "shell.execute_reply": "2026-02-14T02:56:32.596684Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Step 1: Generating synthetic Maxwell relaxation data...\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# True parameters\n",
    "G0_true = 1e5  # Pa\n",
    "eta_true = 1e3  # Pa·s\n",
    "tau_true = eta_true / G0_true  # 0.01 s\n",
    "\n",
    "print(f\"  True parameters:\")\n",
    "print(f\"    G₀  = {G0_true:.2e} Pa\")\n",
    "print(f\"    η   = {eta_true:.2e} Pa·s\")\n",
    "print(f\"    τ   = {tau_true:.4f} s\")\n",
    "\n",
    "# Time array (log-spaced for relaxation)\n",
    "t = np.logspace(-2, 2, 50)  # 0.01 to 100 seconds\n",
    "\n",
    "# True relaxation modulus\n",
    "G_t_true = G0_true * np.exp(-t / tau_true)\n",
    "\n",
    "# Add realistic noise (1.5% relative)\n",
    "np.random.seed(42)\n",
    "noise_level = 0.015\n",
    "noise = np.random.normal(0, noise_level * G_t_true)\n",
    "G_t_noisy = G_t_true + noise\n",
    "\n",
    "print(f\"\\n  Generated {len(t)} data points from {t.min():.2e} to {t.max():.1f} s\")\n",
    "print(f\"  Noise level: {noise_level*100:.1f}% relative\")\n",
    "print(f\"  Signal-to-noise ratio: {np.mean(G_t_true)/np.std(noise):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1: NLSQ Point Estimation (Fast)\n",
    "\n",
    "First, we obtain fast point estimates using nonlinear least squares optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T02:56:32.598215Z",
     "iopub.status.busy": "2026-02-14T02:56:32.598139Z",
     "iopub.status.idle": "2026-02-14T02:56:33.780587Z",
     "shell.execute_reply": "2026-02-14T02:56:33.779951Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STAGE 1: NLSQ POINT ESTIMATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "model = Maxwell()\n",
    "model.parameters.set_bounds('G0', (1e3, 1e7))\n",
    "model.parameters.set_bounds('eta', (1e1, 1e5))\n",
    "\n",
    "print(\"\\nRunning NLSQ optimization...\")\n",
    "import time\n",
    "\n",
    "start_nlsq = time.time()\n",
    "\n",
    "model.fit(t, G_t_noisy, method='nlsq')\n",
    "\n",
    "nlsq_time = time.time() - start_nlsq\n",
    "\n",
    "# Extract NLSQ results\n",
    "G0_nlsq = model.parameters.get_value('G0')\n",
    "eta_nlsq = model.parameters.get_value('eta')\n",
    "tau_nlsq = eta_nlsq / G0_nlsq\n",
    "\n",
    "print(f\"\\n✓ NLSQ completed in {nlsq_time:.3f} seconds\")\n",
    "print(f\"\\nNLSQ Point Estimates:\")\n",
    "print(f\"  G₀  = {G0_nlsq:.4e} Pa  (error: {abs(G0_nlsq-G0_true)/G0_true*100:.2f}%)\")\n",
    "print(f\"  η   = {eta_nlsq:.4e} Pa·s  (error: {abs(eta_nlsq-eta_true)/eta_true*100:.2f}%)\")\n",
    "print(f\"  τ   = {tau_nlsq:.6f} s  (error: {abs(tau_nlsq-tau_true)/tau_true*100:.2f}%)\")\n",
    "print(f\"\\n⚠  Note: NLSQ provides point estimates only (no uncertainty)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2: Bayesian Inference with Warm-Start\n",
    "\n",
    "Now we perform MCMC sampling using NUTS, warm-starting from the NLSQ estimates for faster convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T02:56:33.782028Z",
     "iopub.status.busy": "2026-02-14T02:56:33.781936Z",
     "iopub.status.idle": "2026-02-14T02:56:39.127544Z",
     "shell.execute_reply": "2026-02-14T02:56:39.127061Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STAGE 2: BAYESIAN INFERENCE (NUTS)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nRunning NUTS sampling with NLSQ warm-start...\")\n",
    "print(\"  Configuration:\")\n",
    "print(f\"    • num_chains: 4 (for robust diagnostics)\")\n",
    "print(f\"    • num_warmup: 1000 (burn-in iterations)\")\n",
    "print(f\"    • num_samples: 2000 (posterior samples per chain)\")\n",
    "print(f\"    • warm-start: Yes (from NLSQ estimates)\")\n",
    "print(\"\\n  This may take 20-60 seconds depending on your hardware...\")\n",
    "\n",
    "start_bayes = time.time()\n",
    "\n",
    "# Run Bayesian inference with warm-start\n",
    "result = model.fit_bayesian(\n",
    "    t, G_t_noisy,\n",
    "    num_warmup=1000,\n",
    "    num_samples=2000,\n",
    "    num_chains=4,\n",
    "    initial_values={  # Warm-start from NLSQ\n",
    "        'G0': G0_nlsq,\n",
    "        'eta': eta_nlsq\n",
    "    }\n",
    ")\n",
    "\n",
    "bayes_time = time.time() - start_bayes\n",
    "\n",
    "print(f\"\\n✓ Bayesian inference completed in {bayes_time:.1f} seconds\")\n",
    "print(f\"  Total time (NLSQ + Bayes): {nlsq_time + bayes_time:.1f} seconds\")\n",
    "print(f\"  Generated {result.num_chains * result.num_samples} posterior samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior Results\n",
    "\n",
    "Extract and display the posterior summary statistics and credible intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T02:56:39.129455Z",
     "iopub.status.busy": "2026-02-14T02:56:39.128860Z",
     "iopub.status.idle": "2026-02-14T02:56:39.133761Z",
     "shell.execute_reply": "2026-02-14T02:56:39.133335Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"POSTERIOR RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "posterior = result.posterior_samples\n",
    "summary = result.summary\n",
    "\n",
    "print(\"\\nPosterior Estimates (mean ± std):\")\n",
    "print(f\"  G₀  = {summary['G0']['mean']:.4e} ± {summary['G0']['std']:.4e} Pa\")\n",
    "print(f\"  η   = {summary['eta']['mean']:.4e} ± {summary['eta']['std']:.4e} Pa·s\")\n",
    "\n",
    "# Compute credible intervals\n",
    "intervals = model.get_credible_intervals(posterior, credibility=0.95)\n",
    "print(\"\\n95% Credible Intervals:\")\n",
    "print(f\"  G₀:  [{intervals['G0'][0]:.4e}, {intervals['G0'][1]:.4e}] Pa\")\n",
    "print(f\"  η:   [{intervals['eta'][0]:.4e}, {intervals['eta'][1]:.4e}] Pa·s\")\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"  'There is 95% probability that G₀ lies in the interval above'\")\n",
    "print(\"  This is a DIRECT probabilistic statement (Bayesian interpretation)\")\n",
    "\n",
    "# Relative uncertainties\n",
    "print(\"\\nRelative Uncertainties:\")\n",
    "print(f\"  G₀:  {summary['G0']['std']/summary['G0']['mean']*100:.2f}%\")\n",
    "print(f\"  η:   {summary['eta']['std']/summary['eta']['mean']*100:.2f}%\")\n",
    "\n",
    "# Check if true values are in credible intervals\n",
    "G0_in_CI = intervals['G0'][0] <= G0_true <= intervals['G0'][1]\n",
    "eta_in_CI = intervals['eta'][0] <= eta_true <= intervals['eta'][1]\n",
    "print(\"\\nValidation (true values in 95% CI):\")\n",
    "print(f\"  G₀:  {'✓ Yes' if G0_in_CI else '✗ No'}\")\n",
    "print(f\"  η:   {'✓ Yes' if eta_in_CI else '✗ No'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3: Convergence Diagnostics (CRITICAL!)\n",
    "\n",
    "**Always check convergence before interpreting Bayesian results!**\n",
    "\n",
    "We examine:\n",
    "- **R-hat (Gelman-Rubin)**: Should be < 1.01 for all parameters\n",
    "- **ESS (Effective Sample Size)**: Should be > 400 for reliable inference\n",
    "- **Divergences**: Should be < 1% of total samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T02:56:39.135240Z",
     "iopub.status.busy": "2026-02-14T02:56:39.135156Z",
     "iopub.status.idle": "2026-02-14T02:56:39.139479Z",
     "shell.execute_reply": "2026-02-14T02:56:39.138959Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STAGE 3: CONVERGENCE DIAGNOSTICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "diagnostics = result.diagnostics\n",
    "\n",
    "print(\"\\n⚠  ALWAYS check convergence before interpreting Bayesian results!\")\n",
    "print(\"\\n1. R-hat (Gelman-Rubin Statistic):\")\n",
    "print(f\"   Target: < 1.01 for all parameters\")\n",
    "for param in ['G0', 'eta']:\n",
    "    rhat = diagnostics['r_hat'][param]\n",
    "    status = '✓ Converged' if rhat < 1.01 else '✗ NOT converged'\n",
    "    print(f\"     {param:<5} R-hat = {rhat:.4f}  {status}\")\n",
    "\n",
    "print(\"\\n2. ESS (Effective Sample Size):\")\n",
    "print(f\"   Target: > 400 (out of {result.num_chains * result.num_samples} total)\")\n",
    "for param in ['G0', 'eta']:\n",
    "    ess = diagnostics['ess'][param]\n",
    "    efficiency = ess / (result.num_chains * result.num_samples) * 100\n",
    "    status = '✓ Sufficient' if ess > 400 else '✗ Low (increase samples)'\n",
    "    print(f\"     {param:<5} ESS = {ess:.0f} ({efficiency:.1f}% efficient)  {status}\")\n",
    "\n",
    "if 'num_divergences' in diagnostics:\n",
    "    div_rate = diagnostics['num_divergences'] / (result.num_chains * result.num_samples) * 100\n",
    "    print(\"\\n3. Divergences:\")\n",
    "    print(f\"   Count: {diagnostics['num_divergences']} ({div_rate:.2f}%)\")\n",
    "    status = '✓ Good' if div_rate < 1 else '✗ High (results unreliable)'\n",
    "    print(f\"   Target: < 1%  {status}\")\n",
    "\n",
    "# Overall convergence assessment\n",
    "all_converged = (\n",
    "    all(diagnostics['r_hat'][p] < 1.01 for p in ['G0', 'eta']) and\n",
    "    all(diagnostics['ess'][p] > 400 for p in ['G0', 'eta'])\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "if all_converged:\n",
    "    print(\"✓✓✓ EXCELLENT CONVERGENCE ✓✓✓\")\n",
    "    print(\"All diagnostic criteria met. Results are reliable.\")\n",
    "else:\n",
    "    print(\"⚠⚠⚠ CONVERGENCE ISSUES ⚠⚠⚠\")\n",
    "    print(\"Increase num_warmup or num_samples and rerun.\")\n",
    "print(\"-\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Diagnostics (ArviZ Integration)\n",
    "\n",
    "ArviZ provides comprehensive diagnostic visualizations. We'll generate 6 key plots:\n",
    "\n",
    "1. **Trace plot**: Visual convergence check\n",
    "2. **Rank plot**: Most sensitive convergence test\n",
    "3. **Pair plot**: Parameter correlations + divergences\n",
    "4. **Autocorrelation plot**: Mixing quality\n",
    "5. **ESS plot**: Sampling efficiency\n",
    "6. **Forest plot**: Credible interval comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T02:56:39.140919Z",
     "iopub.status.busy": "2026-02-14T02:56:39.140827Z",
     "iopub.status.idle": "2026-02-14T02:56:39.837564Z",
     "shell.execute_reply": "2026-02-14T02:56:39.837052Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VISUAL DIAGNOSTICS (ArviZ Integration)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nGenerating diagnostic plots...\")\n",
    "\n",
    "# Convert to ArviZ InferenceData\n",
    "\n",
    "idata = result.to_inference_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T02:56:39.839269Z",
     "iopub.status.busy": "2026-02-14T02:56:39.839060Z",
     "iopub.status.idle": "2026-02-14T02:56:40.451846Z",
     "shell.execute_reply": "2026-02-14T02:56:40.451399Z"
    }
   },
   "outputs": [],
   "source": [
    "# ArviZ diagnostic plots (trace, pair, forest, energy, autocorr, rank)\n",
    "display_arviz_diagnostics(result, ['G0', 'eta'], fast_mode=FAST_MODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow Summary\n",
    "\n",
    "Recap of the complete 3-stage workflow and key results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T21:18:27.358305Z",
     "iopub.status.busy": "2026-02-09T21:18:27.358212Z",
     "iopub.status.idle": "2026-02-09T21:18:27.360990Z",
     "shell.execute_reply": "2026-02-09T21:18:27.360634Z"
    }
   },
   "source": [
    "### Residual Analysis in Workflow\n",
    "\n",
    "**Examining residuals** at each stage reveals model adequacy:\n",
    "\n",
    "| Workflow Stage | Residual Check | Interpretation |\n",
    "|----------------|----------------|----------------|\n",
    "| **Stage 1 (NLSQ)** | Quick visual check | Random scatter → proceed to Bayesian |\n",
    "| **Stage 2 (NUTS)** | Posterior predictive | Bands capture data → model adequate |\n",
    "| **Stage 3 (ArviZ)** | Diagnostic plots | No systematic patterns → converged |\n",
    "\n",
    "If residuals show systematic patterns (trends, heteroscedasticity), consider:\n",
    "- Different model (e.g., Zener instead of Maxwell)\n",
    "- Additional parameters\n",
    "- Weighted least squares for non-constant variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "- **Gelman et al. (2020)**: [\"Bayesian Workflow\"](https://arxiv.org/abs/1507.03246) — Comprehensive workflow for applied Bayesian modeling\n",
    "- **Betancourt (2018)**: [\"Towards a Principled Bayesian Workflow\"](https://betanalpha.github.io/assets/case_studies/principled_bayesian_workflow.html) — Deep dive into workflow stages\n",
    "- **RheoJAX Bayesian Guide**: [Documentation](../../docs/source/_includes/bayesian_workflow.rst) — Implementation details and best practices\n",
    "- **ArviZ Workflow Tools**: [Tutorial](https://python.arviz.org/en/stable/getting_started/WorkingWithInferenceData.html) — Using InferenceData for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- **[07-gmm_bayesian_workflow.ipynb](07-gmm_bayesian_workflow.ipynb)**: Apply workflow to multi-mode Generalized Maxwell models\n",
    "- **[08-spp-laos.ipynb](08-spp-laos.ipynb)** & **[09-spp-rheojax-workflow.ipynb](09-spp-rheojax-workflow.ipynb)**: Workflow for nonlinear LAOS analysis\n",
    "- Adapt this workflow template to your own rheological datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For questions or issues:**\n",
    "- GitHub: https://github.com/imewei/rheojax\n",
    "- Docs: https://rheojax.readthedocs.io\n",
    "\n",
    "### Key References\n",
    "\n",
    "- **Gelman, A. et al. (2020).** \"Bayesian Workflow.\" arXiv:2011.01808. [Modern best practices for applied Bayesian analysis]\n",
    "- **McElreath, R. (2020).** *Statistical Rethinking*. 2nd ed. CRC Press. [Complete workflow with practical examples]\n",
    "- **Gabry, J. et al. (2019).** \"Visualization in Bayesian workflow.\" *J. Royal Stat. Soc. A* 182:389-402. [Diagnostic visualization strategies]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rheojax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
