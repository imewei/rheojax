{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# Generalized Maxwell Model - Bayesian Workflow with Prior Safety\n\n> **Handbook:** See [Generalized Maxwell Model](../../docs/source/models/gmm/index.rst) for model theory and [Tiered Prior Safety](../../docs/source/user_guide/03_advanced_topics/bayesian_inference.rst#prior-safety) for the convergence-based prior management system.\n\nThis notebook demonstrates the complete Bayesian inference workflow for the Generalized Maxwell Model (GMM), including NLSQ point estimation, warm-started NUTS sampling, and the tiered prior safety mechanism.\n\n## Learning Objectives\n\nAfter completing this notebook, you will be able to:\n- Perform two-step Bayesian workflow: NLSQ → NUTS with warm-start\n- Understand the tiered prior safety mechanism (hard failure, suspicious, good)\n- Diagnose NLSQ convergence quality before Bayesian inference\n- Analyze GMM posterior distributions with ArviZ\n- Quantify parameter uncertainty and credible intervals\n- Compare models with different numbers of modes (N=1 vs N=3)\n\n## Prerequisites\n\n- Understanding of Bayesian inference basics (recommended: `01-bayesian-basics.ipynb`)\n- Familiarity with GMM fitting (recommended: `08-generalized_maxwell_fitting.ipynb`)\n- Knowledge of MCMC diagnostics (R-hat, ESS, divergences)\n\n**Estimated Time:** 30-45 minutes"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T21:18:36.848116Z",
     "iopub.status.busy": "2026-02-09T21:18:36.848002Z",
     "iopub.status.idle": "2026-02-09T21:18:36.851334Z",
     "shell.execute_reply": "2026-02-09T21:18:36.850948Z"
    }
   },
   "outputs": [],
   "source": [
    "# Google Colab Setup - Run this cell first!\n",
    "# Skip if running locally with rheojax already installed\n",
    "\n",
    "import sys\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Install rheojax and dependencies\n",
    "    !pip install -q rheojax\n",
    "    \n",
    "    # Colab uses float32 by default - we need float64 for numerical stability\n",
    "    # This MUST be set before importing JAX\n",
    "    import os\n",
    "    os.environ['JAX_ENABLE_X64'] = 'true'\n",
    "    \n",
    "    print(\"✓ RheoJAX installed successfully!\")\n",
    "    print(\"✓ Float64 precision enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "We start by importing necessary libraries and verifying float64 precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T21:18:36.853360Z",
     "iopub.status.busy": "2026-02-09T21:18:36.853180Z",
     "iopub.status.idle": "2026-02-09T21:18:38.112992Z",
     "shell.execute_reply": "2026-02-09T21:18:38.112509Z"
    }
   },
   "outputs": [],
   "source": [
    "# Configure matplotlib for inline plotting in VS Code/Jupyter\n",
    "# MUST come before importing matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "# Standard scientific computing imports\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "\n",
    "from rheojax.core.jax_config import safe_import_jax\n",
    "\n",
    "# RheoJAX imports - always explicit\n",
    "from rheojax.models import GeneralizedMaxwell\n",
    "\n",
    "# Safe JAX import - REQUIRED for all notebooks using JAX\n",
    "# This pattern ensures float64 precision enforcement throughout\n",
    "jax, jnp = safe_import_jax()\n",
    "\n",
    "# Set reproducible random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib for publication-quality plots\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "print(\"Setup complete - using JAX with float64 precision\")\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, os.path.dirname(os.path.abspath(\"\")))\n",
    "from utils.plotting_utils import (\n",
    "    display_arviz_diagnostics,\n",
    "    plot_nlsq_fit,\n",
    "    plot_posterior_predictive,\n",
    ")\n",
    "\n",
    "FAST_MODE = os.environ.get(\"FAST_MODE\", \"1\") == \"1\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Background: Two-Step Bayesian Workflow\n",
    "\n",
    "### NLSQ + NumPyro Integration\n",
    "\n",
    "RheoJAX implements a two-step optimization workflow:\n",
    "\n",
    "**Step 1: NLSQ Point Estimation (Fast)**\n",
    "- GPU-accelerated nonlinear least squares\n",
    "- 5-270x speedup over scipy\n",
    "- Provides point estimates and Hessian-based uncertainties\n",
    "- Diagnostics: convergence flag, gradient norm, condition number\n",
    "\n",
    "**Step 2: NUTS Bayesian Inference (Warm-Started)**\n",
    "- NumPyro MCMC with No-U-Turn Sampler\n",
    "- Warm-start from NLSQ point estimates (2-5x faster convergence)\n",
    "- Full posterior distributions with credible intervals\n",
    "- ArviZ diagnostics: R-hat, ESS, divergences\n",
    "\n",
    "### Tiered Prior Safety Mechanism\n",
    "\n",
    "RheoJAX v0.3.0 introduces **intelligent prior management** based on NLSQ convergence quality:\n",
    "\n",
    "**1. Hard Failure** (no convergence, max_iter reached, high gradient norm)\n",
    "- **Mode='strict'**: Raise error, direct user to fix model/data\n",
    "- **Mode='warn'**: Raise error, mention `allow_fallback_priors=True` option\n",
    "- **allow_fallback_priors=True**: Use generic weakly informative priors + BIG warning\n",
    "\n",
    "**2. Suspicious Convergence** (high condition number, params near bounds, high uncertainty)\n",
    "- **Mode='warn'**: Log warning, use safer priors decoupled from Hessian\n",
    "- **Mode='auto_widen'**: Center at NLSQ, inflate std to avoid overly tight priors\n",
    "\n",
    "**3. Good Convergence** (low condition number, reasonable residuals)\n",
    "- Use NLSQ estimates and covariance for prior construction\n",
    "- Cap minimum std to avoid delta-like distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Part 1: Generate Synthetic Data and NLSQ Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T21:18:38.114469Z",
     "iopub.status.busy": "2026-02-09T21:18:38.114305Z",
     "iopub.status.idle": "2026-02-09T21:18:38.379426Z",
     "shell.execute_reply": "2026-02-09T21:18:38.378901Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate synthetic 3-mode relaxation data\n",
    "t = np.logspace(-2, 3, 80)  # Time from 0.01 to 1000 seconds\n",
    "\n",
    "# Ground truth: 3 Maxwell modes + equilibrium modulus\n",
    "G_inf_true = 1e4  # Pa\n",
    "G_modes_true = np.array([5e5, 8e4, 3e4])  # Pa\n",
    "tau_modes_true = np.array([0.1, 1.0, 10.0])  # seconds\n",
    "\n",
    "# Generate relaxation modulus\n",
    "G_t_true = G_inf_true + np.sum([\n",
    "    G_modes_true[i] * np.exp(-t / tau_modes_true[i]) \n",
    "    for i in range(3)\n",
    "], axis=0)\n",
    "\n",
    "# Add 3% noise (slightly higher to test Bayesian uncertainty quantification)\n",
    "noise_level = 0.03\n",
    "noise = noise_level * G_t_true * np.random.randn(len(t))\n",
    "G_t_noisy = G_t_true + noise\n",
    "\n",
    "# Visualize data\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "plt.loglog(t, G_t_true, 'k-', linewidth=2, label='True (3 modes)', alpha=0.5)\n",
    "plt.loglog(t, G_t_noisy, 'o', markersize=4, label='Noisy data (3%)')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Relaxation Modulus G(t) (Pa)')\n",
    "plt.title('Synthetic Multi-Mode Relaxation Data for Bayesian Analysis')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "display(fig)\n",
    "plt.close(fig)\n",
    "\n",
    "print(f\"Generated {len(t)} data points with 3 Maxwell modes\")\n",
    "print(f\"Noise level: {noise_level*100:.1f}%\")\n",
    "print(f\"\\nGround truth parameters:\")\n",
    "print(f\"  G_inf = {G_inf_true:.2e} Pa\")\n",
    "for i in range(3):\n",
    "    print(f\"  Mode {i+1}: G={G_modes_true[i]:.2e} Pa, tau={tau_modes_true[i]:.2f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "### NLSQ Point Estimation (Step 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T21:18:38.380968Z",
     "iopub.status.busy": "2026-02-09T21:18:38.380833Z",
     "iopub.status.idle": "2026-02-09T21:18:41.891492Z",
     "shell.execute_reply": "2026-02-09T21:18:41.890997Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fit GMM with NLSQ (point estimation)\n",
    "gmm = GeneralizedMaxwell(n_modes=5, modulus_type='shear')\n",
    "\n",
    "print(\"Step 1: NLSQ Point Estimation\")\n",
    "print(\"Fitting GMM with n_modes=5 (will auto-minimize to N_opt)...\\n\")\n",
    "\n",
    "# Fit with element minimization\n",
    "gmm.fit(t, G_t_noisy, test_mode='relaxation', optimization_factor=1.5)\n",
    "\n",
    "n_opt = gmm._n_modes\n",
    "print(f\"\\nElement minimization: N=5 → N_opt={n_opt}\")\n",
    "\n",
    "# Extract NLSQ diagnostics\n",
    "if hasattr(gmm, '_nlsq_result'):\n",
    "    nlsq_result = gmm._nlsq_result\n",
    "    print(f\"\\nNLSQ Convergence Diagnostics:\")\n",
    "    print(f\"  Converged: {nlsq_result.success}\")\n",
    "    print(f\"  Iterations: {nlsq_result.nit}\")\n",
    "    print(f\"  Final cost: {nlsq_result.cost:.4e}\")\n",
    "    if hasattr(nlsq_result, 'gradient_norm'):\n",
    "        print(f\"  Gradient norm: {nlsq_result.gradient_norm:.4e}\")\n",
    "else:\n",
    "    print(\"\\nNLSQ diagnostics not available\")\n",
    "\n",
    "# Display fitted parameters\n",
    "print(f\"\\nNLSQ Fitted Parameters:\")\n",
    "G_inf_fit = gmm.parameters.get_value('G_inf')\n",
    "print(f\"  G_inf = {G_inf_fit:.2e} Pa (true: {G_inf_true:.2e} Pa)\")\n",
    "for i in range(1, n_opt + 1):\n",
    "    G_i = gmm.parameters.get_value(f'G_{i}')\n",
    "    tau_i = gmm.parameters.get_value(f'tau_{i}')\n",
    "    print(f\"  Mode {i}: G={G_i:.2e} Pa, tau={tau_i:.3e} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "### Check Prior Safety Classification\n",
    "\n",
    "Before running Bayesian inference, we check NLSQ convergence quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T21:18:41.892809Z",
     "iopub.status.busy": "2026-02-09T21:18:41.892714Z",
     "iopub.status.idle": "2026-02-09T21:18:41.895459Z",
     "shell.execute_reply": "2026-02-09T21:18:41.894951Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extract NLSQ convergence classification (if available)\n",
    "if hasattr(gmm, '_classify_nlsq_convergence'):\n",
    "    # This would normally be called internally during fit_bayesian()\n",
    "    # For demonstration, we'll check convergence manually\n",
    "    \n",
    "    if hasattr(gmm, '_nlsq_result'):\n",
    "        # Simplified convergence check (actual implementation is more detailed)\n",
    "        converged = gmm._nlsq_result.success\n",
    "        \n",
    "        if converged:\n",
    "            classification = \"good\"\n",
    "            print(\"Prior Safety Classification: GOOD\")\n",
    "            print(\"  ✓ NLSQ converged successfully\")\n",
    "            print(\"  ✓ Can use NLSQ estimates for warm-start priors\")\n",
    "            print(\"  ✓ Expected: low divergences, good R-hat, high ESS\")\n",
    "        else:\n",
    "            classification = \"suspicious\"\n",
    "            print(\"Prior Safety Classification: SUSPICIOUS\")\n",
    "            print(\"  ⚠ NLSQ convergence questionable\")\n",
    "            print(\"  ⚠ Will use safer priors decoupled from Hessian\")\n",
    "            print(\"  ⚠ Expected: moderate convergence, may need more warmup\")\n",
    "    else:\n",
    "        print(\"NLSQ result not available for classification\")\n",
    "else:\n",
    "    print(\"Prior safety classification not implemented in this version\")\n",
    "    print(\"Proceeding with standard Bayesian inference...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Part 2: Bayesian Inference with NLSQ Warm-Start (Step 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T21:18:41.896950Z",
     "iopub.status.busy": "2026-02-09T21:18:41.896797Z",
     "iopub.status.idle": "2026-02-09T21:18:51.406053Z",
     "shell.execute_reply": "2026-02-09T21:18:51.405413Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare initial values from NLSQ fit for warm-start\n",
    "initial_values = {}\n",
    "initial_values['G_inf'] = gmm.parameters.get_value('G_inf')\n",
    "for i in range(1, n_opt + 1):\n",
    "    initial_values[f'G_{i}'] = gmm.parameters.get_value(f'G_{i}')\n",
    "    initial_values[f'tau_{i}'] = gmm.parameters.get_value(f'tau_{i}')\n",
    "\n",
    "print(\"Step 2: Bayesian Inference with NUTS\")\n",
    "print(f\"Warm-starting from NLSQ point estimates ({len(initial_values)} parameters)\\n\")\n",
    "\n",
    "# Run Bayesian inference\n",
    "print(\"Running MCMC sampling (this may take 1-2 minutes)...\")\n",
    "result = gmm.fit_bayesian(\n",
    "    t, G_t_noisy,\n",
    "    num_warmup=500,   # Reduced for notebook speed\n",
    "    num_samples=1000, # Reduced for notebook speed\n",
    "    num_chains=1,     # Single chain for speed\n",
    "    initial_values=initial_values,\n",
    ")\n",
    "\n",
    "print(\"\\nMCMC sampling complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "### Convergence Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T21:18:51.407744Z",
     "iopub.status.busy": "2026-02-09T21:18:51.407627Z",
     "iopub.status.idle": "2026-02-09T21:18:51.411240Z",
     "shell.execute_reply": "2026-02-09T21:18:51.410611Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check MCMC convergence diagnostics\n",
    "print(\"MCMC Convergence Diagnostics:\\n\")\n",
    "\n",
    "# R-hat (should be < 1.01)\n",
    "print(\"R-hat (Gelman-Rubin statistic, target < 1.01):\")\n",
    "for param_name in ['G_inf'] + [f'G_{i}' for i in range(1, n_opt + 1)] + [f'tau_{i}' for i in range(1, n_opt + 1)]:\n",
    "    if param_name in result.diagnostics['r_hat']:\n",
    "        rhat = result.diagnostics['r_hat'][param_name]\n",
    "        status = \"✓\" if rhat < 1.01 else \"⚠\"\n",
    "        print(f\"  {status} {param_name}: {rhat:.4f}\")\n",
    "\n",
    "# ESS (should be > 400)\n",
    "print(\"\\nEffective Sample Size (ESS, target > 400):\")\n",
    "for param_name in ['G_inf'] + [f'G_{i}' for i in range(1, n_opt + 1)]:\n",
    "    if param_name in result.diagnostics['ess']:\n",
    "        ess = result.diagnostics['ess'][param_name]\n",
    "        status = \"✓\" if ess > 400 else \"⚠\"\n",
    "        print(f\"  {status} {param_name}: {ess:.0f}\")\n",
    "\n",
    "# Divergences\n",
    "if 'num_divergences' in result.diagnostics:\n",
    "    num_div = result.diagnostics['num_divergences']\n",
    "    div_rate = num_div / result.num_samples\n",
    "    status = \"✓\" if div_rate < 0.01 else \"⚠\"\n",
    "    print(f\"\\nDivergences: {status} {num_div}/{result.num_samples} ({div_rate*100:.2f}%, target < 1%)\")\n",
    "\n",
    "print(\"\\n✓ indicates good convergence, ⚠ indicates potential issues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Part 3: Posterior Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T21:18:51.412464Z",
     "iopub.status.busy": "2026-02-09T21:18:51.412356Z",
     "iopub.status.idle": "2026-02-09T21:18:51.415899Z",
     "shell.execute_reply": "2026-02-09T21:18:51.415498Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extract posterior samples\n",
    "print(\"Posterior Summary Statistics:\\n\")\n",
    "\n",
    "# G_inf posterior\n",
    "G_inf_posterior = result.posterior_samples['G_inf']\n",
    "print(f\"G_inf:\")\n",
    "print(f\"  NLSQ point:    {G_inf_fit:.2e} Pa\")\n",
    "print(f\"  Posterior mean: {np.mean(G_inf_posterior):.2e} Pa\")\n",
    "print(f\"  Posterior std:  {np.std(G_inf_posterior):.2e} Pa\")\n",
    "print(f\"  True value:     {G_inf_true:.2e} Pa\\n\")\n",
    "\n",
    "# Mode parameters\n",
    "for i in range(1, n_opt + 1):\n",
    "    G_i_post = result.posterior_samples[f'G_{i}']\n",
    "    tau_i_post = result.posterior_samples[f'tau_{i}']\n",
    "    \n",
    "    print(f\"Mode {i}:\")\n",
    "    print(f\"  G_{i}:   mean={np.mean(G_i_post):.2e} Pa, std={np.std(G_i_post):.2e} Pa\")\n",
    "    print(f\"  tau_{i}: mean={np.mean(tau_i_post):.3e} s,  std={np.std(tau_i_post):.3e} s\")\n",
    "    \n",
    "    # Compare to true if within range\n",
    "    if i <= len(G_modes_true):\n",
    "        print(f\"  (True: G={G_modes_true[i-1]:.2e} Pa, tau={tau_modes_true[i-1]:.2f} s)\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "### Credible Intervals (95%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T21:18:51.417071Z",
     "iopub.status.busy": "2026-02-09T21:18:51.416988Z",
     "iopub.status.idle": "2026-02-09T21:18:51.420172Z",
     "shell.execute_reply": "2026-02-09T21:18:51.419811Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute 95% credible intervals\n",
    "intervals = gmm.get_credible_intervals(result.posterior_samples, credibility=0.95)\n",
    "\n",
    "print(\"95% Credible Intervals:\\n\")\n",
    "\n",
    "for param_name in ['G_inf'] + [f'G_{i}' for i in range(1, n_opt + 1)] + [f'tau_{i}' for i in range(1, n_opt + 1)]:\n",
    "    if param_name in intervals:\n",
    "        lower, upper = intervals[param_name]\n",
    "        mean_val = np.mean(result.posterior_samples[param_name])\n",
    "        print(f\"{param_name:8s}: [{lower:.3e}, {upper:.3e}] (mean: {mean_val:.3e})\")\n",
    "\n",
    "print(\"\\n95% credible intervals represent uncertainty in each parameter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## Part 4: ArviZ Diagnostic Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df716a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T21:18:51.421324Z",
     "iopub.status.busy": "2026-02-09T21:18:51.421249Z",
     "iopub.status.idle": "2026-02-09T21:18:52.646501Z",
     "shell.execute_reply": "2026-02-09T21:18:52.646014Z"
    }
   },
   "outputs": [],
   "source": [
    "# ArviZ diagnostic plots (trace, pair, forest, energy, autocorr, rank)\n",
    "display_arviz_diagnostics(result, ['G_inf'] + [f'G_{i}' for i in range(1, min(n_opt + 1, 3))], fast_mode=FAST_MODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## Part 5: Posterior Predictive Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T21:18:52.648193Z",
     "iopub.status.busy": "2026-02-09T21:18:52.647925Z",
     "iopub.status.idle": "2026-02-09T21:18:52.829241Z",
     "shell.execute_reply": "2026-02-09T21:18:52.828746Z"
    }
   },
   "outputs": [],
   "source": [
    "# Sample from posterior predictive distribution\n",
    "print(\"Posterior Predictive Check: Does model capture data distribution?\\n\")\n",
    "\n",
    "# Use posterior samples to generate predictions\n",
    "n_posterior_samples = 100  # Sample 100 parameter sets from posterior\n",
    "sample_indices = np.random.choice(len(G_inf_posterior), n_posterior_samples, replace=False)\n",
    "\n",
    "predictions = []\n",
    "for idx in sample_indices:\n",
    "    # Set parameters to posterior sample\n",
    "    gmm.parameters.set_value('G_inf', G_inf_posterior[idx])\n",
    "    for i in range(1, n_opt + 1):\n",
    "        gmm.parameters.set_value(f'G_{i}', result.posterior_samples[f'G_{i}'][idx])\n",
    "        gmm.parameters.set_value(f'tau_{i}', result.posterior_samples[f'tau_{i}'][idx])\n",
    "    \n",
    "    # Predict with this parameter set\n",
    "    predictions.append(gmm.predict(t))\n",
    "\n",
    "predictions = np.array(predictions)\n",
    "\n",
    "# Compute prediction intervals\n",
    "pred_mean = np.mean(predictions, axis=0)\n",
    "pred_lower = np.percentile(predictions, 2.5, axis=0)\n",
    "pred_upper = np.percentile(predictions, 97.5, axis=0)\n",
    "\n",
    "# Plot posterior predictive\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "plt.loglog(t, G_t_true, 'k-', linewidth=2, label='True', alpha=0.5)\n",
    "plt.loglog(t, G_t_noisy, 'o', markersize=4, alpha=0.6, label='Data')\n",
    "plt.loglog(t, pred_mean, 'r-', linewidth=2, label='Posterior mean')\n",
    "plt.fill_between(t, pred_lower, pred_upper, alpha=0.3, color='red', label='95% posterior predictive interval')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Relaxation Modulus G(t) (Pa)')\n",
    "plt.title('Posterior Predictive Distribution')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "display(fig)\n",
    "plt.close(fig)\n",
    "\n",
    "print(f\"Posterior predictive samples: {n_posterior_samples}\")\n",
    "print(f\"95% interval captures {np.sum((G_t_noisy >= pred_lower) & (G_t_noisy <= pred_upper))/len(t)*100:.1f}% of data points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## Part 6: Model Comparison (N=1 vs N=3)\n",
    "\n",
    "Compare Bayesian inference for single-mode (N=1) vs optimized multi-mode (N=3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T21:18:52.830526Z",
     "iopub.status.busy": "2026-02-09T21:18:52.830374Z",
     "iopub.status.idle": "2026-02-09T21:18:54.128954Z",
     "shell.execute_reply": "2026-02-09T21:18:54.128543Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fit N=1 model for comparison\n",
    "print(\"Model Comparison: N=1 (single Maxwell) vs N=3 (multi-mode GMM)\\n\")\n",
    "\n",
    "gmm_n1 = GeneralizedMaxwell(n_modes=1, modulus_type='shear')\n",
    "gmm_n1.fit(t, G_t_noisy, test_mode='relaxation')\n",
    "\n",
    "# NLSQ predictions\n",
    "pred_n1 = gmm_n1.predict(t)\n",
    "pred_n3 = gmm.predict(t)  # Using N_opt from earlier (should be 3)\n",
    "\n",
    "# Compute R²\n",
    "r2_n1 = 1 - np.sum((G_t_noisy - pred_n1)**2) / np.sum((G_t_noisy - np.mean(G_t_noisy))**2)\n",
    "r2_n3 = 1 - np.sum((G_t_noisy - pred_n3)**2) / np.sum((G_t_noisy - np.mean(G_t_noisy))**2)\n",
    "\n",
    "# Plot comparison\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "plt.loglog(t, G_t_true, 'k-', linewidth=3, label='True (3 modes)', alpha=0.5)\n",
    "plt.loglog(t, G_t_noisy, 'o', markersize=4, alpha=0.4, label='Data')\n",
    "plt.loglog(t, pred_n1, '--', linewidth=2, label=f'N=1 (R²={r2_n1:.4f})', color='blue')\n",
    "plt.loglog(t, pred_n3, '-', linewidth=2, label=f'N={n_opt} (R²={r2_n3:.4f})', color='red')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Relaxation Modulus G(t) (Pa)')\n",
    "plt.title('Model Comparison: Single vs Multi-Mode GMM')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "display(fig)\n",
    "plt.close(fig)\n",
    "\n",
    "print(f\"Fit quality comparison:\")\n",
    "print(f\"  N=1: R² = {r2_n1:.6f}\")\n",
    "print(f\"  N={n_opt}: R² = {r2_n3:.6f}\")\n",
    "print(f\"  Improvement: {(r2_n3 - r2_n1)*100:.2f}% increase\")\n",
    "print(f\"\\nMulti-mode GMM significantly outperforms single Maxwell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "## Interpretation and Insights\n",
    "\n",
    "### Two-Step Workflow Benefits\n",
    "\n",
    "1. **NLSQ provides fast point estimates** (seconds to minutes)\n",
    "2. **NUTS warm-start reduces MCMC time** by 2-5x (fewer warmup iterations needed)\n",
    "3. **Diagnostics guide prior selection** via tiered safety mechanism\n",
    "4. **Full uncertainty quantification** through posterior distributions\n",
    "\n",
    "### Prior Safety Mechanism\n",
    "\n",
    "- **Prevents misleading posteriors** when NLSQ fails\n",
    "- **Automatic classification**: hard failure → suspicious → good\n",
    "- **User control**: `prior_mode='strict'` (fail-fast), `'warn'` (default), `'auto_widen'` (expert)\n",
    "- **Opt-in fallback**: `allow_fallback_priors=True` for generic weakly informative priors\n",
    "\n",
    "### Convergence Diagnostics Interpretation\n",
    "\n",
    "- **R-hat < 1.01**: All chains converged to same posterior (good)\n",
    "- **ESS > 400**: Sufficient effective samples for reliable estimates\n",
    "- **Divergences < 1%**: NUTS sampler well-behaved (no pathological geometry)\n",
    "- **Trace plot 'fuzzy caterpillar'**: Good mixing across parameter space\n",
    "\n",
    "### Model Comparison Insights\n",
    "\n",
    "- **N=1 underfits** multi-mode data (low R²)\n",
    "- **N=3 captures complexity** without overfitting (element minimization prevents N>3)\n",
    "- **Bayesian uncertainty** quantifies parameter non-identifiability\n",
    "\n",
    "### When to Use Bayesian GMM\n",
    "\n",
    "- **Uncertainty quantification** for material properties\n",
    "- **Model comparison** via posterior predictive checks\n",
    "- **Parameter correlations** (pair plots reveal identifiability issues)\n",
    "- **Propagating uncertainty** to downstream predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acae54e37e7d407bbb7b55eff062a284",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "- **Baumgaertel & Winter (1989)**: [\"Determination of discrete relaxation and retardation time spectra\"](https://doi.org/10.1007/BF01331356) — Prony series methods\n",
    "- **Stadler et al. (2008)**: [\"Multi-mode viscoelastic models\"](https://doi.org/10.1007/s00397-007-0187-6) — GMM applications to polymers\n",
    "- **RheoJAX GMM Guide**: [Documentation](../../docs/source/models/multi_mode/gmm.rst) — Implementation details and optimization strategies\n",
    "- **ArviZ Multi-Parameter Models**: [Examples](https://python.arviz.org/en/stable/examples/index.html#high-dimensional-models) — Visualizing complex posteriors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a63283cbaf04dbcab1f6479b197f3a8",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- **[advanced/01-multi-technique-fitting.ipynb](../advanced/01-multi-technique-fitting.ipynb)**: Fit GMM to multiple test protocols simultaneously\n",
    "- **[advanced/06-frequentist-model-selection.ipynb](../advanced/06-frequentist-model-selection.ipynb)**: Compare Bayesian vs frequentist GMM mode selection\n",
    "- Apply GMM Bayesian workflow to your own multi-relaxation-time materials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": "### Apply Bayesian GMM to Your Data\n\n1. **Prepare data:** Relaxation or oscillation test (time or frequency domain)\n2. **Initial fit:** NLSQ with n_modes=5, let element minimization determine N_opt\n3. **Bayesian inference:** Warm-start from NLSQ, use num_chains=4 for robust diagnostics\n4. **Check convergence:** Verify $\\hat{R}$ < 1.01, ESS > 400 for all parameters\n5. **Interpret posteriors:** Analyze mode credible intervals and correlations\n6. **Report:** Document N_opt, posterior means ± 95% CI, convergence metrics\n\n### Key References\n\n- **Ferry, J.D. (1980).** *Viscoelastic Properties of Polymers*. 3rd ed. Wiley. [Chapter 4: Relaxation spectra - Physical interpretation of Maxwell modes]\n- **Honerkamp, J. & Weese, J. (1989).** \"Determination of the relaxation spectrum by a regularization method.\" *Macromolecules* 22:4372-4377. [Classic regularization vs Bayesian comparison]\n- **Bae, J.-E. & Cho, K.S. (2015).** \"Logarithmic method for continuous relaxation spectrum.\" *J. Rheol.* 59:1081-1112. [Modern continuous spectrum methods]"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rheojax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}