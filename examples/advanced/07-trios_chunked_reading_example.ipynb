{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRIOS Chunked Reading Examples\n",
    "\n",
    "This notebook demonstrates memory-efficient chunked reading of large TRIOS files,\n",
    "particularly useful for OWChirp arbitrary wave files (66-80 MB, 150k+ points).\n",
    "\n",
    "## Memory Savings\n",
    "- **Traditional loading**: ~12 MB for 150k points\n",
    "- **Chunked loading (10k chunks)**: ~800 KB peak memory usage\n",
    "- **Reduction**: ~93% memory savings\n",
    "\n",
    "**Author**: Rheo development team  \n",
    "**Date**: 2025-10-31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T21:39:53.642234Z",
     "iopub.status.busy": "2026-02-02T21:39:53.642018Z",
     "iopub.status.idle": "2026-02-02T21:39:53.650166Z",
     "shell.execute_reply": "2026-02-02T21:39:53.647907Z"
    }
   },
   "outputs": [],
   "source": [
    "# Google Colab Setup - Run this cell first!\n",
    "# Skip if running locally with rheojax already installed\n",
    "\n",
    "import sys\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Install rheojax and dependencies\n",
    "    !pip install -q rheojax\n",
    "    \n",
    "    # Colab uses float32 by default - we need float64 for numerical stability\n",
    "    # This MUST be set before importing JAX\n",
    "    import os\n",
    "    os.environ['JAX_ENABLE_X64'] = 'true'\n",
    "    \n",
    "    print(\"✓ RheoJAX installed successfully!\")\n",
    "    print(\"✓ Float64 precision enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T21:39:53.656908Z",
     "iopub.status.busy": "2026-02-02T21:39:53.656756Z",
     "iopub.status.idle": "2026-02-02T21:39:56.576360Z",
     "shell.execute_reply": "2026-02-02T21:39:56.573992Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from rheojax.models import Maxwell\n",
    "\n",
    "from rheojax.io.readers.trios import load_trios, load_trios_chunked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Basic Chunked Reading\n",
    "\n",
    "Iterate through chunks of a large file to process data without loading everything into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T21:39:56.579548Z",
     "iopub.status.busy": "2026-02-02T21:39:56.578609Z",
     "iopub.status.idle": "2026-02-02T21:39:56.585133Z",
     "shell.execute_reply": "2026-02-02T21:39:56.583263Z"
    }
   },
   "outputs": [],
   "source": [
    "def example_basic_chunked_reading():\n",
    "    \"\"\"Basic chunked reading example - iterate through chunks.\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Example 1: Basic Chunked Reading\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # For large files (> 10 MB, > 50k points), use chunked reading\n",
    "    for i, chunk in enumerate(load_trios_chunked('large_file.txt', chunk_size=10000)):\n",
    "        print(f\"Chunk {i + 1}:\")\n",
    "        print(f\"  Points: {len(chunk.x)}\")\n",
    "        print(f\"  Time range: {chunk.x.min():.3f} - {chunk.x.max():.3f} s\")\n",
    "        print(f\"  Stress range: {chunk.y.min():.2f} - {chunk.y.max():.2f} Pa\")\n",
    "        print()\n",
    "\n",
    "    # Notes:\n",
    "    # - Each chunk is a complete RheoData object with metadata\n",
    "    # - Chunk boundaries are arbitrary (based on row count, not time)\n",
    "    # - File handle automatically closes when iteration completes\n",
    "\n",
    "# Note: This example requires an actual TRIOS file to run\n",
    "# example_basic_chunked_reading()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Aggregating Results Across Chunks\n",
    "\n",
    "Compute statistics across chunks without loading the entire file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T21:39:56.588718Z",
     "iopub.status.busy": "2026-02-02T21:39:56.588558Z",
     "iopub.status.idle": "2026-02-02T21:39:56.594649Z",
     "shell.execute_reply": "2026-02-02T21:39:56.591658Z"
    }
   },
   "outputs": [],
   "source": [
    "def example_aggregate_statistics():\n",
    "    \"\"\"Compute statistics across chunks without loading entire file.\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Example 2: Aggregating Statistics\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Compute global statistics efficiently\n",
    "    total_points = 0\n",
    "    max_stress = -float('inf')\n",
    "    min_stress = float('inf')\n",
    "    sum_stress = 0.0\n",
    "\n",
    "    for chunk in load_trios_chunked('large_file.txt', chunk_size=10000):\n",
    "        total_points += len(chunk.x)\n",
    "        max_stress = max(max_stress, float(chunk.y.max()))\n",
    "        min_stress = min(min_stress, float(chunk.y.min()))\n",
    "        sum_stress += float(chunk.y.sum())\n",
    "\n",
    "    mean_stress = sum_stress / total_points\n",
    "\n",
    "    print(f\"Total data points: {total_points}\")\n",
    "    print(f\"Max stress: {max_stress:.2f} Pa\")\n",
    "    print(f\"Min stress: {min_stress:.2f} Pa\")\n",
    "    print(f\"Mean stress: {mean_stress:.2f} Pa\")\n",
    "    print()\n",
    "\n",
    "    # Memory usage: Only ~800 KB at any time (for 10k chunk_size)\n",
    "    # vs ~12 MB for full file load\n",
    "\n",
    "# example_aggregate_statistics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Model Fitting on Chunks\n",
    "\n",
    "Fit a rheological model to each chunk independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T21:39:56.598035Z",
     "iopub.status.busy": "2026-02-02T21:39:56.597901Z",
     "iopub.status.idle": "2026-02-02T21:39:56.609751Z",
     "shell.execute_reply": "2026-02-02T21:39:56.607545Z"
    }
   },
   "outputs": [],
   "source": [
    "def example_model_fitting_chunks():\n",
    "    \"\"\"Fit model to each chunk independently.\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Example 3: Model Fitting on Chunks\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    model = Maxwell()\n",
    "    chunk_results = []\n",
    "\n",
    "    for i, chunk in enumerate(load_trios_chunked('relaxation_data.txt', chunk_size=5000)):\n",
    "        # Fit Maxwell model to this chunk\n",
    "        model.fit(chunk.x, chunk.y)\n",
    "\n",
    "        # Store results\n",
    "        G0 = model.parameters.get_value('G0')\n",
    "        eta = model.parameters.get_value('eta')\n",
    "\n",
    "        chunk_results.append({\n",
    "            'chunk': i + 1,\n",
    "            'points': len(chunk.x),\n",
    "            'G0': G0,\n",
    "            'eta': eta,\n",
    "            'time_range': (chunk.x.min(), chunk.x.max())\n",
    "        })\n",
    "\n",
    "        print(f\"Chunk {i + 1}: G0 = {G0:.3e} Pa, eta = {eta:.3e} Pa·s\")\n",
    "\n",
    "    # Analyze how parameters evolve across chunks\n",
    "    print(\"\\nParameter evolution:\")\n",
    "    print(f\"  G0 variation: {np.std([r['G0'] for r in chunk_results]):.3e} Pa\")\n",
    "    print(f\"  eta variation: {np.std([r['eta'] for r in chunk_results]):.3e} Pa·s\")\n",
    "    print()\n",
    "\n",
    "# example_model_fitting_chunks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Processing Specific Segment\n",
    "\n",
    "TRIOS files often contain multiple procedure steps. Process only a specific segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T21:39:56.613564Z",
     "iopub.status.busy": "2026-02-02T21:39:56.613390Z",
     "iopub.status.idle": "2026-02-02T21:39:56.618929Z",
     "shell.execute_reply": "2026-02-02T21:39:56.617399Z"
    }
   },
   "outputs": [],
   "source": [
    "def example_segment_selection():\n",
    "    \"\"\"Process only a specific segment from multi-segment file.\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Example 4: Segment Selection\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # TRIOS files often contain multiple procedure steps\n",
    "    # Process only segment 2 (0-indexed)\n",
    "\n",
    "    print(\"Processing only segment index 1:\")\n",
    "    for i, chunk in enumerate(\n",
    "        load_trios_chunked(\n",
    "            'multi_step.txt',\n",
    "            chunk_size=10000,\n",
    "            segment_index=1  # Second segment\n",
    "        )\n",
    "    ):\n",
    "        print(f\"  Chunk {i + 1}: {len(chunk.x)} points\")\n",
    "        print(f\"    Test mode: {chunk.metadata.get('test_mode', 'unknown')}\")\n",
    "\n",
    "    print()\n",
    "\n",
    "# example_segment_selection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5: Comparison - Chunked vs Full Load\n",
    "\n",
    "Compare memory usage and verify both methods process the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T21:39:56.622023Z",
     "iopub.status.busy": "2026-02-02T21:39:56.621789Z",
     "iopub.status.idle": "2026-02-02T21:39:56.627016Z",
     "shell.execute_reply": "2026-02-02T21:39:56.625788Z"
    }
   },
   "outputs": [],
   "source": [
    "def example_compare_methods(filepath: str):\n",
    "    \"\"\"Compare chunked vs full loading for same file.\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Example 5: Chunked vs Full Loading Comparison\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Method 1: Full load (high memory)\n",
    "    print(\"Method 1: Full loading\")\n",
    "    full_data = load_trios(filepath)\n",
    "    print(f\"  Total points: {len(full_data.x)}\")\n",
    "    print(f\"  Memory usage: ~{len(full_data.x) * 80 / 1024 / 1024:.1f} MB\")\n",
    "    print()\n",
    "\n",
    "    # Method 2: Chunked load (low memory)\n",
    "    print(\"Method 2: Chunked loading (10k chunks)\")\n",
    "    chunk_count = 0\n",
    "    total_points_chunked = 0\n",
    "\n",
    "    for chunk in load_trios_chunked(filepath, chunk_size=10000):\n",
    "        chunk_count += 1\n",
    "        total_points_chunked += len(chunk.x)\n",
    "\n",
    "    print(f\"  Total chunks: {chunk_count}\")\n",
    "    print(f\"  Total points: {total_points_chunked}\")\n",
    "    print(f\"  Peak memory usage: ~{10000 * 80 / 1024 / 1024:.1f} MB\")\n",
    "    print()\n",
    "\n",
    "    # Verify equivalence\n",
    "    assert len(full_data.x) == total_points_chunked\n",
    "    print(\"✓ Both methods processed same number of points\")\n",
    "    print()\n",
    "\n",
    "# example_compare_methods('your_file.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 6: Backward Compatibility\n",
    "\n",
    "Use `load_trios` with `chunk_size` parameter for backward compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T21:39:56.629621Z",
     "iopub.status.busy": "2026-02-02T21:39:56.629513Z",
     "iopub.status.idle": "2026-02-02T21:39:56.635031Z",
     "shell.execute_reply": "2026-02-02T21:39:56.632590Z"
    }
   },
   "outputs": [],
   "source": [
    "def example_backward_compatibility():\n",
    "    \"\"\"Use load_trios with chunk_size parameter for compatibility.\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Example 6: Backward Compatibility\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Option 1: Direct chunked reading (recommended)\n",
    "    chunks_list = list(load_trios_chunked('data.txt', chunk_size=10000))\n",
    "    print(f\"Option 1 (load_trios_chunked): {len(chunks_list)} chunks\")\n",
    "\n",
    "    # Option 2: Using load_trios with chunk_size (backward compatible)\n",
    "    result = load_trios('data.txt', chunk_size=10000)\n",
    "    if isinstance(result, list):\n",
    "        print(f\"Option 2 (load_trios): {len(result)} chunks\")\n",
    "    else:\n",
    "        print(f\"Option 2 (load_trios): Single RheoData object\")\n",
    "\n",
    "    print(\"\\nBoth methods produce identical results.\")\n",
    "    print()\n",
    "\n",
    "# example_backward_compatibility()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 7: Real-World OWChirp Processing\n",
    "\n",
    "Process large OWChirp arbitrary wave files efficiently.\n",
    "\n",
    "OWChirp files are typically:\n",
    "- 66-80 MB file size\n",
    "- 150,000+ data points\n",
    "- High-frequency sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T21:39:56.637709Z",
     "iopub.status.busy": "2026-02-02T21:39:56.637438Z",
     "iopub.status.idle": "2026-02-02T21:39:56.642990Z",
     "shell.execute_reply": "2026-02-02T21:39:56.641612Z"
    }
   },
   "outputs": [],
   "source": [
    "def example_owchirp_processing():\n",
    "    \"\"\"Process large OWChirp arbitrary wave file efficiently.\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Example 7: OWChirp File Processing\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    owchirp_file = 'owchirp_data.txt'\n",
    "\n",
    "    print(f\"Processing OWChirp file: {owchirp_file}\")\n",
    "    print(\"File characteristics:\")\n",
    "    print(\"  - Size: ~70 MB\")\n",
    "    print(\"  - Points: ~150,000\")\n",
    "    print(\"  - Traditional loading: ~12 MB memory\")\n",
    "    print(\"  - Chunked loading: ~800 KB memory (10k chunks)\")\n",
    "    print()\n",
    "\n",
    "    # Process in chunks\n",
    "    print(\"Processing chunks:\")\n",
    "    chunk_statistics = []\n",
    "\n",
    "    for i, chunk in enumerate(load_trios_chunked(owchirp_file, chunk_size=10000)):\n",
    "        # Compute chunk statistics\n",
    "        stats = {\n",
    "            'chunk': i + 1,\n",
    "            'points': len(chunk.x),\n",
    "            'time_start': float(chunk.x[0]),\n",
    "            'time_end': float(chunk.x[-1]),\n",
    "            'stress_mean': float(chunk.y.mean()),\n",
    "            'stress_std': float(chunk.y.std()),\n",
    "            'stress_max': float(chunk.y.max()),\n",
    "        }\n",
    "        chunk_statistics.append(stats)\n",
    "\n",
    "        # Print progress every 5 chunks\n",
    "        if (i + 1) % 5 == 0:\n",
    "            print(f\"  Processed {i + 1} chunks ({(i + 1) * 10000} points)...\")\n",
    "\n",
    "    print(f\"\\n✓ Completed: {len(chunk_statistics)} chunks processed\")\n",
    "    print(f\"  Total points: {sum(s['points'] for s in chunk_statistics)}\")\n",
    "    print(f\"  Global max stress: {max(s['stress_max'] for s in chunk_statistics):.2f} Pa\")\n",
    "    print()\n",
    "\n",
    "# example_owchirp_processing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 8: Validation Disabled for Speed\n",
    "\n",
    "Disable validation checks for maximum reading speed when you trust the data quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T21:39:56.646410Z",
     "iopub.status.busy": "2026-02-02T21:39:56.646167Z",
     "iopub.status.idle": "2026-02-02T21:39:56.649957Z",
     "shell.execute_reply": "2026-02-02T21:39:56.648606Z"
    }
   },
   "outputs": [],
   "source": [
    "def example_validation_disabled():\n",
    "    \"\"\"Disable validation for maximum reading speed.\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Example 8: Validation Disabled\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    print(\"Reading with validation disabled (faster, but less safe):\")\n",
    "\n",
    "    chunk_count = 0\n",
    "    for chunk in load_trios_chunked(\n",
    "        'large_file.txt',\n",
    "        chunk_size=10000,\n",
    "        validate_data=False  # Skip validation for speed\n",
    "    ):\n",
    "        chunk_count += 1\n",
    "\n",
    "    print(f\"  Processed {chunk_count} chunks\")\n",
    "    print(\"\\nNote: Validation checks for NaN, non-finite values, and monotonicity.\")\n",
    "    print(\"      Disable only if you trust the data quality.\")\n",
    "    print()\n",
    "\n",
    "# example_validation_disabled()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 9: Memory-Efficient Data Export\n",
    "\n",
    "Export processed data in chunks to avoid memory issues with large files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T21:39:56.652978Z",
     "iopub.status.busy": "2026-02-02T21:39:56.652621Z",
     "iopub.status.idle": "2026-02-02T21:39:56.658167Z",
     "shell.execute_reply": "2026-02-02T21:39:56.656258Z"
    }
   },
   "outputs": [],
   "source": [
    "def example_chunked_export():\n",
    "    \"\"\"Export data in chunks to avoid memory issues.\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Example 9: Chunked Data Export\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    output_file = Path('processed_data.csv')\n",
    "\n",
    "    print(f\"Exporting processed data to: {output_file}\")\n",
    "\n",
    "    # Write header\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write(\"time,stress,processing_flag\\n\")\n",
    "\n",
    "    # Process and write chunks\n",
    "    chunk_count = 0\n",
    "    for chunk in load_trios_chunked('large_file.txt', chunk_size=10000):\n",
    "        # Process chunk (e.g., apply threshold)\n",
    "        threshold = 1000.0\n",
    "        flags = chunk.y > threshold\n",
    "\n",
    "        # Append to file\n",
    "        with open(output_file, 'a') as f:\n",
    "            for t, s, flag in zip(chunk.x, chunk.y, flags):\n",
    "                f.write(f\"{t:.6f},{s:.2f},{int(flag)}\\n\")\n",
    "\n",
    "        chunk_count += 1\n",
    "\n",
    "    print(f\"✓ Exported {chunk_count} chunks\")\n",
    "    print(f\"  Output file: {output_file}\")\n",
    "    print()\n",
    "\n",
    "# example_chunked_export()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 10: Choosing Optimal Chunk Size\n",
    "\n",
    "Guide for selecting optimal chunk size based on file size and memory constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T21:39:56.661368Z",
     "iopub.status.busy": "2026-02-02T21:39:56.660916Z",
     "iopub.status.idle": "2026-02-02T21:39:56.667797Z",
     "shell.execute_reply": "2026-02-02T21:39:56.667091Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Example 10: Choosing Optimal Chunk Size\n",
      "======================================================================\n",
      "Chunk Size Guidelines:\n",
      "\n",
      "Small chunks (1,000 - 5,000 points):\n",
      "  Memory: ~80-400 KB\n",
      "  Use case: Very limited memory, point-by-point processing\n",
      "  Trade-off: More overhead, slower overall\n",
      "\n",
      "Medium chunks (5,000 - 20,000 points): [RECOMMENDED]\n",
      "  Memory: ~400 KB - 1.6 MB\n",
      "  Use case: Most applications, good balance\n",
      "  Trade-off: Optimal for typical workflows\n",
      "\n",
      "Large chunks (20,000 - 50,000 points):\n",
      "  Memory: ~1.6 - 4 MB\n",
      "  Use case: Ample memory, fewer iterations needed\n",
      "  Trade-off: Higher memory, faster processing\n",
      "\n",
      "For your file (70 MB, ~150000 points):\n",
      "  Recommended chunk size: 10000\n",
      "  Number of chunks: ~15\n",
      "  Memory per chunk: ~0.8 MB\n",
      "  Total memory saved: ~69.2 MB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def example_chunk_size_selection():\n",
    "    \"\"\"Guide for selecting optimal chunk size.\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Example 10: Choosing Optimal Chunk Size\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    file_size_mb = 70  # Example: 70 MB file\n",
    "    estimated_points = 150000  # Example: 150k points\n",
    "\n",
    "    print(\"Chunk Size Guidelines:\")\n",
    "    print()\n",
    "\n",
    "    # Small chunks (1,000 - 5,000)\n",
    "    print(\"Small chunks (1,000 - 5,000 points):\")\n",
    "    print(\"  Memory: ~80-400 KB\")\n",
    "    print(\"  Use case: Very limited memory, point-by-point processing\")\n",
    "    print(\"  Trade-off: More overhead, slower overall\")\n",
    "    print()\n",
    "\n",
    "    # Medium chunks (5,000 - 20,000) - RECOMMENDED\n",
    "    print(\"Medium chunks (5,000 - 20,000 points): [RECOMMENDED]\")\n",
    "    print(\"  Memory: ~400 KB - 1.6 MB\")\n",
    "    print(\"  Use case: Most applications, good balance\")\n",
    "    print(\"  Trade-off: Optimal for typical workflows\")\n",
    "    print()\n",
    "\n",
    "    # Large chunks (20,000 - 50,000)\n",
    "    print(\"Large chunks (20,000 - 50,000 points):\")\n",
    "    print(\"  Memory: ~1.6 - 4 MB\")\n",
    "    print(\"  Use case: Ample memory, fewer iterations needed\")\n",
    "    print(\"  Trade-off: Higher memory, faster processing\")\n",
    "    print()\n",
    "\n",
    "    # Recommendation for this file\n",
    "    recommended_chunk_size = 10000\n",
    "    estimated_chunks = (estimated_points + recommended_chunk_size - 1) // recommended_chunk_size\n",
    "    memory_per_chunk_mb = recommended_chunk_size * 80 / 1024 / 1024\n",
    "\n",
    "    print(f\"For your file ({file_size_mb} MB, ~{estimated_points} points):\")\n",
    "    print(f\"  Recommended chunk size: {recommended_chunk_size}\")\n",
    "    print(f\"  Number of chunks: ~{estimated_chunks}\")\n",
    "    print(f\"  Memory per chunk: ~{memory_per_chunk_mb:.1f} MB\")\n",
    "    print(f\"  Total memory saved: ~{file_size_mb - memory_per_chunk_mb:.1f} MB\")\n",
    "    print()\n",
    "\n",
    "# Run this example (doesn't require files)\n",
    "example_chunk_size_selection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated 10 examples of memory-efficient chunked reading:\n",
    "\n",
    "1. **Basic chunked reading** - Iterate through data chunks\n",
    "2. **Aggregating statistics** - Compute global stats without full load\n",
    "3. **Model fitting on chunks** - Fit models to individual chunks\n",
    "4. **Segment selection** - Process specific segments from multi-step files\n",
    "5. **Method comparison** - Chunked vs full loading verification\n",
    "6. **Backward compatibility** - Using `load_trios` with `chunk_size`\n",
    "7. **OWChirp processing** - Real-world large file example\n",
    "8. **Validation disabled** - Speed optimization for trusted data\n",
    "9. **Chunked export** - Memory-efficient data export\n",
    "10. **Chunk size selection** - Guidelines for optimal performance\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- Use **10,000 chunk size** for most applications (good balance)\n",
    "- Chunked reading provides **~93% memory savings** for large files\n",
    "- Each chunk is a complete `RheoData` object with full metadata\n",
    "- File handles close automatically after iteration\n",
    "\n",
    "### For More Information\n",
    "\n",
    "- Documentation: `rheojax.io.readers.trios.load_trios_chunked`\n",
    "- Tests: `tests/io/test_trios_chunked.py`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rheojax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
