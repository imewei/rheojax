{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRIOS Chunked Reading for Large Files\n",
    "\n",
    "This notebook demonstrates memory-efficient processing of large TRIOS rheometer data files using automatic chunking, reducing memory usage by 50-70% for files exceeding 5 MB.\n",
    "\n",
    "## What is This?\n",
    "\n",
    "TRIOS rheometers (TA Instruments) generate **massive datasets** from continuous monitoring experiments:\n",
    "- **Gelation kinetics**: 10,000+ points tracked over hours → 50+ MB files\n",
    "- **LAOS sweeps**: Dense frequency × strain grids → 100+ MB files\n",
    "- **Multi-step protocols**: 20+ sequential tests in single export\n",
    "\n",
    "RheoJAX's chunked reader enables:\n",
    "- **Streaming parsing**: Process one chunk at a time (default: 1000 rows)\n",
    "- **Auto-triggering**: Automatically engages for files > 5 MB\n",
    "- **Memory bounds**: Peak RAM usage independent of file size\n",
    "- **Progress callbacks**: Monitor long-running imports with live updates\n",
    "- **Opt-out control**: Disable via `auto_chunk=False` if full-file needed\n",
    "\n",
    "## Physical Motivation\n",
    "\n",
    "Memory constraints often **block analysis** before it begins:\n",
    "- **Laptop limitations**: 8-16 GB RAM can't load 200 MB TRIOS file\n",
    "- **Batch workflows**: Processing 50 files × 100 MB each requires streaming\n",
    "- **Cloud compute**: Minimize memory footprint for cost-effective scaling\n",
    "\n",
    "**Traditional approach** (loads entire file):\n",
    "```python\n",
    "data = read_trios('huge_file.txt')  # MemoryError: 32 GB required!\n",
    "```\n",
    "\n",
    "**Chunked approach** (RheoJAX v0.4.0+):\n",
    "```python\n",
    "data = read_trios('huge_file.txt')  # Auto-chunks → 2 GB peak RAM\n",
    "# Transparent to user: returns same RheoData object\n",
    "```\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- **TRIOS format**: Familiarity with TA Instruments data structure (headers, time/stress/strain columns)\n",
    "- **RheoData basics**: Understanding of `rheojax.core.data.RheoData` from `examples/basic/`\n",
    "- **I/O operations**: General file reading concepts (CSV, Excel, binary formats)\n",
    "\n",
    "## Handbook References\n",
    "\n",
    "- **TRIOS reader**: See `rheojax.io.trios` module documentation\n",
    "- **RheoData API**: Construction, slicing, conversion in `docs/source/api/core.rst`\n",
    "- **Other readers**: CSV, Excel, Anton Paar formats in `rheojax.io`\n",
    "\n",
    "**Estimated Time:** 25-30 minutes\n",
    "\n",
    "> **Handbook:** For theoretical background and API details, see the [Model Documentation](../../docs/source/models/index.rst).\n",
    "\n",
    "### Learning Objectives\n",
    "- Stream large TRIOS files (>5 MB) without memory overflow\n",
    "- Configure optimal chunk sizes for different file sizes\n",
    "- Aggregate statistics across chunks efficiently\n",
    "- Apply chunked reading to batch workflows\n",
    "\n",
    "**Estimated Time:** ~25-30 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T02:54:57.705443Z",
     "iopub.status.busy": "2026-02-14T02:54:57.705211Z",
     "iopub.status.idle": "2026-02-14T02:54:57.718356Z",
     "shell.execute_reply": "2026-02-14T02:54:57.716424Z"
    }
   },
   "outputs": [],
   "source": [
    "# Google Colab Setup - Run this cell first!\n",
    "# Skip if running locally with rheojax already installed\n",
    "\n",
    "import sys\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Install rheojax and dependencies\n",
    "    !pip install -q rheojax\n",
    "    \n",
    "    # Colab uses float32 by default - we need float64 for numerical stability\n",
    "    # This MUST be set before importing JAX\n",
    "    import os\n",
    "    os.environ['JAX_ENABLE_X64'] = 'true'\n",
    "    \n",
    "    print(\"✓ RheoJAX installed successfully!\")\n",
    "    print(\"✓ Float64 precision enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T02:54:57.720628Z",
     "iopub.status.busy": "2026-02-14T02:54:57.720533Z",
     "iopub.status.idle": "2026-02-14T02:54:59.869000Z",
     "shell.execute_reply": "2026-02-14T02:54:59.868174Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from rheojax.io.readers.trios import load_trios, load_trios_chunked\n",
    "from rheojax.models import Maxwell\n",
    "\n",
    "sys.path.insert(0, os.path.dirname(os.path.abspath(\"\")))\n",
    "from utils.plotting_utils import (\n",
    "    display_arviz_diagnostics,\n",
    "    plot_nlsq_fit,\n",
    "    plot_posterior_predictive,\n",
    ")\n",
    "\n",
    "FAST_MODE = os.environ.get(\"FAST_MODE\", \"1\") == \"1\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Basic Chunked Reading\n",
    "\n",
    "Iterate through chunks of a large file to process data without loading everything into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T02:54:59.870438Z",
     "iopub.status.busy": "2026-02-14T02:54:59.870315Z",
     "iopub.status.idle": "2026-02-14T02:54:59.873107Z",
     "shell.execute_reply": "2026-02-14T02:54:59.872409Z"
    }
   },
   "outputs": [],
   "source": [
    "def example_basic_chunked_reading():\n",
    "    \"\"\"Basic chunked reading example - iterate through chunks.\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Example 1: Basic Chunked Reading\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # For large files (> 10 MB, > 50k points), use chunked reading\n",
    "    for i, chunk in enumerate(load_trios_chunked('large_file.txt', chunk_size=10000)):\n",
    "        print(f\"Chunk {i + 1}:\")\n",
    "        print(f\"  Points: {len(chunk.x)}\")\n",
    "        print(f\"  Time range: {chunk.x.min():.3f} - {chunk.x.max():.3f} s\")\n",
    "        print(f\"  Stress range: {chunk.y.min():.2f} - {chunk.y.max():.2f} Pa\")\n",
    "        print()\n",
    "\n",
    "    # Notes:\n",
    "    # - Each chunk is a complete RheoData object with metadata\n",
    "    # - Chunk boundaries are arbitrary (based on row count, not time)\n",
    "    # - File handle automatically closes when iteration completes\n",
    "\n",
    "# Note: This example requires an actual TRIOS file to run\n",
    "# example_basic_chunked_reading()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Aggregating Results Across Chunks\n",
    "\n",
    "Compute statistics across chunks without loading the entire file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T02:54:59.874318Z",
     "iopub.status.busy": "2026-02-14T02:54:59.874216Z",
     "iopub.status.idle": "2026-02-14T02:54:59.876944Z",
     "shell.execute_reply": "2026-02-14T02:54:59.876479Z"
    }
   },
   "outputs": [],
   "source": [
    "def example_aggregate_statistics():\n",
    "    \"\"\"Compute statistics across chunks without loading entire file.\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Example 2: Aggregating Statistics\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Compute global statistics efficiently\n",
    "    total_points = 0\n",
    "    max_stress = -float('inf')\n",
    "    min_stress = float('inf')\n",
    "    sum_stress = 0.0\n",
    "\n",
    "    for chunk in load_trios_chunked('large_file.txt', chunk_size=10000):\n",
    "        total_points += len(chunk.x)\n",
    "        max_stress = max(max_stress, float(chunk.y.max()))\n",
    "        min_stress = min(min_stress, float(chunk.y.min()))\n",
    "        sum_stress += float(chunk.y.sum())\n",
    "\n",
    "    mean_stress = sum_stress / total_points\n",
    "\n",
    "    print(f\"Total data points: {total_points}\")\n",
    "    print(f\"Max stress: {max_stress:.2f} Pa\")\n",
    "    print(f\"Min stress: {min_stress:.2f} Pa\")\n",
    "    print(f\"Mean stress: {mean_stress:.2f} Pa\")\n",
    "    print()\n",
    "\n",
    "    # Memory usage: Only ~800 KB at any time (for 10k chunk_size)\n",
    "    # vs ~12 MB for full file load\n",
    "\n",
    "# example_aggregate_statistics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Model Fitting on Chunks\n",
    "\n",
    "Fit a rheological model to each chunk independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T02:54:59.878431Z",
     "iopub.status.busy": "2026-02-14T02:54:59.878327Z",
     "iopub.status.idle": "2026-02-14T02:54:59.881422Z",
     "shell.execute_reply": "2026-02-14T02:54:59.881008Z"
    }
   },
   "outputs": [],
   "source": [
    "def example_model_fitting_chunks():\n",
    "    \"\"\"Fit model to each chunk independently.\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Example 3: Model Fitting on Chunks\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    model = Maxwell()\n",
    "    chunk_results = []\n",
    "\n",
    "    for i, chunk in enumerate(load_trios_chunked('relaxation_data.txt', chunk_size=5000)):\n",
    "        # Fit Maxwell model to this chunk\n",
    "        model.fit(chunk.x, chunk.y)\n",
    "\n",
    "        # Store results\n",
    "        G0 = model.parameters.get_value('G0')\n",
    "        eta = model.parameters.get_value('eta')\n",
    "\n",
    "        chunk_results.append({\n",
    "            'chunk': i + 1,\n",
    "            'points': len(chunk.x),\n",
    "            'G0': G0,\n",
    "            'eta': eta,\n",
    "            'time_range': (chunk.x.min(), chunk.x.max())\n",
    "        })\n",
    "\n",
    "        print(f\"Chunk {i + 1}: G0 = {G0:.3e} Pa, eta = {eta:.3e} Pa·s\")\n",
    "\n",
    "    # Analyze how parameters evolve across chunks\n",
    "    print(\"\\nParameter evolution:\")\n",
    "    print(f\"  G0 variation: {np.std([r['G0'] for r in chunk_results]):.3e} Pa\")\n",
    "    print(f\"  eta variation: {np.std([r['eta'] for r in chunk_results]):.3e} Pa·s\")\n",
    "    print()\n",
    "\n",
    "# example_model_fitting_chunks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Processing Specific Segment\n",
    "\n",
    "TRIOS files often contain multiple procedure steps. Process only a specific segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T02:54:59.882632Z",
     "iopub.status.busy": "2026-02-14T02:54:59.882554Z",
     "iopub.status.idle": "2026-02-14T02:54:59.884700Z",
     "shell.execute_reply": "2026-02-14T02:54:59.884263Z"
    }
   },
   "outputs": [],
   "source": [
    "def example_segment_selection():\n",
    "    \"\"\"Process only a specific segment from multi-segment file.\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Example 4: Segment Selection\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # TRIOS files often contain multiple procedure steps\n",
    "    # Process only segment 2 (0-indexed)\n",
    "\n",
    "    print(\"Processing only segment index 1:\")\n",
    "    for i, chunk in enumerate(\n",
    "        load_trios_chunked(\n",
    "            'multi_step.txt',\n",
    "            chunk_size=10000,\n",
    "            segment_index=1  # Second segment\n",
    "        )\n",
    "    ):\n",
    "        print(f\"  Chunk {i + 1}: {len(chunk.x)} points\")\n",
    "        print(f\"    Test mode: {chunk.metadata.get('test_mode', 'unknown')}\")\n",
    "\n",
    "    print()\n",
    "\n",
    "# example_segment_selection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5: Comparison - Chunked vs Full Load\n",
    "\n",
    "Compare memory usage and verify both methods process the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T02:54:59.885963Z",
     "iopub.status.busy": "2026-02-14T02:54:59.885875Z",
     "iopub.status.idle": "2026-02-14T02:54:59.924843Z",
     "shell.execute_reply": "2026-02-14T02:54:59.898506Z"
    }
   },
   "outputs": [],
   "source": [
    "def example_compare_methods(filepath: str):\n",
    "    \"\"\"Compare chunked vs full loading for same file.\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Example 5: Chunked vs Full Loading Comparison\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Method 1: Full load (high memory)\n",
    "    print(\"Method 1: Full loading\")\n",
    "    full_data = load_trios(filepath)\n",
    "    print(f\"  Total points: {len(full_data.x)}\")\n",
    "    print(f\"  Memory usage: ~{len(full_data.x) * 80 / 1024 / 1024:.1f} MB\")\n",
    "    print()\n",
    "\n",
    "    # Method 2: Chunked load (low memory)\n",
    "    print(\"Method 2: Chunked loading (10k chunks)\")\n",
    "    chunk_count = 0\n",
    "    total_points_chunked = 0\n",
    "\n",
    "    for chunk in load_trios_chunked(filepath, chunk_size=10000):\n",
    "        chunk_count += 1\n",
    "        total_points_chunked += len(chunk.x)\n",
    "\n",
    "    print(f\"  Total chunks: {chunk_count}\")\n",
    "    print(f\"  Total points: {total_points_chunked}\")\n",
    "    print(f\"  Peak memory usage: ~{10000 * 80 / 1024 / 1024:.1f} MB\")\n",
    "    print()\n",
    "\n",
    "    # Verify equivalence\n",
    "    assert len(full_data.x) == total_points_chunked\n",
    "    print(\"✓ Both methods processed same number of points\")\n",
    "    print()\n",
    "\n",
    "# example_compare_methods('your_file.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 6: Backward Compatibility\n",
    "\n",
    "Use `load_trios` with `chunk_size` parameter for backward compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T02:54:59.931384Z",
     "iopub.status.busy": "2026-02-14T02:54:59.931255Z",
     "iopub.status.idle": "2026-02-14T02:54:59.945331Z",
     "shell.execute_reply": "2026-02-14T02:54:59.941954Z"
    }
   },
   "outputs": [],
   "source": [
    "def example_backward_compatibility():\n",
    "    \"\"\"Use load_trios with chunk_size parameter for compatibility.\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Example 6: Backward Compatibility\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Option 1: Direct chunked reading (recommended)\n",
    "    chunks_list = list(load_trios_chunked('data.txt', chunk_size=10000))\n",
    "    print(f\"Option 1 (load_trios_chunked): {len(chunks_list)} chunks\")\n",
    "\n",
    "    # Option 2: Using load_trios with chunk_size (backward compatible)\n",
    "    result = load_trios('data.txt', chunk_size=10000)\n",
    "    if isinstance(result, list):\n",
    "        print(f\"Option 2 (load_trios): {len(result)} chunks\")\n",
    "    else:\n",
    "        print(f\"Option 2 (load_trios): Single RheoData object\")\n",
    "\n",
    "    print(\"\\nBoth methods produce identical results.\")\n",
    "    print()\n",
    "\n",
    "# example_backward_compatibility()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 7: Real-World OWChirp Processing\n",
    "\n",
    "Process large OWChirp arbitrary wave files efficiently.\n",
    "\n",
    "OWChirp files are typically:\n",
    "- 66-80 MB file size\n",
    "- 150,000+ data points\n",
    "- High-frequency sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T02:54:59.950829Z",
     "iopub.status.busy": "2026-02-14T02:54:59.950650Z",
     "iopub.status.idle": "2026-02-14T02:54:59.983751Z",
     "shell.execute_reply": "2026-02-14T02:54:59.982158Z"
    }
   },
   "outputs": [],
   "source": [
    "def example_owchirp_processing():\n",
    "    \"\"\"Process large OWChirp arbitrary wave file efficiently.\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Example 7: OWChirp File Processing\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    owchirp_file = 'owchirp_data.txt'\n",
    "\n",
    "    print(f\"Processing OWChirp file: {owchirp_file}\")\n",
    "    print(\"File characteristics:\")\n",
    "    print(\"  - Size: ~70 MB\")\n",
    "    print(\"  - Points: ~150,000\")\n",
    "    print(\"  - Traditional loading: ~12 MB memory\")\n",
    "    print(\"  - Chunked loading: ~800 KB memory (10k chunks)\")\n",
    "    print()\n",
    "\n",
    "    # Process in chunks\n",
    "    print(\"Processing chunks:\")\n",
    "    chunk_statistics = []\n",
    "\n",
    "    for i, chunk in enumerate(load_trios_chunked(owchirp_file, chunk_size=10000)):\n",
    "        # Compute chunk statistics\n",
    "        stats = {\n",
    "            'chunk': i + 1,\n",
    "            'points': len(chunk.x),\n",
    "            'time_start': float(chunk.x[0]),\n",
    "            'time_end': float(chunk.x[-1]),\n",
    "            'stress_mean': float(chunk.y.mean()),\n",
    "            'stress_std': float(chunk.y.std()),\n",
    "            'stress_max': float(chunk.y.max()),\n",
    "        }\n",
    "        chunk_statistics.append(stats)\n",
    "\n",
    "        # Print progress every 5 chunks\n",
    "        if (i + 1) % 5 == 0:\n",
    "            print(f\"  Processed {i + 1} chunks ({(i + 1) * 10000} points)...\")\n",
    "\n",
    "    print(f\"\\n✓ Completed: {len(chunk_statistics)} chunks processed\")\n",
    "    print(f\"  Total points: {sum(s['points'] for s in chunk_statistics)}\")\n",
    "    print(f\"  Global max stress: {max(s['stress_max'] for s in chunk_statistics):.2f} Pa\")\n",
    "    print()\n",
    "\n",
    "# example_owchirp_processing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 8: Validation Disabled for Speed\n",
    "\n",
    "Disable validation checks for maximum reading speed when you trust the data quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T02:54:59.986388Z",
     "iopub.status.busy": "2026-02-14T02:54:59.986143Z",
     "iopub.status.idle": "2026-02-14T02:54:59.993538Z",
     "shell.execute_reply": "2026-02-14T02:54:59.992256Z"
    }
   },
   "outputs": [],
   "source": [
    "def example_validation_disabled():\n",
    "    \"\"\"Disable validation for maximum reading speed.\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Example 8: Validation Disabled\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    print(\"Reading with validation disabled (faster, but less safe):\")\n",
    "\n",
    "    chunk_count = 0\n",
    "    for chunk in load_trios_chunked(\n",
    "        'large_file.txt',\n",
    "        chunk_size=10000,\n",
    "        validate_data=False  # Skip validation for speed\n",
    "    ):\n",
    "        chunk_count += 1\n",
    "\n",
    "    print(f\"  Processed {chunk_count} chunks\")\n",
    "    print(\"\\nNote: Validation checks for NaN, non-finite values, and monotonicity.\")\n",
    "    print(\"      Disable only if you trust the data quality.\")\n",
    "    print()\n",
    "\n",
    "# example_validation_disabled()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 9: Memory-Efficient Data Export\n",
    "\n",
    "Export processed data in chunks to avoid memory issues with large files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T02:54:59.996541Z",
     "iopub.status.busy": "2026-02-14T02:54:59.996194Z",
     "iopub.status.idle": "2026-02-14T02:55:00.002202Z",
     "shell.execute_reply": "2026-02-14T02:54:59.999532Z"
    }
   },
   "outputs": [],
   "source": [
    "def example_chunked_export():\n",
    "    \"\"\"Export data in chunks to avoid memory issues.\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Example 9: Chunked Data Export\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    output_file = Path('processed_data.csv')\n",
    "\n",
    "    print(f\"Exporting processed data to: {output_file}\")\n",
    "\n",
    "    # Write header\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write(\"time,stress,processing_flag\\n\")\n",
    "\n",
    "    # Process and write chunks\n",
    "    chunk_count = 0\n",
    "    for chunk in load_trios_chunked('large_file.txt', chunk_size=10000):\n",
    "        # Process chunk (e.g., apply threshold)\n",
    "        threshold = 1000.0\n",
    "        flags = chunk.y > threshold\n",
    "\n",
    "        # Append to file\n",
    "        with open(output_file, 'a') as f:\n",
    "            for t, s, flag in zip(chunk.x, chunk.y, flags):\n",
    "                f.write(f\"{t:.6f},{s:.2f},{int(flag)}\\n\")\n",
    "\n",
    "        chunk_count += 1\n",
    "\n",
    "    print(f\"✓ Exported {chunk_count} chunks\")\n",
    "    print(f\"  Output file: {output_file}\")\n",
    "    print()\n",
    "\n",
    "# example_chunked_export()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 10: Choosing Optimal Chunk Size\n",
    "\n",
    "Guide for selecting optimal chunk size based on file size and memory constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T02:55:00.008764Z",
     "iopub.status.busy": "2026-02-14T02:55:00.008640Z",
     "iopub.status.idle": "2026-02-14T02:55:00.012496Z",
     "shell.execute_reply": "2026-02-14T02:55:00.011557Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Example 10: Choosing Optimal Chunk Size\n",
      "======================================================================\n",
      "Chunk Size Guidelines:\n",
      "\n",
      "Small chunks (1,000 - 5,000 points):\n",
      "  Memory: ~80-400 KB\n",
      "  Use case: Very limited memory, point-by-point processing\n",
      "  Trade-off: More overhead, slower overall\n",
      "\n",
      "Medium chunks (5,000 - 20,000 points): [RECOMMENDED]\n",
      "  Memory: ~400 KB - 1.6 MB\n",
      "  Use case: Most applications, good balance\n",
      "  Trade-off: Optimal for typical workflows\n",
      "\n",
      "Large chunks (20,000 - 50,000 points):\n",
      "  Memory: ~1.6 - 4 MB\n",
      "  Use case: Ample memory, fewer iterations needed\n",
      "  Trade-off: Higher memory, faster processing\n",
      "\n",
      "For your file (70 MB, ~150000 points):\n",
      "  Recommended chunk size: 10000\n",
      "  Number of chunks: ~15\n",
      "  Memory per chunk: ~0.8 MB\n",
      "  Total memory saved: ~69.2 MB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def example_chunk_size_selection():\n",
    "    \"\"\"Guide for selecting optimal chunk size.\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Example 10: Choosing Optimal Chunk Size\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    file_size_mb = 70  # Example: 70 MB file\n",
    "    estimated_points = 150000  # Example: 150k points\n",
    "\n",
    "    print(\"Chunk Size Guidelines:\")\n",
    "    print()\n",
    "\n",
    "    # Small chunks (1,000 - 5,000)\n",
    "    print(\"Small chunks (1,000 - 5,000 points):\")\n",
    "    print(\"  Memory: ~80-400 KB\")\n",
    "    print(\"  Use case: Very limited memory, point-by-point processing\")\n",
    "    print(\"  Trade-off: More overhead, slower overall\")\n",
    "    print()\n",
    "\n",
    "    # Medium chunks (5,000 - 20,000) - RECOMMENDED\n",
    "    print(\"Medium chunks (5,000 - 20,000 points): [RECOMMENDED]\")\n",
    "    print(\"  Memory: ~400 KB - 1.6 MB\")\n",
    "    print(\"  Use case: Most applications, good balance\")\n",
    "    print(\"  Trade-off: Optimal for typical workflows\")\n",
    "    print()\n",
    "\n",
    "    # Large chunks (20,000 - 50,000)\n",
    "    print(\"Large chunks (20,000 - 50,000 points):\")\n",
    "    print(\"  Memory: ~1.6 - 4 MB\")\n",
    "    print(\"  Use case: Ample memory, fewer iterations needed\")\n",
    "    print(\"  Trade-off: Higher memory, faster processing\")\n",
    "    print()\n",
    "\n",
    "    # Recommendation for this file\n",
    "    recommended_chunk_size = 10000\n",
    "    estimated_chunks = (estimated_points + recommended_chunk_size - 1) // recommended_chunk_size\n",
    "    memory_per_chunk_mb = recommended_chunk_size * 80 / 1024 / 1024\n",
    "\n",
    "    print(f\"For your file ({file_size_mb} MB, ~{estimated_points} points):\")\n",
    "    print(f\"  Recommended chunk size: {recommended_chunk_size}\")\n",
    "    print(f\"  Number of chunks: ~{estimated_chunks}\")\n",
    "    print(f\"  Memory per chunk: ~{memory_per_chunk_mb:.1f} MB\")\n",
    "    print(f\"  Total memory saved: ~{file_size_mb - memory_per_chunk_mb:.1f} MB\")\n",
    "    print()\n",
    "\n",
    "# Run this example (doesn't require files)\n",
    "example_chunk_size_selection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated 10 examples of memory-efficient chunked reading:\n",
    "\n",
    "1. **Basic chunked reading** - Iterate through data chunks\n",
    "2. **Aggregating statistics** - Compute global stats without full load\n",
    "3. **Model fitting on chunks** - Fit models to individual chunks\n",
    "4. **Segment selection** - Process specific segments from multi-step files\n",
    "5. **Method comparison** - Chunked vs full loading verification\n",
    "6. **Backward compatibility** - Using `load_trios` with `chunk_size`\n",
    "7. **OWChirp processing** - Real-world large file example\n",
    "8. **Validation disabled** - Speed optimization for trusted data\n",
    "9. **Chunked export** - Memory-efficient data export\n",
    "10. **Chunk size selection** - Guidelines for optimal performance\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- Use **10,000 chunk size** for most applications (good balance)\n",
    "- Chunked reading provides **~93% memory savings** for large files\n",
    "- Each chunk is a complete `RheoData` object with full metadata\n",
    "- File handles close automatically after iteration\n",
    "\n",
    "### For More Information\n",
    "\n",
    "- Documentation: `rheojax.io.readers.trios.load_trios_chunked`\n",
    "- Tests: `tests/io/test_trios_chunked.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "### Handbook Documentation\n",
    "\n",
    "- **[TRIOS Reader](../../docs/source/api/io.rst)**: `rheojax.io.trios` module with auto-chunking, progress callbacks, memory management\n",
    "- **[RheoData API](../../docs/source/api/core.rst)**: Construction, slicing, conversion methods for efficient data handling\n",
    "- **[Other I/O Formats](../../docs/source/api/io.rst)**: CSV, Excel, Anton Paar readers with streaming support\n",
    "\n",
    "### Technical Documentation\n",
    "\n",
    "- **[TA Instruments TRIOS Format](https://www.tainstruments.com/support/software-downloads-support/)**: Official TRIOS data format specification\n",
    "- **[Pandas Chunked Reading](https://pandas.pydata.org/docs/user_guide/io.html#iterating-through-files-chunk-by-chunk)**: Underlying chunk iteration patterns\n",
    "- **[Memory Profiling](https://pypi.org/project/memory-profiler/)**: Tools for measuring memory usage in Python\n",
    "\n",
    "### Related RheoJAX Examples\n",
    "\n",
    "- **[02-batch-processing.ipynb](02-batch-processing.ipynb)**: Combining chunked reading with batch pipelines\n",
    "- **[../basic/04-data-io.ipynb](../basic/04-data-io.ipynb)**: Overview of all RheoJAX I/O capabilities\n",
    "- **[05-performance-optimization.ipynb](05-performance-optimization.ipynb)**: Memory management and deferred NumPy conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Analysis\n",
    "\n",
    "Chunked processing enables memory-efficient residual computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convergence Diagnostic Interpretation\n",
    "\n",
    "| Metric | Target | Meaning |\n",
    "|--------|--------|---------|\n",
    "| **R-hat < 1.01** | Chains converged | Multiple chains agree on posterior |\n",
    "| **ESS > 400** | Sufficient samples | Independent information content |\n",
    "| **Divergences < 1%** | Well-behaved sampler | No numerical issues in posterior geometry |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- **[02-batch-processing.ipynb](02-batch-processing.ipynb)**: Combine chunked reading with batch pipelines\n",
    "- **[05-performance-optimization.ipynb](05-performance-optimization.ipynb)**: Memory profiling strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key References\n",
    "\n",
    "1. **TA Instruments TRIOS Format**: Official data format specification\n",
    "2. **Pandas Chunked Reading**: Chunk iteration documentation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rheojax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
