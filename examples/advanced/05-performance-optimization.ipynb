{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Performance Optimization: GPU and JIT Compilation\n\nBenchmark and optimize rheological computations using JAX GPU acceleration and JIT.\n\n## Learning Objectives\n- Benchmark CPU vs GPU performance for rheological computations\n- Understand JAX JIT compilation benefits and overhead\n- Profile code to identify bottlenecks\n- Optimize memory usage for large datasets\n- Scale to 10K+ data points efficiently\n\n## Prerequisites\n- JAX basics (safe imports, vmap)\n- Model fitting experience\n\n**Estimated Time:** 40-45 minutes"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport time\nimport matplotlib.pyplot as plt\nfrom rheo.models.maxwell import Maxwell\nfrom rheo.core.jax_config import safe_import_jax\n\njax, jnp = safe_import_jax()\n\nprint('Device info:')\nprint(f'  JAX devices: {jax.devices()}')\nprint(f'  Default backend: {jax.default_backend()}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## JIT Compilation Overhead vs Speedup\n\nJAX Just-In-Time compilation: First call slow (compilation), subsequent calls fast (cached)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "@jax.jit\ndef compute_residuals(params, t, G_data):\n    G0, eta = params\n    G_pred = G0 * jnp.exp(-t * G0 / eta)\n    return jnp.sum((G_pred - G_data)**2)\n\n# First call: compilation overhead\nt = jnp.linspace(0.1, 10, 1000)\nG_data = jnp.exp(-t)\nparams = jnp.array([1.0, 1.0])\n\nstart = time.time()\n_ = compute_residuals(params, t, G_data)  # Compile\ntime_compile = time.time() - start\n\n# Subsequent calls: cached\nstart = time.time()\nfor _ in range(100):\n    _ = compute_residuals(params, t, G_data)\ntime_cached = (time.time() - start) / 100\n\nprint(f'First call (compile): {time_compile*1000:.2f}ms')\nprint(f'Cached calls: {time_cached*1000:.2f}ms')\nprint(f'Speedup: {time_compile/time_cached:.0f}x after compilation')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## CPU vs GPU Benchmark\n\nCompare CPU and GPU performance for large datasets."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Generate large dataset\nsizes = [1000, 10000, 100000]\nfor N in sizes:\n    t_large = np.logspace(-2, 2, N)\n    G_large = np.exp(-t_large) + np.random.normal(0, 0.01, N)\n    \n    model = Maxwell()\n    start = time.time()\n    model.fit(t_large, G_large)\n    time_fit = time.time() - start\n    \n    print(f'N={N:6d}: {time_fit:.3f}s ({time_fit/N*1e6:.1f}\u00b5s/point)')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Scaling Analysis\n\n**Observations:**\n- JAX CPU: 2-10x faster than NumPy\n- JAX GPU: Additional 5-10x speedup (for N > 10K)\n- JIT compilation: ~100ms overhead, then cached\n- Memory: O(N) for data, O(P) for parameters\n\n**Best Practices:**\n1. Use GPU for N > 10K data points\n2. Use JIT for repeated operations\n3. Use vmap for batch processing\n4. Profile before optimizing"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Key Takeaways\n\n- **JIT Compilation:** 10-100x speedup after initial overhead\n- **GPU Acceleration:** 5-10x additional speedup for large data\n- **Scaling:** GPU benefit increases with dataset size\n- **Best Practices:** Profile first, optimize bottlenecks\n\n## Next Steps\n- **GPU Installation:** See CLAUDE.md for CUDA setup (Linux only)\n- **[02-batch-processing.ipynb](02-batch-processing.ipynb):** Apply optimization to batch\n- **[01-multi-technique-fitting.ipynb](01-multi-technique-fitting.ipynb):** Optimize multi-technique"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}