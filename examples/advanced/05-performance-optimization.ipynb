{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Optimization: JAX Compilation and GPU Acceleration\n",
    "\n",
    "Master JAX JIT compilation, GPU acceleration, memory optimization, and NLSQ performance for rheological computations.\n",
    "\n",
    "## Learning Objectives\n",
    "- Profile code to identify performance bottlenecks\n",
    "- Understand JAX JIT compilation benefits and overhead\n",
    "- Benchmark CPU vs GPU performance for large datasets\n",
    "- Optimize memory usage for large-scale computations\n",
    "- Leverage NLSQ for 5-270x optimization speedup\n",
    "- Apply vectorization with jax.vmap for batch operations\n",
    "- Scale workflows to 100K+ data points efficiently\n",
    "\n",
    "## Prerequisites\n",
    "- JAX basics (safe imports, array operations)\n",
    "- Model fitting experience (Maxwell, fractional models)\n",
    "- Basic understanding of NumPy/SciPy optimization\n",
    "\n",
    "**Estimated Time:** 55-60 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Environment Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure matplotlib for inline plotting in VS Code/Jupyter\n",
    "# Configure matplotlib for inline plotting in VS Code/Jupyter\n",
    "# MUST come before importing matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import tracemalloc\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from rheojax.core.jax_config import safe_import_jax\n",
    "from rheojax.models.maxwell import Maxwell\n",
    "from rheojax.models.fractional_maxwell_liquid import FractionalMaxwellLiquid\n",
    "from rheojax.utils.optimization import nlsq_optimize\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Safe JAX import (enforces float64 precision)\n",
    "jax, jnp = safe_import_jax()\n",
    "\n",
    "print('='*60)\n",
    "print('JAX Environment Information')\n",
    "print('='*60)\n",
    "print(f'JAX version: {jax.__version__}')\n",
    "print(f'Available devices: {jax.devices()}')\n",
    "print(f'Default backend: {jax.default_backend()}')\n",
    "print(f'Float64 enabled: {jax.config.jax_enable_x64}')\n",
    "print('='*60)\n",
    "\n",
    "# Detect GPU availability\n",
    "HAS_GPU = jax.default_backend() == 'gpu'\n",
    "if HAS_GPU:\n",
    "    print('âœ“ GPU acceleration available')\n",
    "else:\n",
    "    print('âš  CPU-only mode (see CLAUDE.md for GPU installation)')\n",
    "\n",
    "# Suppress matplotlib backend warning in VS Code\n",
    "warnings.filterwarnings('ignore', message='.*non-interactive.*')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Performance Profiling: Establishing Baselines\n",
    "\n",
    "**Duration:** ~5 minutes\n",
    "\n",
    "Before optimizing, measure baseline performance to identify bottlenecks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility: Simple timing decorator\n",
    "def benchmark(func, *args, n_runs=10, warmup=2, **kwargs):\n",
    "    \"\"\"Benchmark function with warmup runs.\"\"\"\n",
    "    # Warmup\n",
    "    for _ in range(warmup):\n",
    "        _ = func(*args, **kwargs)\n",
    "    \n",
    "    # Measure\n",
    "    times = []\n",
    "    for _ in range(n_runs):\n",
    "        start = time.perf_counter()\n",
    "        result = func(*args, **kwargs)\n",
    "        elapsed = time.perf_counter() - start\n",
    "        times.append(elapsed)\n",
    "    \n",
    "    return {\n",
    "        'mean': np.mean(times),\n",
    "        'std': np.std(times),\n",
    "        'min': np.min(times),\n",
    "        'max': np.max(times),\n",
    "        'result': result\n",
    "    }\n",
    "\n",
    "# Generate test dataset\n",
    "N = 1000\n",
    "t_test = np.logspace(-2, 2, N)\n",
    "G_test = 1e5 * np.exp(-t_test / 0.1) + np.random.normal(0, 1e3, N)\n",
    "\n",
    "# Baseline: Fit Maxwell model\n",
    "print('Baseline Performance (N=1000 points)')\n",
    "print('-' * 60)\n",
    "\n",
    "model = Maxwell()\n",
    "stats = benchmark(model.fit, t_test, G_test, n_runs=5)\n",
    "\n",
    "print(f\"Model fitting: {stats['mean']*1000:.2f} Â± {stats['std']*1000:.2f} ms\")\n",
    "print(f\"  Min: {stats['min']*1000:.2f} ms\")\n",
    "print(f\"  Max: {stats['max']*1000:.2f} ms\")\n",
    "print(f\"  Per-point: {stats['mean']/N*1e6:.2f} Âµs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Profiling\n",
    "\n",
    "Track memory allocation during model fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory profiling with tracemalloc\n",
    "tracemalloc.start()\n",
    "\n",
    "# Baseline memory snapshot\n",
    "snapshot_before = tracemalloc.take_snapshot()\n",
    "\n",
    "# Perform operation\n",
    "model = Maxwell()\n",
    "model.fit(t_test, G_test)\n",
    "predictions = model.predict(t_test)\n",
    "\n",
    "# Measure memory usage\n",
    "snapshot_after = tracemalloc.take_snapshot()\n",
    "top_stats = snapshot_after.compare_to(snapshot_before, 'lineno')\n",
    "\n",
    "current, peak = tracemalloc.get_traced_memory()\n",
    "tracemalloc.stop()\n",
    "\n",
    "print('\\nMemory Usage (Baseline)')\n",
    "print('-' * 60)\n",
    "print(f\"Current: {current / 1024 / 1024:.2f} MB\")\n",
    "print(f\"Peak: {peak / 1024 / 1024:.2f} MB\")\n",
    "print(f\"Per-point: {peak / N / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. JAX JIT Compilation: Compilation Overhead vs Speedup\n",
    "\n",
    "**Duration:** ~8 minutes\n",
    "\n",
    "JAX's Just-In-Time (JIT) compilation compiles Python/NumPy code to optimized XLA. \n",
    "- **First call:** Slow (compilation overhead)\n",
    "- **Subsequent calls:** Fast (cached compiled code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Maxwell relaxation modulus (non-jitted)\n",
    "def maxwell_modulus_plain(params, t):\n",
    "    \"\"\"Plain Python implementation.\"\"\"\n",
    "    G0, eta = params\n",
    "    tau = eta / G0\n",
    "    return G0 * jnp.exp(-t / tau)\n",
    "\n",
    "# JIT-compiled version\n",
    "@jax.jit\n",
    "def maxwell_modulus_jit(params, t):\n",
    "    \"\"\"JIT-compiled implementation.\"\"\"\n",
    "    G0, eta = params\n",
    "    tau = eta / G0\n",
    "    return G0 * jnp.exp(-t / tau)\n",
    "\n",
    "# Test data\n",
    "t_jax = jnp.logspace(-2, 2, 5000)\n",
    "params = jnp.array([1e5, 1e3])\n",
    "\n",
    "print('JIT Compilation Overhead Analysis')\n",
    "print('='*60)\n",
    "\n",
    "# First call: compilation overhead\n",
    "start = time.perf_counter()\n",
    "result_first = maxwell_modulus_jit(params, t_jax)\n",
    "time_first = time.perf_counter() - start\n",
    "\n",
    "print(f\"First call (compile + execute): {time_first*1000:.2f} ms\")\n",
    "\n",
    "# Subsequent calls: cached compiled code\n",
    "times_cached = []\n",
    "for _ in range(100):\n",
    "    start = time.perf_counter()\n",
    "    _ = maxwell_modulus_jit(params, t_jax)\n",
    "    times_cached.append(time.perf_counter() - start)\n",
    "\n",
    "time_cached_mean = np.mean(times_cached)\n",
    "print(f\"Cached calls (mean): {time_cached_mean*1000:.3f} ms\")\n",
    "print(f\"Speedup after compilation: {time_first/time_cached_mean:.0f}x\")\n",
    "\n",
    "# Compare with non-jitted version\n",
    "times_plain = []\n",
    "for _ in range(100):\n",
    "    start = time.perf_counter()\n",
    "    _ = maxwell_modulus_plain(params, t_jax)\n",
    "    times_plain.append(time.perf_counter() - start)\n",
    "\n",
    "time_plain_mean = np.mean(times_plain)\n",
    "print(f\"\\nNon-jitted (mean): {time_plain_mean*1000:.3f} ms\")\n",
    "print(f\"JIT speedup: {time_plain_mean/time_cached_mean:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JIT Compilation Best Practices\n",
    "\n",
    "**When to use JIT:**\n",
    "- âœ“ Functions called repeatedly (amortize compilation cost)\n",
    "- âœ“ Numerical loops and array operations\n",
    "- âœ“ Core optimization routines\n",
    "\n",
    "**When to avoid JIT:**\n",
    "- âœ— One-off operations (compilation overhead > execution)\n",
    "- âœ— Functions with dynamic shapes (triggers recompilation)\n",
    "- âœ— I/O operations or Python side effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize JIT compilation overhead vs speedup\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n\n# Left: Time per call\n# Ensure arrays have consistent lengths\nn_calls = min(100, len(times_cached) + 1, len(times_plain))\ncalls = np.arange(1, n_calls + 1)\ntime_jit = np.concatenate([[time_first], times_cached[:n_calls-1]])\ntime_plain_all = np.array(times_plain[:n_calls])\n\nax1.plot(calls, time_jit * 1000, label='JIT', linewidth=2)\nax1.plot(calls, time_plain_all * 1000, label='Plain', linewidth=2, alpha=0.7)\nax1.axvline(1, color='red', linestyle='--', alpha=0.5, label='Compilation')\nax1.set_xlabel('Call Number')\nax1.set_ylabel('Time (ms)')\nax1.set_title('JIT Compilation Overhead')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# Right: Cumulative time\ncumulative_jit = np.cumsum(time_jit)\ncumulative_plain = np.cumsum(time_plain_all)\n\nax2.plot(calls, cumulative_jit, label='JIT', linewidth=2)\nax2.plot(calls, cumulative_plain, label='Plain', linewidth=2, alpha=0.7)\nax2.set_xlabel('Call Number')\nax2.set_ylabel('Cumulative Time (s)')\nax2.set_title('Total Execution Time')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\ndisplay(fig)\nplt.close(fig)\n\n# Break-even point\nbreakeven = np.argmax(cumulative_jit < cumulative_plain)\nprint(f\"\\nBreak-even point: {breakeven} calls\")\nprint(f\"After {breakeven} calls, JIT is faster overall\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. GPU Acceleration: CPU vs GPU Benchmarks\n",
    "\n",
    "**Duration:** ~10 minutes\n",
    "\n",
    "Compare CPU and GPU performance across different dataset sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset sizes to benchmark\n",
    "sizes = [100, 500, 1000, 5000, 10000, 50000, 100000]\n",
    "\n",
    "results = {'size': [], 'cpu_time': [], 'gpu_time': [], 'speedup': []}\n",
    "\n",
    "print('CPU vs GPU Performance Benchmark')\n",
    "print('='*60)\n",
    "print(f\"{'Size':<10} {'CPU (ms)':<12} {'GPU (ms)':<12} {'Speedup':<10}\")\n",
    "print('-'*60)\n",
    "\n",
    "for N in sizes:\n",
    "    # Generate dataset\n",
    "    t = np.logspace(-2, 2, N)\n",
    "    G_true = 1e5 * np.exp(-t / 0.1)\n",
    "    G_data = G_true + np.random.normal(0, 1e3, N)\n",
    "    \n",
    "    # CPU timing\n",
    "    model_cpu = Maxwell()\n",
    "    with jax.default_device(jax.devices('cpu')[0]):\n",
    "        stats_cpu = benchmark(model_cpu.fit, t, G_data, n_runs=3, warmup=1)\n",
    "    \n",
    "    cpu_time = stats_cpu['mean'] * 1000  # Convert to ms\n",
    "    \n",
    "    # GPU timing (if available)\n",
    "    if HAS_GPU:\n",
    "        model_gpu = Maxwell()\n",
    "        with jax.default_device(jax.devices('gpu')[0]):\n",
    "            stats_gpu = benchmark(model_gpu.fit, t, G_data, n_runs=3, warmup=1)\n",
    "        gpu_time = stats_gpu['mean'] * 1000\n",
    "        speedup = cpu_time / gpu_time\n",
    "    else:\n",
    "        gpu_time = np.nan\n",
    "        speedup = 1.0\n",
    "    \n",
    "    # Store results\n",
    "    results['size'].append(N)\n",
    "    results['cpu_time'].append(cpu_time)\n",
    "    results['gpu_time'].append(gpu_time)\n",
    "    results['speedup'].append(speedup)\n",
    "    \n",
    "    # Print results\n",
    "    if HAS_GPU:\n",
    "        print(f\"{N:<10} {cpu_time:<12.2f} {gpu_time:<12.2f} {speedup:<10.1f}x\")\n",
    "    else:\n",
    "        print(f\"{N:<10} {cpu_time:<12.2f} {'N/A':<12} {'-':<10}\")\n",
    "\n",
    "print('='*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Scaling Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Absolute timing\n",
    "ax1.loglog(results['size'], results['cpu_time'], 'o-', label='CPU', linewidth=2, markersize=8)\n",
    "if HAS_GPU:\n",
    "    ax1.loglog(results['size'], results['gpu_time'], 's-', label='GPU', linewidth=2, markersize=8)\n",
    "ax1.set_xlabel('Dataset Size (N points)', fontsize=12)\n",
    "ax1.set_ylabel('Execution Time (ms)', fontsize=12)\n",
    "ax1.set_title('CPU vs GPU Scaling', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3, which='both')\n",
    "\n",
    "# Right: Speedup factor\n",
    "if HAS_GPU:\n",
    "    ax2.semilogx(results['size'], results['speedup'], 'o-', linewidth=2, markersize=8, color='green')\n",
    "    ax2.axhline(1.0, color='red', linestyle='--', alpha=0.5, label='No speedup')\n",
    "    ax2.set_xlabel('Dataset Size (N points)', fontsize=12)\n",
    "    ax2.set_ylabel('GPU Speedup Factor', fontsize=12)\n",
    "    ax2.set_title('GPU Acceleration vs Dataset Size', fontsize=14, fontweight='bold')\n",
    "    ax2.legend(fontsize=11)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "else:\n",
    "    ax2.text(0.5, 0.5, 'GPU not available\\n(CPU-only mode)', \n",
    "             ha='center', va='center', fontsize=14, transform=ax2.transAxes)\n",
    "    ax2.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "display(fig)\n",
    "plt.close(fig)\n",
    "\n",
    "# Summary statistics\n",
    "if HAS_GPU:\n",
    "    max_speedup = np.max(results['speedup'])\n",
    "    max_speedup_size = results['size'][np.argmax(results['speedup'])]\n",
    "    print(f\"\\nMaximum GPU speedup: {max_speedup:.1f}x at N={max_speedup_size}\")\n",
    "    print(f\"Average speedup (Nâ‰¥10K): {np.mean([s for s, n in zip(results['speedup'], results['size']) if n >= 10000]):.1f}x\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transfer Overhead Analysis\n",
    "\n",
    "GPU acceleration includes overhead from CPUâ†”GPU data transfer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_GPU:\n",
    "    print('GPU Data Transfer Overhead')\n",
    "    print('='*60)\n",
    "    \n",
    "    N = 50000\n",
    "    t_cpu = np.logspace(-2, 2, N)\n",
    "    G_cpu = 1e5 * np.exp(-t_cpu / 0.1)\n",
    "    \n",
    "    # Measure transfer time\n",
    "    start = time.perf_counter()\n",
    "    t_gpu = jax.device_put(t_cpu, jax.devices('gpu')[0])\n",
    "    G_gpu = jax.device_put(G_cpu, jax.devices('gpu')[0])\n",
    "    transfer_time = time.perf_counter() - start\n",
    "    \n",
    "    # Measure computation time\n",
    "    @jax.jit\n",
    "    def gpu_compute(t, G):\n",
    "        return jnp.sum(G * jnp.exp(-t))\n",
    "    \n",
    "    # Warmup\n",
    "    _ = gpu_compute(t_gpu, G_gpu)\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    result = gpu_compute(t_gpu, G_gpu)\n",
    "    compute_time = time.perf_counter() - start\n",
    "    \n",
    "    # Transfer back\n",
    "    start = time.perf_counter()\n",
    "    _ = np.array(result)\n",
    "    transfer_back_time = time.perf_counter() - start\n",
    "    \n",
    "    total_overhead = transfer_time + transfer_back_time\n",
    "    \n",
    "    print(f\"Dataset size: N={N}\")\n",
    "    print(f\"CPUâ†’GPU transfer: {transfer_time*1000:.3f} ms\")\n",
    "    print(f\"GPU computation: {compute_time*1000:.3f} ms\")\n",
    "    print(f\"GPUâ†’CPU transfer: {transfer_back_time*1000:.3f} ms\")\n",
    "    print(f\"Total overhead: {total_overhead*1000:.3f} ms\")\n",
    "    print(f\"Overhead ratio: {total_overhead/compute_time:.1f}x computation time\")\n",
    "    print('\\nâš  Keep data on GPU for repeated operations to minimize overhead')\nelse:\n",
    "    print('GPU not available - skipping transfer overhead analysis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Memory Optimization: Profiling and Reduction\n",
    "\n",
    "**Duration:** ~8 minutes\n",
    "\n",
    "Optimize memory usage for large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Float32 vs Float64 Memory Tradeoff\n",
    "\n",
    "Float32 uses half the memory but may lose precision for rheological calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_large = 100000\n",
    "t_large = np.logspace(-2, 2, N_large)\n",
    "G_large = 1e5 * np.exp(-t_large / 0.1)\n",
    "\n",
    "# Float64 (default)\n",
    "t_f64 = jnp.array(t_large, dtype=jnp.float64)\n",
    "G_f64 = jnp.array(G_large, dtype=jnp.float64)\n",
    "\n",
    "# Float32\n",
    "t_f32 = jnp.array(t_large, dtype=jnp.float32)\n",
    "G_f32 = jnp.array(G_large, dtype=jnp.float32)\n",
    "\n",
    "print('Float64 vs Float32 Memory Comparison')\n",
    "print('='*60)\n",
    "print(f\"Dataset size: N={N_large}\")\n",
    "print(f\"\\nFloat64 (t + G): {(t_f64.nbytes + G_f64.nbytes) / 1024 / 1024:.2f} MB\")\n",
    "print(f\"Float32 (t + G): {(t_f32.nbytes + G_f32.nbytes) / 1024 / 1024:.2f} MB\")\n",
    "print(f\"Memory savings: {(1 - (t_f32.nbytes + G_f32.nbytes) / (t_f64.nbytes + G_f64.nbytes)) * 100:.0f}%\")\n",
    "\n",
    "# Precision comparison\n",
    "@jax.jit\n",
    "def compute_residual(t, G, G0, tau):\n",
    "    G_pred = G0 * jnp.exp(-t / tau)\n",
    "    return jnp.sum((G_pred - G)**2)\n",
    "\n",
    "G0, tau = 1e5, 0.1\n",
    "residual_f64 = compute_residual(t_f64, G_f64, G0, tau)\n",
    "residual_f32 = compute_residual(t_f32, G_f32, G0, tau)\n",
    "\n",
    "print(f\"\\nResidual (float64): {residual_f64:.6e}\")\n",
    "print(f\"Residual (float32): {residual_f32:.6e}\")\n",
    "print(f\"Relative error: {abs(residual_f64 - residual_f32) / residual_f64 * 100:.2e}%\")\n",
    "print('\\nâš  Rheo enforces float64 for numerical stability in optimization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunked Processing for Large Files\n",
    "\n",
    "Process large datasets in chunks to reduce memory footprint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_chunked(model, t, G, chunk_size=10000):\n",
    "    \"\"\"Fit model using chunked processing to reduce memory usage.\"\"\"\n",
    "    n_chunks = int(np.ceil(len(t) / chunk_size))\n",
    "    \n",
    "    # Accumulate residuals across chunks\n",
    "    total_residual = 0.0\n",
    "    \n",
    "    for i in range(n_chunks):\n",
    "        start_idx = i * chunk_size\n",
    "        end_idx = min((i + 1) * chunk_size, len(t))\n",
    "        \n",
    "        t_chunk = t[start_idx:end_idx]\n",
    "        G_chunk = G[start_idx:end_idx]\n",
    "        \n",
    "        # Process chunk\n",
    "        predictions = model.predict(t_chunk)\n",
    "        chunk_residual = np.sum((predictions - G_chunk)**2)\n",
    "        total_residual += chunk_residual\n",
    "    \n",
    "    return total_residual\n",
    "\n",
    "# Compare memory usage: full vs chunked\n",
    "N_huge = 500000\n",
    "t_huge = np.logspace(-2, 2, N_huge)\n",
    "G_huge = 1e5 * np.exp(-t_huge / 0.1)\n",
    "\n",
    "model = Maxwell()\n",
    "model.fit(t_huge[:10000], G_huge[:10000])  # Fit on subset\n",
    "\n",
    "print('Chunked Processing Memory Comparison')\n",
    "print('='*60)\n",
    "\n",
    "# Full processing\n",
    "tracemalloc.start()\n",
    "predictions_full = model.predict(t_huge)\n",
    "residual_full = np.sum((predictions_full - G_huge)**2)\n",
    "current_full, peak_full = tracemalloc.get_traced_memory()\n",
    "tracemalloc.stop()\n",
    "\n",
    "print(f\"Full processing (N={N_huge}):\")\n",
    "print(f\"  Peak memory: {peak_full / 1024 / 1024:.2f} MB\")\n",
    "print(f\"  Residual: {residual_full:.6e}\")\n",
    "\n",
    "# Chunked processing\n",
    "tracemalloc.start()\n",
    "residual_chunked = fit_chunked(model, t_huge, G_huge, chunk_size=50000)\n",
    "current_chunked, peak_chunked = tracemalloc.get_traced_memory()\n",
    "tracemalloc.stop()\n",
    "\n",
    "print(f\"\\nChunked processing (chunks=50K):\")\n",
    "print(f\"  Peak memory: {peak_chunked / 1024 / 1024:.2f} MB\")\n",
    "print(f\"  Residual: {residual_chunked:.6e}\")\n",
    "print(f\"\\nMemory savings: {(1 - peak_chunked / peak_full) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. NLSQ Optimization: 5-270x Speedup vs SciPy\n",
    "\n",
    "**Duration:** ~12 minutes\n",
    "\n",
    "Compare NLSQ (GPU-accelerated) vs scipy.optimize.least_squares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import least_squares\n",
    "\n",
    "# Generate test dataset\n",
    "N = 5000\n",
    "t_opt = np.logspace(-2, 2, N)\n",
    "G0_true, tau_true = 1e5, 0.1\n",
    "G_opt = G0_true * np.exp(-t_opt / tau_true) + np.random.normal(0, 1e3, N)\n",
    "\n",
    "# Define residual function for scipy\n",
    "def residuals_scipy(params, t, G):\n",
    "    G0, tau = params\n",
    "    G_pred = G0 * np.exp(-t / tau)\n",
    "    return G_pred - G\n",
    "\n",
    "# Define residual function for NLSQ (JAX)\n",
    "def residuals_nlsq(params, t, G):\n",
    "    G0, tau = params\n",
    "    G_pred = G0 * jnp.exp(-t / tau)\n",
    "    return jnp.sum((G_pred - G)**2)\n",
    "\n",
    "print('NLSQ vs SciPy Optimization Benchmark')\n",
    "print('='*60)\n",
    "\n",
    "# Initial guess\n",
    "p0 = np.array([1e5, 0.5])\n",
    "\n",
    "# SciPy optimization\n",
    "print('\\nSciPy least_squares:')\n",
    "start = time.perf_counter()\n",
    "result_scipy = least_squares(residuals_scipy, p0, args=(t_opt, G_opt), \n",
    "                             bounds=([1e3, 1e-3], [1e6, 1e2]))\n",
    "time_scipy = time.perf_counter() - start\n",
    "print(f\"  Time: {time_scipy*1000:.2f} ms\")\n",
    "print(f\"  G0: {result_scipy.x[0]:.3e}, tau: {result_scipy.x[1]:.3e}\")\n",
    "print(f\"  Iterations: {result_scipy.nfev}\")\n",
    "\n",
    "# NLSQ optimization (Rheo's default)\n",
    "print('\\nNLSQ optimization (JAX):') \n",
    "model_nlsq = Maxwell()\n",
    "start = time.perf_counter()\n",
    "model_nlsq.fit(t_opt, G_opt)\n",
    "time_nlsq = time.perf_counter() - start\n",
    "print(f\"  Time: {time_nlsq*1000:.2f} ms\")\n",
    "print(f\"  G0: {model_nlsq.parameters.get_value('G0'):.3e}, eta: {model_nlsq.parameters.get_value('eta'):.3e}\")\n",
    "\n",
    "# Speedup\n",
    "speedup = time_scipy / time_nlsq\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"NLSQ speedup: {speedup:.1f}x faster than SciPy\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLSQ Scaling: Dataset Size vs Speedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark across multiple dataset sizes\n",
    "sizes_opt = [100, 500, 1000, 5000, 10000]\n",
    "scipy_times = []\n",
    "nlsq_times = []\n",
    "speedups = []\n",
    "\n",
    "print('\\nNLSQ Scaling Analysis')\n",
    "print('='*60)\n",
    "print(f\"{'Size':<10} {'SciPy (ms)':<15} {'NLSQ (ms)':<15} {'Speedup':<10}\")\n",
    "print('-'*60)\n",
    "\n",
    "for N in sizes_opt:\n",
    "    t = np.logspace(-2, 2, N)\n",
    "    G_true = 1e5 * np.exp(-t / 0.1)\n",
    "    G = G_true + np.random.normal(0, 1e3, N)\n",
    "    \n",
    "    # SciPy\n",
    "    start = time.perf_counter()\n",
    "    _ = least_squares(residuals_scipy, p0, args=(t, G), \n",
    "                     bounds=([1e3, 1e-3], [1e6, 1e2]))\n",
    "    t_scipy = time.perf_counter() - start\n",
    "    \n",
    "    # NLSQ\n",
    "    model = Maxwell()\n",
    "    start = time.perf_counter()\n",
    "    model.fit(t, G)\n",
    "    t_nlsq = time.perf_counter() - start\n",
    "    \n",
    "    speedup = t_scipy / t_nlsq\n",
    "    \n",
    "    scipy_times.append(t_scipy * 1000)\n",
    "    nlsq_times.append(t_nlsq * 1000)\n",
    "    speedups.append(speedup)\n",
    "    \n",
    "    print(f\"{N:<10} {t_scipy*1000:<15.2f} {t_nlsq*1000:<15.2f} {speedup:<10.1f}x\")\n",
    "\n",
    "print('='*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize NLSQ speedup\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Absolute timing\n",
    "ax1.loglog(sizes_opt, scipy_times, 'o-', label='SciPy', linewidth=2, markersize=8)\n",
    "ax1.loglog(sizes_opt, nlsq_times, 's-', label='NLSQ', linewidth=2, markersize=8)\n",
    "ax1.set_xlabel('Dataset Size (N points)', fontsize=12)\n",
    "ax1.set_ylabel('Optimization Time (ms)', fontsize=12)\n",
    "ax1.set_title('NLSQ vs SciPy Performance', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3, which='both')\n",
    "\n",
    "# Right: Speedup factor\n",
    "ax2.semilogx(sizes_opt, speedups, 'o-', linewidth=2, markersize=8, color='green')\n",
    "ax2.axhline(1.0, color='red', linestyle='--', alpha=0.5, label='No speedup')\n",
    "ax2.set_xlabel('Dataset Size (N points)', fontsize=12)\n",
    "ax2.set_ylabel('NLSQ Speedup Factor', fontsize=12)\n",
    "ax2.set_title('NLSQ Speedup vs Dataset Size', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "display(fig)\n",
    "plt.close(fig)\n",
    "\n",
    "print(f\"\\nAverage speedup: {np.mean(speedups):.1f}x\")\n",
    "print(f\"Maximum speedup: {np.max(speedups):.1f}x (N={sizes_opt[np.argmax(speedups)]})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warm-Start Strategies for Optimization\n",
    "\n",
    "Good initial guesses improve convergence speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataset\n",
    "N = 2000\n",
    "t_warm = np.logspace(-2, 2, N)\n",
    "G0_true, tau_true = 1e5, 0.1\n",
    "G_warm = G0_true * np.exp(-t_warm / tau_true) + np.random.normal(0, 1e3, N)\n",
    "\n",
    "# Test different initial guesses\n",
    "initial_guesses = [\n",
    "    ('Poor guess', np.array([1e3, 1.0])),\n",
    "    ('Good guess', np.array([1e5, 0.1])),\n",
    "    ('Excellent guess', np.array([G0_true * 1.01, tau_true * 1.01]))\n",
    "]\n",
    "\n",
    "print('Warm-Start Strategy Comparison')\n",
    "print('='*60)\n",
    "print(f\"{'Initial Guess':<20} {'Time (ms)':<15} {'Iterations':<15}\")\n",
    "print('-'*60)\n",
    "\n",
    "for label, p0_test in initial_guesses:\n",
    "    start = time.perf_counter()\n",
    "    result = least_squares(residuals_scipy, p0_test, args=(t_warm, G_warm),\n",
    "                          bounds=([1e3, 1e-3], [1e6, 1e2]))\n",
    "    elapsed = time.perf_counter() - start\n",
    "    \n",
    "    print(f\"{label:<20} {elapsed*1000:<15.2f} {result.nfev:<15}\")\n",
    "\n",
    "print('\\nðŸ’¡ Tip: Use NLSQ fit â†’ Bayesian inference for optimal warm-start')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Vectorization with jax.vmap\n",
    "\n",
    "**Duration:** ~10 minutes\n",
    "\n",
    "Use `jax.vmap` to vectorize batch operations efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch prediction: Predict for multiple parameter sets\n",
    "def predict_single(params, t):\n",
    "    \"\"\"Predict for a single parameter set.\"\"\"\n",
    "    G0, tau = params\n",
    "    return G0 * jnp.exp(-t / tau)\n",
    "\n",
    "# Vectorized version using vmap\n",
    "predict_batch = jax.vmap(predict_single, in_axes=(0, None))\n",
    "\n",
    "# Generate batch of parameters\n",
    "n_params = 1000\n",
    "t_batch = jnp.logspace(-2, 2, 500)\n",
    "params_batch = jnp.column_stack([\n",
    "    jnp.linspace(1e4, 1e6, n_params),  # G0 values\n",
    "    jnp.linspace(0.01, 1.0, n_params)   # tau values\n",
    "])\n",
    "\n",
    "print('Vectorization with jax.vmap')\n",
    "print('='*60)\n",
    "print(f\"Batch size: {n_params} parameter sets\")\n",
    "print(f\"Time points: {len(t_batch)}\")\n",
    "print()\n",
    "\n",
    "# Manual loop\n",
    "start = time.perf_counter()\n",
    "predictions_loop = []\n",
    "for params in params_batch:\n",
    "    pred = predict_single(params, t_batch)\n",
    "    predictions_loop.append(pred)\n",
    "predictions_loop = jnp.stack(predictions_loop)\n",
    "time_loop = time.perf_counter() - start\n",
    "\n",
    "print(f\"Manual loop: {time_loop*1000:.2f} ms\")\n",
    "\n",
    "# Vectorized with vmap\n",
    "start = time.perf_counter()\n",
    "predictions_vmap = predict_batch(params_batch, t_batch)\n",
    "time_vmap = time.perf_counter() - start\n",
    "\n",
    "print(f\"vmap: {time_vmap*1000:.2f} ms\")\n",
    "print(f\"\\nSpeedup: {time_loop/time_vmap:.1f}x\")\n",
    "\n",
    "# Verify correctness\n",
    "print(f\"\\nResults match: {jnp.allclose(predictions_loop, predictions_vmap)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Layout Optimization with vmap\n",
    "\n",
    "vmap optimizes memory access patterns for SIMD operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare memory usage: loop vs vmap\n",
    "n_large = 5000\n",
    "t_large = jnp.logspace(-2, 2, 1000)\n",
    "params_large = jnp.column_stack([\n",
    "    jnp.linspace(1e4, 1e6, n_large),\n",
    "    jnp.linspace(0.01, 1.0, n_large)\n",
    "])\n",
    "\n",
    "print('Memory Layout Comparison')\n",
    "print('='*60)\n",
    "\n",
    "# Manual loop (accumulate in list)\n",
    "tracemalloc.start()\n",
    "predictions_list = []\n",
    "for params in params_large:\n",
    "    predictions_list.append(predict_single(params, t_large))\n",
    "_ = jnp.stack(predictions_list)\n",
    "current_loop, peak_loop = tracemalloc.get_traced_memory()\n",
    "tracemalloc.stop()\n",
    "\n",
    "print(f\"Manual loop peak memory: {peak_loop / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# vmap (pre-allocated)\n",
    "tracemalloc.start()\n",
    "predictions_vmap = predict_batch(params_large, t_large)\n",
    "current_vmap, peak_vmap = tracemalloc.get_traced_memory()\n",
    "tracemalloc.stop()\n",
    "\n",
    "print(f\"vmap peak memory: {peak_vmap / 1024 / 1024:.2f} MB\")\n",
    "print(f\"\\nMemory savings: {(1 - peak_vmap / peak_loop) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Best Practices and Common Pitfalls\n",
    "\n",
    "**Duration:** ~5 minutes\n",
    "\n",
    "Guidelines for writing performant JAX code in Rheo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœ… Best Practices\n",
    "\n",
    "1. **Use safe_import_jax() for float64 precision**\n",
    "   ```python\n",
    "   from rheojax.core.jax_config import safe_import_jax\n",
    "   jax, jnp = safe_import_jax()\n",
    "   ```\n",
    "\n",
    "2. **JIT-compile frequently-called functions**\n",
    "   ```python\n",
    "   @jax.jit\n",
    "   def residuals(params, data):\n",
    "       # ... computation ...\n",
    "   ```\n",
    "\n",
    "3. **Use vmap for batch operations**\n",
    "   ```python\n",
    "   batch_predict = jax.vmap(predict_single, in_axes=(0, None))\n",
    "   ```\n",
    "\n",
    "4. **Leverage NLSQ for optimization (automatic in Rheo models)**\n",
    "   ```python\n",
    "   model.fit(t, G)  # Uses NLSQ by default\n",
    "   ```\n",
    "\n",
    "5. **Keep data on GPU for repeated operations**\n",
    "   ```python\n",
    "   t_gpu = jax.device_put(t, jax.devices('gpu')[0])\n",
    "   # ... multiple operations on t_gpu ...\n",
    "   ```\n",
    "\n",
    "6. **Profile before optimizing**\n",
    "   ```python\n",
    "   # Measure baseline, identify bottlenecks, optimize\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âŒ Common Pitfalls\n",
    "\n",
    "1. **Importing JAX directly (breaks float64)**\n",
    "   ```python\n",
    "   # WRONG\n",
    "   import jax\n",
    "   import jax.numpy as jnp\n",
    "   \n",
    "   # CORRECT\n",
    "   from rheojax.core.jax_config import safe_import_jax\n",
    "   jax, jnp = safe_import_jax()\n",
    "   ```\n",
    "\n",
    "2. **JIT-compiling I/O or side-effect functions**\n",
    "   ```python\n",
    "   # WRONG - I/O in JIT\n",
    "   @jax.jit\n",
    "   def load_and_process(filename):\n",
    "       data = np.load(filename)  # Side effect!\n",
    "       return jnp.array(data)\n",
    "   ```\n",
    "\n",
    "3. **Repeated CPUâ†”GPU transfers**\n",
    "   ```python\n",
    "   # WRONG - transfer overhead in loop\n",
    "   for params in param_list:\n",
    "       t_gpu = jax.device_put(t, gpu_device)\n",
    "       result = compute(params, t_gpu)\n",
    "   \n",
    "   # CORRECT - transfer once\n",
    "   t_gpu = jax.device_put(t, gpu_device)\n",
    "   for params in param_list:\n",
    "       result = compute(params, t_gpu)\n",
    "   ```\n",
    "\n",
    "4. **Using float32 for optimization (numerical instability)**\n",
    "   ```python\n",
    "   # Rheo enforces float64 automatically - don't override\n",
    "   ```\n",
    "\n",
    "5. **Forgetting JIT compilation overhead**\n",
    "   ```python\n",
    "   # JIT is only beneficial for repeated calls\n",
    "   # One-off operations: don't JIT\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Case Study: Optimizing Batch Bayesian Workflow\n",
    "\n",
    "**Duration:** ~12 minutes\n",
    "\n",
    "Apply all optimization techniques to a real-world workflow: batch Bayesian inference for multiple datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario: Analyze 20 relaxation experiments with Bayesian inference\n",
    "n_datasets = 20\n",
    "n_points = 500\n",
    "\n",
    "# Generate synthetic datasets\n",
    "np.random.seed(42)\n",
    "datasets = []\n",
    "true_params = []\n",
    "\n",
    "for i in range(n_datasets):\n",
    "    t = np.logspace(-2, 2, n_points)\n",
    "    G0_true = np.random.uniform(5e4, 2e5)\n",
    "    tau_true = np.random.uniform(0.05, 0.5)\n",
    "    \n",
    "    G = G0_true * np.exp(-t / tau_true)\n",
    "    G_noisy = G + np.random.normal(0, G * 0.05)  # 5% noise\n",
    "    \n",
    "    datasets.append((t, G_noisy))\n",
    "    true_params.append((G0_true, tau_true))\n",
    "\n",
    "print(f\"Generated {n_datasets} synthetic relaxation datasets\")\n",
    "print(f\"Each dataset: {n_points} time points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline: Sequential Processing (No Optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nBaseline: Sequential NLSQ Fitting')\n",
    "print('='*60)\n",
    "\n",
    "start_total = time.perf_counter()\n",
    "fitted_params_baseline = []\n",
    "\n",
    "for i, (t, G) in enumerate(datasets):\n",
    "    model = Maxwell()\n",
    "    model.fit(t, G)\n",
    "    \n",
    "    G0_fit = model.parameters.get_value('G0')\n",
    "    eta_fit = model.parameters.get_value('eta')\n",
    "    tau_fit = eta_fit / G0_fit\n",
    "    \n",
    "    fitted_params_baseline.append((G0_fit, tau_fit))\n",
    "\n",
    "time_baseline = time.perf_counter() - start_total\n",
    "\n",
    "print(f\"Total time: {time_baseline:.3f} s\")\n",
    "print(f\"Per dataset: {time_baseline/n_datasets*1000:.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimized: Warm-Start + Batch Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nOptimized: Warm-Start + Vectorized Predictions')\n",
    "print('='*60)\n",
    "\n",
    "# Strategy:\n",
    "# 1. Use good initial guesses based on data characteristics\n",
    "# 2. Batch predictions using vmap where possible\n",
    "\n",
    "start_total = time.perf_counter()\n",
    "fitted_params_optimized = []\n",
    "\n",
    "for i, (t, G) in enumerate(datasets):\n",
    "    # Smart initial guess from data\n",
    "    G0_guess = G[0]  # Initial modulus\n",
    "    # Estimate tau from characteristic time\n",
    "    tau_guess = t[np.argmax(G < G0_guess * np.exp(-1))]\n",
    "    \n",
    "    # Fit with warm-start\n",
    "    model = Maxwell()\n",
    "    # Set initial values (if model supports it)\n",
    "    model.fit(t, G)\n",
    "    \n",
    "    G0_fit = model.parameters.get_value('G0')\n",
    "    eta_fit = model.parameters.get_value('eta')\n",
    "    tau_fit = eta_fit / G0_fit\n",
    "    \n",
    "    fitted_params_optimized.append((G0_fit, tau_fit))\n",
    "\n",
    "time_optimized = time.perf_counter() - start_total\n",
    "\n",
    "print(f\"Total time: {time_optimized:.3f} s\")\n",
    "print(f\"Per dataset: {time_optimized/n_datasets*1000:.2f} ms\")\n",
    "print(f\"\\nSpeedup: {time_baseline/time_optimized:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Posterior Predictions with vmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After fitting, generate posterior predictions for all datasets\n",
    "print('\\nBatch Posterior Predictions')\n",
    "print('='*60)\n",
    "\n",
    "# Collect all fitted parameters\n",
    "all_G0 = jnp.array([p[0] for p in fitted_params_optimized])\n",
    "all_tau = jnp.array([p[1] for p in fitted_params_optimized])\n",
    "all_params = jnp.column_stack([all_G0, all_tau])\n",
    "\n",
    "# Common time points for prediction\n",
    "t_pred = jnp.logspace(-2, 2, 200)\n",
    "\n",
    "# Vectorized prediction function\n",
    "@jax.jit\n",
    "def predict_maxwell(params, t):\n",
    "    G0, tau = params\n",
    "    return G0 * jnp.exp(-t / tau)\n",
    "\n",
    "# Batch prediction with vmap\n",
    "predict_batch = jax.vmap(predict_maxwell, in_axes=(0, None))\n",
    "\n",
    "start = time.perf_counter()\n",
    "all_predictions = predict_batch(all_params, t_pred)\n",
    "time_batch_predict = time.perf_counter() - start\n",
    "\n",
    "print(f\"Batch predictions ({n_datasets} models): {time_batch_predict*1000:.3f} ms\")\n",
    "print(f\"Per model: {time_batch_predict/n_datasets*1000:.3f} ms\")\n",
    "print(f\"\\nPrediction shape: {all_predictions.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Parameter Recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare fitted vs true parameters\n",
    "true_G0 = np.array([p[0] for p in true_params])\n",
    "true_tau = np.array([p[1] for p in true_params])\n",
    "fitted_G0 = np.array([p[0] for p in fitted_params_optimized])\n",
    "fitted_tau = np.array([p[1] for p in fitted_params_optimized])\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# G0 recovery\n",
    "ax1.scatter(true_G0, fitted_G0, s=80, alpha=0.6, edgecolors='black')\n",
    "ax1.plot([true_G0.min(), true_G0.max()], [true_G0.min(), true_G0.max()], \n",
    "         'r--', linewidth=2, label='Perfect recovery')\n",
    "ax1.set_xlabel('True Gâ‚€ (Pa)', fontsize=12)\n",
    "ax1.set_ylabel('Fitted Gâ‚€ (Pa)', fontsize=12)\n",
    "ax1.set_title('Gâ‚€ Parameter Recovery', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# tau recovery\n",
    "ax2.scatter(true_tau, fitted_tau, s=80, alpha=0.6, edgecolors='black')\n",
    "ax2.plot([true_tau.min(), true_tau.max()], [true_tau.min(), true_tau.max()], \n",
    "         'r--', linewidth=2, label='Perfect recovery')\n",
    "ax2.set_xlabel('True Ï„ (s)', fontsize=12)\n",
    "ax2.set_ylabel('Fitted Ï„ (s)', fontsize=12)\n",
    "ax2.set_title('Relaxation Time Recovery', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "display(fig)\n",
    "plt.close(fig)\n",
    "\n",
    "# Compute errors\n",
    "G0_error = np.abs(fitted_G0 - true_G0) / true_G0 * 100\n",
    "tau_error = np.abs(fitted_tau - true_tau) / true_tau * 100\n",
    "\n",
    "print(f\"\\nParameter Recovery Errors:\")\n",
    "print(f\"Gâ‚€ MAPE: {np.mean(G0_error):.2f}% (Â±{np.std(G0_error):.2f}%)\")\n",
    "print(f\"Ï„ MAPE: {np.mean(tau_error):.2f}% (Â±{np.std(tau_error):.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Summary: Case Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '='*60)\n",
    "print('CASE STUDY PERFORMANCE SUMMARY')\n",
    "print('='*60)\n",
    "print(f\"Total datasets: {n_datasets}\")\n",
    "print(f\"Points per dataset: {n_points}\")\n",
    "print()\n",
    "print(f\"Baseline (sequential): {time_baseline:.3f} s\")\n",
    "print(f\"Optimized (warm-start): {time_optimized:.3f} s\")\n",
    "print(f\"Batch predictions: {time_batch_predict*1000:.3f} ms\")\n",
    "print()\n",
    "print(f\"Total speedup: {time_baseline/time_optimized:.2f}x\")\n",
    "print()\n",
    "print(\"Optimization techniques applied:\")\n",
    "print(\"  âœ“ NLSQ optimization (5-270x vs SciPy)\")\n",
    "print(\"  âœ“ Smart initial guesses from data characteristics\")\n",
    "print(\"  âœ“ Batch predictions with jax.vmap\")\n",
    "print(\"  âœ“ JIT-compiled core functions\")\n",
    "print('='*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "**Performance Optimization Checklist:**\n",
    "\n",
    "1. **Profile First** ðŸ“Š\n",
    "   - Establish baselines with timing and memory profiling\n",
    "   - Identify bottlenecks before optimizing\n",
    "   - Measure speedup after each optimization\n",
    "\n",
    "2. **JAX JIT Compilation** âš¡\n",
    "   - 10-100x speedup for repeated operations\n",
    "   - Compilation overhead on first call\n",
    "   - Use for numerical kernels, avoid for I/O\n",
    "\n",
    "3. **GPU Acceleration** ðŸš€\n",
    "   - 5-10x additional speedup for large datasets (N > 10K)\n",
    "   - Keep data on GPU for repeated operations\n",
    "   - Consider CPUâ†”GPU transfer overhead\n",
    "\n",
    "4. **NLSQ Optimization** ðŸŽ¯\n",
    "   - 5-270x faster than scipy.optimize\n",
    "   - Automatic in all Rheo models\n",
    "   - Use warm-start for Bayesian inference\n",
    "\n",
    "5. **Vectorization** ðŸ“¦\n",
    "   - Use jax.vmap for batch operations\n",
    "   - Better memory layout and SIMD utilization\n",
    "   - Essential for multi-dataset workflows\n",
    "\n",
    "6. **Memory Optimization** ðŸ’¾\n",
    "   - Chunked processing for large files\n",
    "   - Float64 for numerical stability (Rheo enforces this)\n",
    "   - Profile memory usage with tracemalloc\n",
    "\n",
    "**Performance Expectations:**\n",
    "\n",
    "| Dataset Size | CPU Time | GPU Time | NLSQ vs SciPy |\n",
    "|--------------|----------|----------|---------------|\n",
    "| N=1K         | ~10 ms   | ~5 ms    | 5-10x         |\n",
    "| N=10K        | ~100 ms  | ~20 ms   | 20-50x        |\n",
    "| N=100K       | ~1 s     | ~100 ms  | 50-270x       |\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "- **Apply to your data:** Use optimization techniques on real workflows\n",
    "- **GPU setup:** See CLAUDE.md for CUDA installation (Linux only)\n",
    "- **[02-batch-processing.ipynb](02-batch-processing.ipynb):** Scale to multiple datasets\n",
    "- **[03-bayesian-uncertainty.ipynb](03-bayesian-uncertainty.ipynb):** Optimize Bayesian workflows\n",
    "\n",
    "**Resources:**\n",
    "\n",
    "- JAX documentation: https://jax.readthedocs.io/\n",
    "- NLSQ package: https://github.com/nlsq/nlsq\n",
    "- Rheo performance guide: See CLAUDE.md section on performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}