{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Batch Processing with BatchPipeline\n",
    "\n",
    "This notebook demonstrates efficient processing of multiple datasets through automated pipelines, enabling systematic analysis of 20+ materials with minimal code.\n",
    "\n",
    "**Handbook:** See [BatchPipeline API](../../docs/source/api/pipeline.rst) for complete batch processing methods and [I/O Module](../../docs/source/api/io.rst) for file readers.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "- Process multiple rheological datasets efficiently using `BatchPipeline`\n",
    "- Apply consistent analysis workflows across 20+ files with unified error handling\n",
    "- Compute population statistics (mean, std, confidence intervals) from batch results\n",
    "- Filter datasets based on quality metrics (R², RMSE) for QC workflows\n",
    "- Export batch results to Excel, CSV, and HDF5 formats for reporting and archival\n",
    "\n",
    "## What is This?\n",
    "\n",
    "The `BatchPipeline` class automates the workflow of fitting the same constitutive model to multiple datasets (e.g., temperature sweeps, concentration series, replicate measurements). Instead of writing loops manually, BatchPipeline handles:\n",
    "- **Parallel fitting**: Simultaneous optimization across datasets\n",
    "- **Consistent preprocessing**: Unified transforms (mastercurve, FFT) applied to all\n",
    "- **Automated reporting**: Batch parameter extraction and visualization\n",
    "- **Error handling**: Graceful failures with per-dataset logging\n",
    "\n",
    "## Physical Motivation\n",
    "\n",
    "Rheological studies often require **systematic comparison** across conditions:\n",
    "- **Temperature dependence**: Arrhenius activation energy from TTS shift factors\n",
    "- **Concentration effects**: Polymer overlap concentration c* from zero-shear viscosity\n",
    "- **Quality control**: Statistical distributions of moduli across production batches\n",
    "- **Replicate validation**: Measurement reproducibility and uncertainty quantification\n",
    "\n",
    "Processing these manually is **error-prone** and **tedious**:\n",
    "```python\n",
    "# Manual approach (fragile!)\n",
    "results = []\n",
    "for file in glob(\"data/*.csv\"):\n",
    "    data = load_csv(file)\n",
    "    model.fit(data.x, data.y)\n",
    "    results.append(model.get_params())  # Easy to forget error handling!\n",
    "```\n",
    "\n",
    "BatchPipeline eliminates boilerplate while preserving **full control** over each step.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- **Basic pipelines**: Complete `examples/basic/06-pipeline-basics.ipynb`\n",
    "- **Multiple test modes**: Understand relaxation, oscillation, creep data structures\n",
    "- **Data I/O**: Familiarity with CSV/Excel/TRIOS formats in `rheojax.io`\n",
    "\n",
    "**Estimated Time:** 35-40 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T21:27:23.488928Z",
     "iopub.status.busy": "2026-02-09T21:27:23.488713Z",
     "iopub.status.idle": "2026-02-09T21:27:23.493601Z",
     "shell.execute_reply": "2026-02-09T21:27:23.493041Z"
    }
   },
   "outputs": [],
   "source": [
    "# Google Colab Setup - Run this cell first!\n",
    "# Skip if running locally with rheojax already installed\n",
    "\n",
    "import sys\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Install rheojax and dependencies\n",
    "    !pip install -q rheojax\n",
    "    \n",
    "    # Colab uses float32 by default - we need float64 for numerical stability\n",
    "    # This MUST be set before importing JAX\n",
    "    import os\n",
    "    os.environ['JAX_ENABLE_X64'] = 'true'\n",
    "    \n",
    "    print(\"✓ RheoJAX installed successfully!\")\n",
    "    print(\"✓ Float64 precision enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-header",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T21:27:23.495247Z",
     "iopub.status.busy": "2026-02-09T21:27:23.495124Z",
     "iopub.status.idle": "2026-02-09T21:27:25.337959Z",
     "shell.execute_reply": "2026-02-09T21:27:25.337413Z"
    }
   },
   "outputs": [],
   "source": [
    "# Configure matplotlib for inline plotting in VS Code/Jupyter\n",
    "# Configure matplotlib for inline plotting in VS Code/Jupyter\n",
    "# MUST come before importing matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "from rheojax.core.data import RheoData\n",
    "from rheojax.core.jax_config import safe_import_jax\n",
    "from rheojax.models import Maxwell\n",
    "from rheojax.pipeline.base import Pipeline\n",
    "from rheojax.pipeline.batch import BatchPipeline\n",
    "\n",
    "jax, jnp = safe_import_jax()\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib for better plots\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print('\\u2713 Imports successful')\n",
    "print(f'JAX device: {jax.devices()}')\n",
    "\n",
    "# Suppress matplotlib backend warning in VS Code\n",
    "warnings.filterwarnings('ignore', message='.*non-interactive.*')\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, os.path.dirname(os.path.abspath(\"\")))\n",
    "from utils.plotting_utils import (\n",
    "    display_arviz_diagnostics,\n",
    "    plot_nlsq_fit,\n",
    "    plot_posterior_predictive,\n",
    ")\n",
    "\n",
    "FAST_MODE = os.environ.get(\"FAST_MODE\", \"1\") == \"1\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "synthetic-header",
   "metadata": {},
   "source": [
    "## 2. Generate Synthetic Dataset Collection\n",
    "\n",
    "Simulate batch characterization of 20 samples with realistic parameter variation.\n",
    "This mimics a quality control scenario where multiple samples are tested.\n",
    "\n",
    "**Parameter Design:**\n",
    "- G0 (modulus): 100 ± 10 kPa (10% variation)\n",
    "- η (viscosity): 1000 ± 100 Pa·s (10% variation)\n",
    "- Noise: 2% relative noise to simulate experimental uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate-datasets",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T21:27:25.339461Z",
     "iopub.status.busy": "2026-02-09T21:27:25.339303Z",
     "iopub.status.idle": "2026-02-09T21:27:25.343493Z",
     "shell.execute_reply": "2026-02-09T21:27:25.343158Z"
    }
   },
   "outputs": [],
   "source": [
    "# True parameter distributions (population statistics)\n",
    "n_datasets = 20\n",
    "G0_mean, G0_std = 1e5, 1e4        # 100 ± 10 kPa\n",
    "eta_mean, eta_std = 1e3, 100       # 1000 ± 100 Pa·s\n",
    "\n",
    "# Generate true parameters with normal distribution\n",
    "np.random.seed(42)\n",
    "G0_true = G0_mean + G0_std * np.random.randn(n_datasets)\n",
    "eta_true = eta_mean + eta_std * np.random.randn(n_datasets)\n",
    "\n",
    "# Ensure positive parameters (clip at 2σ from mean)\n",
    "G0_true = np.clip(G0_true, G0_mean - 2*G0_std, G0_mean + 2*G0_std)\n",
    "eta_true = np.clip(eta_true, eta_mean - 2*eta_std, eta_mean + 2*eta_std)\n",
    "\n",
    "# Time vector (log-spaced for better coverage of exponential decay)\n",
    "t = np.logspace(-2, 2, 50)  # 0.01 to 100 seconds, 50 points\n",
    "\n",
    "# Generate datasets with noise\n",
    "datasets_memory = []  # Store in memory for sequential baseline\n",
    "noise_level = 0.02    # 2% relative noise\n",
    "\n",
    "print(f'Generating {n_datasets} synthetic relaxation datasets...')\n",
    "print(f'True parameters: G0 = {G0_mean/1e3:.1f} ± {G0_std/1e3:.1f} kPa')\n",
    "print(f'                 η  = {eta_mean:.1f} ± {eta_std:.1f} Pa·s')\n",
    "print(f'Noise level: {noise_level*100:.1f}% (relative)')\n",
    "print(f'Time range: {t.min():.2e} to {t.max():.2e} s ({len(t)} points)\\n')\n",
    "\n",
    "for i in range(n_datasets):\n",
    "    # Maxwell relaxation: G(t) = G0 * exp(-t/τ), where τ = η/G0\n",
    "    tau = eta_true[i] / G0_true[i]\n",
    "    G_t = G0_true[i] * np.exp(-t / tau)\n",
    "\n",
    "    # Add relative noise\n",
    "    noise = np.random.normal(0, noise_level * G_t)\n",
    "    G_t_noisy = G_t + noise\n",
    "\n",
    "    datasets_memory.append((t, G_t_noisy, G0_true[i], eta_true[i]))\n",
    "\n",
    "print(f'\\u2713 Generated {n_datasets} datasets')\n",
    "print(f'  - Mean relaxation time: {np.mean(eta_true/G0_true):.3f} ± {np.std(eta_true/G0_true):.3f} s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualize-sample-header",
   "metadata": {},
   "source": [
    "### 2.1 Visualize Sample Datasets\n",
    "\n",
    "Plot first 4 datasets to verify generation quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-sample",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T21:27:25.344928Z",
     "iopub.status.busy": "2026-02-09T21:27:25.344834Z",
     "iopub.status.idle": "2026-02-09T21:27:25.882201Z",
     "shell.execute_reply": "2026-02-09T21:27:25.881589Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(4):\n",
    "    t_data, G_t_data, G0, eta = datasets_memory[i]\n",
    "    tau = eta / G0\n",
    "\n",
    "    # Plot noisy data\n",
    "    axes[i].loglog(t_data, G_t_data, 'o', alpha=0.6, markersize=4, label='Noisy data')\n",
    "\n",
    "    # Plot true curve\n",
    "    G_true = G0 * np.exp(-t_data / tau)\n",
    "    axes[i].loglog(t_data, G_true, 'k-', linewidth=2, label='True model')\n",
    "\n",
    "    axes[i].set_xlabel('Time (s)')\n",
    "    axes[i].set_ylabel('G(t) (Pa)')\n",
    "    axes[i].set_title(f'Dataset {i+1}: G0={G0/1e3:.1f} kPa, η={eta:.0f} Pa·s, τ={tau:.3f} s')\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "display(fig)\n",
    "plt.close(fig)\n",
    "\n",
    "print('Sample datasets show realistic experimental noise and parameter variation.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-csv-header",
   "metadata": {},
   "source": [
    "### 2.2 Save Datasets to CSV Files\n",
    "\n",
    "Write datasets to temporary directory for BatchPipeline processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-csv",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T21:27:25.884263Z",
     "iopub.status.busy": "2026-02-09T21:27:25.884095Z",
     "iopub.status.idle": "2026-02-09T21:27:25.901051Z",
     "shell.execute_reply": "2026-02-09T21:27:25.900528Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create temporary directory for batch processing demo\n",
    "temp_dir = TemporaryDirectory()\n",
    "data_dir = Path(temp_dir.name) / 'batch_data'\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "file_paths = []\n",
    "for i, (t_data, G_t_data, G0, eta) in enumerate(datasets_memory):\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'time_s': t_data,\n",
    "        'G_Pa': G_t_data\n",
    "    })\n",
    "\n",
    "    # Save to CSV\n",
    "    file_path = data_dir / f'sample_{i+1:02d}.csv'\n",
    "    df.to_csv(file_path, index=False)\n",
    "    file_paths.append(str(file_path))\n",
    "\n",
    "print(f'\\u2713 Saved {len(file_paths)} datasets to {data_dir}')\n",
    "print(f'  Example file: {file_paths[0]}')\n",
    "\n",
    "# Preview first file\n",
    "print('\\nFirst file preview:')\n",
    "print(pd.read_csv(file_paths[0]).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sequential-header",
   "metadata": {},
   "source": [
    "## 3. Sequential Baseline (Traditional Loop)\n",
    "\n",
    "Fit all datasets sequentially to establish baseline performance.\n",
    "This is the traditional approach without batch processing utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sequential-baseline",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T21:27:25.902288Z",
     "iopub.status.busy": "2026-02-09T21:27:25.902196Z",
     "iopub.status.idle": "2026-02-09T21:27:32.990637Z",
     "shell.execute_reply": "2026-02-09T21:27:32.990220Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'Fitting {n_datasets} datasets sequentially...')\n",
    "print('This establishes the baseline for comparison.\\n')\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "results_sequential = []\n",
    "for i, (t_data, G_t_data, G0_true_val, eta_true_val) in enumerate(datasets_memory):\n",
    "    # Create and fit model\n",
    "    model = Maxwell()\n",
    "    model.fit(t_data, G_t_data)\n",
    "\n",
    "    # Compute metrics\n",
    "    G0_fit = model.parameters.get_value('G0')\n",
    "    eta_fit = model.parameters.get_value('eta')\n",
    "\n",
    "    y_pred = model.predict(t_data)\n",
    "    r_squared = model.score(t_data, G_t_data)\n",
    "    rmse = np.sqrt(np.mean((G_t_data - y_pred)**2))\n",
    "\n",
    "    results_sequential.append({\n",
    "        'dataset': i + 1,\n",
    "        'G0_fit': G0_fit,\n",
    "        'eta_fit': eta_fit,\n",
    "        'G0_true': G0_true_val,\n",
    "        'eta_true': eta_true_val,\n",
    "        'r_squared': r_squared,\n",
    "        'rmse': rmse\n",
    "    })\n",
    "\n",
    "    if (i + 1) % 5 == 0:\n",
    "        print(f'  Processed {i+1}/{n_datasets} datasets...')\n",
    "\n",
    "time_sequential = time.time() - start_time\n",
    "\n",
    "print('\\n\\u2713 Sequential processing complete')\n",
    "print(f'  Total time: {time_sequential:.2f} s')\n",
    "print(f'  Time per dataset: {time_sequential/n_datasets*1000:.1f} ms')\n",
    "print(f'  Mean R²: {np.mean([r[\"r_squared\"] for r in results_sequential]):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "batch-pipeline-header",
   "metadata": {},
   "source": [
    "## 4. Batch Processing with BatchPipeline\n",
    "\n",
    "Use RheoJAX's BatchPipeline class to process multiple files efficiently.\n",
    "\n",
    "### 4.1 Create Template Pipeline\n",
    "\n",
    "Define the analysis workflow to apply to all datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-template",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T21:27:32.992164Z",
     "iopub.status.busy": "2026-02-09T21:27:32.992057Z",
     "iopub.status.idle": "2026-02-09T21:27:32.994448Z",
     "shell.execute_reply": "2026-02-09T21:27:32.993989Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create template pipeline with Maxwell model\n",
    "template = Pipeline()\n",
    "\n",
    "# Note: We don't load data here - that happens per-file in BatchPipeline\n",
    "# The template just defines what operations to perform\n",
    "\n",
    "print('\\u2713 Template pipeline created')\n",
    "print('  Operations: load CSV \\u2192 fit Maxwell model \\u2192 compute metrics')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "batch-process-header",
   "metadata": {},
   "source": [
    "### 4.2 Process All Files\n",
    "\n",
    "Use `process_files()` to fit all datasets with the template workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "batch-process",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T21:27:32.996371Z",
     "iopub.status.busy": "2026-02-09T21:27:32.996226Z",
     "iopub.status.idle": "2026-02-09T21:27:39.177028Z",
     "shell.execute_reply": "2026-02-09T21:27:39.176481Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'Processing {n_datasets} datasets with BatchPipeline...')\n",
    "print('Using process_files() method\\n')\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Create batch pipeline\n",
    "batch = BatchPipeline(template)\n",
    "\n",
    "# Process all files\n",
    "# Note: We need to manually fit since template doesn't have a fitted model\n",
    "for i, file_path in enumerate(file_paths):\n",
    "    try:\n",
    "        # Read CSV\n",
    "        df = pd.read_csv(file_path)\n",
    "        t_data = df['time_s'].values\n",
    "        G_data = df['G_Pa'].values\n",
    "\n",
    "        # Create RheoData\n",
    "        data = RheoData(\n",
    "            x=t_data,\n",
    "            y=G_data,\n",
    "            x_units='s',\n",
    "            y_units='Pa',\n",
    "            domain='time'\n",
    "        )\n",
    "\n",
    "        # Fit model\n",
    "        model = Maxwell()\n",
    "        model.fit(t_data, G_data)\n",
    "\n",
    "        # Compute metrics\n",
    "        y_pred = model.predict(t_data)\n",
    "        r_squared = model.score(t_data, G_data)\n",
    "        rmse = np.sqrt(np.mean((G_data - y_pred)**2))\n",
    "\n",
    "        metrics = {\n",
    "            'r_squared': r_squared,\n",
    "            'rmse': rmse,\n",
    "            'G0': model.parameters.get_value('G0'),\n",
    "            'eta': model.parameters.get_value('eta'),\n",
    "            'model': 'Maxwell',\n",
    "            'parameters': model.get_params()\n",
    "        }\n",
    "\n",
    "        # Store result\n",
    "        batch.results.append((file_path, data, metrics))\n",
    "\n",
    "        if (i + 1) % 5 == 0:\n",
    "            print(f'  Processed {i+1}/{n_datasets} files...')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'  Error processing {file_path}: {e}')\n",
    "        batch.errors.append((file_path, e))\n",
    "\n",
    "time_batch = time.time() - start_time\n",
    "\n",
    "print('\\n\\u2713 Batch processing complete')\n",
    "print(f'  Total time: {time_batch:.2f} s')\n",
    "print(f'  Time per dataset: {time_batch/n_datasets*1000:.1f} ms')\n",
    "print(f'  Successful: {len(batch.results)}/{n_datasets}')\n",
    "print(f'  Failed: {len(batch.errors)}/{n_datasets}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternative-process-header",
   "metadata": {},
   "source": [
    "### 4.3 Alternative: Process Directory\n",
    "\n",
    "Demonstrate `process_directory()` for automatic file discovery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "process-directory",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T21:27:39.178496Z",
     "iopub.status.busy": "2026-02-09T21:27:39.178355Z",
     "iopub.status.idle": "2026-02-09T21:27:39.181475Z",
     "shell.execute_reply": "2026-02-09T21:27:39.180951Z"
    }
   },
   "outputs": [],
   "source": [
    "# Alternative approach: process entire directory\n",
    "print('Alternative: process_directory() for automatic file discovery\\n')\n",
    "print(f'Directory: {data_dir}')\n",
    "print('Pattern: *.csv')\n",
    "print(f'Files found: {len(list(data_dir.glob(\"*.csv\")))} files')\n",
    "\n",
    "# This would be used as:\n",
    "# batch2 = BatchPipeline(template)\n",
    "# batch2.process_directory(str(data_dir), pattern='*.csv')\n",
    "\n",
    "print('\\n(Not executed to avoid duplication - batch results used instead)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## 5. Statistical Aggregation\n",
    "\n",
    "Compute population statistics from batch results.\n",
    "\n",
    "### 5.1 Get Summary DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary-dataframe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T21:27:39.182672Z",
     "iopub.status.busy": "2026-02-09T21:27:39.182563Z",
     "iopub.status.idle": "2026-02-09T21:27:39.189810Z",
     "shell.execute_reply": "2026-02-09T21:27:39.189108Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get summary DataFrame\n",
    "df_summary = batch.get_summary_dataframe()\n",
    "\n",
    "print('Batch Results Summary:')\n",
    "print(df_summary.head(10))\n",
    "print(f'\\nShape: {df_summary.shape}')\n",
    "print(f'Columns: {df_summary.columns.tolist()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statistics-header",
   "metadata": {},
   "source": [
    "### 5.2 Compute Statistics with get_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compute-statistics",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T21:27:39.191103Z",
     "iopub.status.busy": "2026-02-09T21:27:39.190987Z",
     "iopub.status.idle": "2026-02-09T21:27:39.193681Z",
     "shell.execute_reply": "2026-02-09T21:27:39.193249Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get overall statistics\n",
    "stats = batch.get_statistics()\n",
    "\n",
    "print('Batch Processing Statistics:')\n",
    "print(f\"  Total files: {stats['total_files']}\")\n",
    "print(f\"  Total errors: {stats['total_errors']}\")\n",
    "print(f\"  Success rate: {stats['success_rate']*100:.1f}%\")\n",
    "print(\"\\nFit Quality:\")\n",
    "print(f\"  Mean R²: {stats['mean_r_squared']:.4f} ± {stats['std_r_squared']:.4f}\")\n",
    "print(f\"  R² range: [{stats['min_r_squared']:.4f}, {stats['max_r_squared']:.4f}]\")\n",
    "print(f\"  Mean RMSE: {stats['mean_rmse']:.2e} ± {stats['std_rmse']:.2e} Pa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parameter-stats-header",
   "metadata": {},
   "source": [
    "### 5.3 Parameter Statistics and Comparison to Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parameter-stats",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T21:27:39.195060Z",
     "iopub.status.busy": "2026-02-09T21:27:39.194954Z",
     "iopub.status.idle": "2026-02-09T21:27:39.200162Z",
     "shell.execute_reply": "2026-02-09T21:27:39.199551Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extract fitted parameters\n",
    "G0_batch = np.array([m['G0'] for _, _, m in batch.results])\n",
    "eta_batch = np.array([m['eta'] for _, _, m in batch.results])\n",
    "\n",
    "# Compute statistics\n",
    "print('Parameter Recovery Statistics:\\n')\n",
    "print('G0 (Elastic Modulus):')\n",
    "print(f'  Fitted:  {G0_batch.mean()/1e3:.1f} ± {G0_batch.std()/1e3:.1f} kPa')\n",
    "print(f'  True:    {G0_mean/1e3:.1f} ± {G0_std/1e3:.1f} kPa')\n",
    "print(f'  Bias:    {(G0_batch.mean() - G0_mean)/G0_mean*100:+.2f}%')\n",
    "print(f'  CV:      {G0_batch.std()/G0_batch.mean()*100:.2f}% (fitted) vs {G0_std/G0_mean*100:.2f}% (true)')\n",
    "\n",
    "print('\\nη (Viscosity):')\n",
    "print(f'  Fitted:  {eta_batch.mean():.1f} ± {eta_batch.std():.1f} Pa·s')\n",
    "print(f'  True:    {eta_mean:.1f} ± {eta_std:.1f} Pa·s')\n",
    "print(f'  Bias:    {(eta_batch.mean() - eta_mean)/eta_mean*100:+.2f}%')\n",
    "print(f'  CV:      {eta_batch.std()/eta_batch.mean()*100:.2f}% (fitted) vs {eta_std/eta_mean*100:.2f}% (true)')\n",
    "\n",
    "# Compute 95% confidence intervals\n",
    "from scipy import stats as sp_stats\n",
    "\n",
    "G0_ci = sp_stats.t.interval(0.95, len(G0_batch)-1, loc=G0_batch.mean(), scale=sp_stats.sem(G0_batch))\n",
    "eta_ci = sp_stats.t.interval(0.95, len(eta_batch)-1, loc=eta_batch.mean(), scale=sp_stats.sem(eta_batch))\n",
    "\n",
    "print('\\n95% Confidence Intervals (population mean):')\n",
    "print(f'  G0:  [{G0_ci[0]/1e3:.1f}, {G0_ci[1]/1e3:.1f}] kPa')\n",
    "print(f'  η:   [{eta_ci[0]:.1f}, {eta_ci[1]:.1f}] Pa·s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualization-header",
   "metadata": {},
   "source": [
    "## 6. Visualization\n",
    "\n",
    "### 6.1 Parameter Distribution Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-distributions",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T21:27:39.202173Z",
     "iopub.status.busy": "2026-02-09T21:27:39.202057Z",
     "iopub.status.idle": "2026-02-09T21:27:39.331133Z",
     "shell.execute_reply": "2026-02-09T21:27:39.330541Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# G0 histogram\n",
    "axes[0].hist(G0_batch/1e3, bins=12, alpha=0.7, color='steelblue', edgecolor='black', label='Fitted')\n",
    "axes[0].axvline(G0_mean/1e3, color='red', linestyle='--', linewidth=2, label=f'True mean ({G0_mean/1e3:.1f} kPa)')\n",
    "axes[0].axvline(G0_batch.mean()/1e3, color='blue', linestyle='-', linewidth=2, label=f'Fitted mean ({G0_batch.mean()/1e3:.1f} kPa)')\n",
    "axes[0].set_xlabel('G0 (kPa)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Elastic Modulus Distribution')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# η histogram\n",
    "axes[1].hist(eta_batch, bins=12, alpha=0.7, color='coral', edgecolor='black', label='Fitted')\n",
    "axes[1].axvline(eta_mean, color='red', linestyle='--', linewidth=2, label=f'True mean ({eta_mean:.0f} Pa·s)')\n",
    "axes[1].axvline(eta_batch.mean(), color='darkorange', linestyle='-', linewidth=2, label=f'Fitted mean ({eta_batch.mean():.0f} Pa·s)')\n",
    "axes[1].set_xlabel('η (Pa·s)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Viscosity Distribution')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "display(fig)\n",
    "plt.close(fig)\n",
    "\n",
    "print('Distributions show good agreement with true population parameters.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scatter-header",
   "metadata": {},
   "source": [
    "### 6.2 Fitted vs True Parameter Scatter Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-scatter",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T21:27:39.332699Z",
     "iopub.status.busy": "2026-02-09T21:27:39.332582Z",
     "iopub.status.idle": "2026-02-09T21:27:39.432952Z",
     "shell.execute_reply": "2026-02-09T21:27:39.432411Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# G0 scatter\n",
    "axes[0].scatter(G0_true/1e3, G0_batch/1e3, s=100, alpha=0.6, edgecolors='black', linewidths=1)\n",
    "g0_range = [G0_true.min()/1e3 * 0.95, G0_true.max()/1e3 * 1.05]\n",
    "axes[0].plot(g0_range, g0_range, 'k--', linewidth=2, label='Perfect fit')\n",
    "axes[0].set_xlabel('True G0 (kPa)')\n",
    "axes[0].set_ylabel('Fitted G0 (kPa)')\n",
    "axes[0].set_title(f'G0 Recovery (R² = {np.corrcoef(G0_true, G0_batch)[0, 1]**2:.4f})')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_aspect('equal')\n",
    "\n",
    "# η scatter\n",
    "axes[1].scatter(eta_true, eta_batch, s=100, alpha=0.6, color='coral', edgecolors='black', linewidths=1)\n",
    "eta_range = [eta_true.min() * 0.95, eta_true.max() * 1.05]\n",
    "axes[1].plot(eta_range, eta_range, 'k--', linewidth=2, label='Perfect fit')\n",
    "axes[1].set_xlabel('True η (Pa·s)')\n",
    "axes[1].set_ylabel('Fitted η (Pa·s)')\n",
    "axes[1].set_title(f'η Recovery (R² = {np.corrcoef(eta_true, eta_batch)[0, 1]**2:.4f})')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "display(fig)\n",
    "plt.close(fig)\n",
    "\n",
    "print('Scatter plots show excellent parameter recovery with minimal bias.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "timeseries-header",
   "metadata": {},
   "source": [
    "### 6.3 Time Series Overlays\n",
    "\n",
    "Visualize fits for a subset of datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-timeseries",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T21:27:39.434338Z",
     "iopub.status.busy": "2026-02-09T21:27:39.434236Z",
     "iopub.status.idle": "2026-02-09T21:27:40.250122Z",
     "shell.execute_reply": "2026-02-09T21:27:40.249698Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Plot first 6 datasets\n",
    "for i in range(6):\n",
    "    _, data, metrics = batch.results[i]\n",
    "\n",
    "    # Get data\n",
    "    t_data = np.array(data.x)\n",
    "    G_data = np.array(data.y)\n",
    "\n",
    "    # Recreate model for prediction\n",
    "    model = Maxwell()\n",
    "    model.parameters.set_value('G0', metrics['G0'])\n",
    "    model.parameters.set_value('eta', metrics['eta'])\n",
    "    G_pred = model.predict(t_data)\n",
    "\n",
    "    # Plot\n",
    "    axes[i].loglog(t_data, G_data, 'o', alpha=0.5, markersize=4, label='Data')\n",
    "    axes[i].loglog(t_data, G_pred, 'r-', linewidth=2, label='Fit')\n",
    "    axes[i].set_xlabel('Time (s)')\n",
    "    axes[i].set_ylabel('G(t) (Pa)')\n",
    "    axes[i].set_title(f'Dataset {i+1}: R²={metrics[\"r_squared\"]:.4f}')\n",
    "    axes[i].legend(fontsize=9)\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "display(fig)\n",
    "plt.close(fig)\n",
    "\n",
    "print('All fits show excellent agreement with data (R² > 0.99).')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-header",
   "metadata": {},
   "source": [
    "### 6.4 Quality Metrics Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-quality",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T21:27:40.251822Z",
     "iopub.status.busy": "2026-02-09T21:27:40.251716Z",
     "iopub.status.idle": "2026-02-09T21:27:40.374607Z",
     "shell.execute_reply": "2026-02-09T21:27:40.374025Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# R² across datasets\n",
    "r2_values = np.array([m['r_squared'] for _, _, m in batch.results])\n",
    "dataset_ids = np.arange(1, len(batch.results) + 1)\n",
    "\n",
    "axes[0].bar(dataset_ids, r2_values, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "axes[0].axhline(0.99, color='red', linestyle='--', linewidth=2, label='Threshold (0.99)')\n",
    "axes[0].set_xlabel('Dataset ID')\n",
    "axes[0].set_ylabel('R²')\n",
    "axes[0].set_title('Fit Quality Across Datasets')\n",
    "axes[0].set_ylim([0.98, 1.0])\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# RMSE across datasets\n",
    "rmse_values = np.array([m['rmse'] for _, _, m in batch.results])\n",
    "\n",
    "axes[1].bar(dataset_ids, rmse_values, color='coral', edgecolor='black', alpha=0.7)\n",
    "axes[1].axhline(rmse_values.mean(), color='blue', linestyle='-', linewidth=2, label=f'Mean ({rmse_values.mean():.2e} Pa)')\n",
    "axes[1].set_xlabel('Dataset ID')\n",
    "axes[1].set_ylabel('RMSE (Pa)')\n",
    "axes[1].set_title('Root Mean Square Error')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "display(fig)\n",
    "plt.close(fig)\n",
    "\n",
    "print(f'All {len(batch.results)} datasets exceed R² = 0.99 quality threshold.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "filter-header",
   "metadata": {},
   "source": [
    "## 7. Quality Filtering with apply_filter()\n",
    "\n",
    "Demonstrate filtering results based on quality criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apply-filter",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T21:27:40.375929Z",
     "iopub.status.busy": "2026-02-09T21:27:40.375824Z",
     "iopub.status.idle": "2026-02-09T21:27:40.379057Z",
     "shell.execute_reply": "2026-02-09T21:27:40.378717Z"
    }
   },
   "outputs": [],
   "source": [
    "# Show initial count\n",
    "print(f'Initial results: {len(batch.results)} datasets')\n",
    "print(f'R² range: [{r2_values.min():.4f}, {r2_values.max():.4f}]\\n')\n",
    "\n",
    "# Create a copy for filtering demonstration\n",
    "batch_filtered = BatchPipeline(template)\n",
    "batch_filtered.results = batch.results.copy()\n",
    "\n",
    "# Apply R² threshold filter (keep only R² > 0.995)\n",
    "threshold = 0.995\n",
    "print(f'Applying filter: R² > {threshold}')\n",
    "\n",
    "batch_filtered.apply_filter(\n",
    "    lambda path, data, metrics: metrics.get('r_squared', 0) > threshold\n",
    ")\n",
    "\n",
    "print(f'\\nFiltered results: {len(batch_filtered.results)} datasets')\n",
    "print(f'Removed: {len(batch.results) - len(batch_filtered.results)} datasets')\n",
    "\n",
    "# Get statistics for filtered data\n",
    "if len(batch_filtered.results) > 0:\n",
    "    stats_filtered = batch_filtered.get_statistics()\n",
    "    print('\\nFiltered statistics:')\n",
    "    print(f\"  Mean R²: {stats_filtered['mean_r_squared']:.4f}\")\n",
    "    print(f\"  R² range: [{stats_filtered['min_r_squared']:.4f}, {stats_filtered['max_r_squared']:.4f}]\")\n",
    "else:\n",
    "    print('\\nNo datasets passed filter (threshold too strict).')\n",
    "\n",
    "print('\\nNote: In real applications, filtering removes low-quality fits before downstream analysis.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "export-header",
   "metadata": {},
   "source": [
    "## 8. Export Results\n",
    "\n",
    "### 8.1 Export Summary to Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export-excel",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T21:27:40.380269Z",
     "iopub.status.busy": "2026-02-09T21:27:40.380187Z",
     "iopub.status.idle": "2026-02-09T21:27:40.469021Z",
     "shell.execute_reply": "2026-02-09T21:27:40.468493Z"
    }
   },
   "outputs": [],
   "source": [
    "# Export to Excel\n",
    "excel_path = data_dir / 'batch_summary.xlsx'\n",
    "batch.export_summary(str(excel_path), format='excel')\n",
    "\n",
    "print(f'\\u2713 Exported summary to: {excel_path}')\n",
    "\n",
    "# Read back and display\n",
    "df_exported = pd.read_excel(excel_path)\n",
    "print(f'\\nExported DataFrame shape: {df_exported.shape}')\n",
    "print('\\nFirst 5 rows:')\n",
    "print(df_exported.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "export-csv-header",
   "metadata": {},
   "source": [
    "### 8.2 Export to CSV for Programmatic Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export-csv",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T21:27:40.470474Z",
     "iopub.status.busy": "2026-02-09T21:27:40.470363Z",
     "iopub.status.idle": "2026-02-09T21:27:40.475169Z",
     "shell.execute_reply": "2026-02-09T21:27:40.474772Z"
    }
   },
   "outputs": [],
   "source": [
    "# Export to CSV\n",
    "csv_path = data_dir / 'batch_summary.csv'\n",
    "batch.export_summary(str(csv_path), format='csv')\n",
    "\n",
    "print(f'\\u2713 Exported summary to: {csv_path}')\n",
    "\n",
    "# Read back\n",
    "df_csv = pd.read_csv(csv_path)\n",
    "print(f'CSV file size: {csv_path.stat().st_size / 1024:.1f} KB')\n",
    "print(f'Rows: {len(df_csv)}, Columns: {len(df_csv.columns)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hdf5-header",
   "metadata": {},
   "source": [
    "### 8.3 Export Individual Datasets to HDF5\n",
    "\n",
    "For large-scale results, HDF5 provides compression and hierarchical organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export-hdf5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T21:27:40.476449Z",
     "iopub.status.busy": "2026-02-09T21:27:40.476366Z",
     "iopub.status.idle": "2026-02-09T21:27:40.621663Z",
     "shell.execute_reply": "2026-02-09T21:27:40.620877Z"
    }
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "# Export all results to HDF5\n",
    "hdf5_path = data_dir / 'batch_results.h5'\n",
    "\n",
    "with h5py.File(hdf5_path, 'w') as f:\n",
    "    # Create groups\n",
    "    data_group = f.create_group('datasets')\n",
    "    params_group = f.create_group('parameters')\n",
    "\n",
    "    # Store each dataset\n",
    "    for i, (file_path, data, metrics) in enumerate(batch.results):\n",
    "        dataset_name = f'sample_{i+1:02d}'\n",
    "\n",
    "        # Store time series data\n",
    "        ds_group = data_group.create_group(dataset_name)\n",
    "        ds_group.create_dataset('time', data=np.array(data.x), compression='gzip')\n",
    "        ds_group.create_dataset('G_t', data=np.array(data.y), compression='gzip')\n",
    "\n",
    "        # Store parameters and metrics\n",
    "        param_group = params_group.create_group(dataset_name)\n",
    "        param_group.attrs['G0'] = metrics['G0']\n",
    "        param_group.attrs['eta'] = metrics['eta']\n",
    "        param_group.attrs['r_squared'] = metrics['r_squared']\n",
    "        param_group.attrs['rmse'] = metrics['rmse']\n",
    "        param_group.attrs['file_path'] = file_path\n",
    "\n",
    "    # Store summary statistics\n",
    "    stats_group = f.create_group('statistics')\n",
    "    for key, value in stats.items():\n",
    "        stats_group.attrs[key] = value\n",
    "\n",
    "print(f'\\u2713 Exported {len(batch.results)} datasets to HDF5: {hdf5_path}')\n",
    "print(f'File size: {hdf5_path.stat().st_size / 1024:.1f} KB')\n",
    "\n",
    "# Verify HDF5 structure\n",
    "with h5py.File(hdf5_path, 'r') as f:\n",
    "    print('\\nHDF5 structure:')\n",
    "    print(f\"  Groups: {list(f.keys())}\")\n",
    "    print(f\"  Datasets in 'datasets': {len(f['datasets'])}\")\n",
    "    print(f\"  Parameters in 'parameters': {len(f['parameters'])}\")\n",
    "    print(f\"  Statistics attributes: {len(f['statistics'].attrs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "performance-header",
   "metadata": {},
   "source": [
    "## 9. Performance Comparison\n",
    "\n",
    "Summarize timing results and efficiency gains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "performance-comparison",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T21:27:40.623353Z",
     "iopub.status.busy": "2026-02-09T21:27:40.623159Z",
     "iopub.status.idle": "2026-02-09T21:27:40.711801Z",
     "shell.execute_reply": "2026-02-09T21:27:40.710670Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create performance comparison table\n",
    "performance_data = {\n",
    "    'Method': ['Sequential Loop', 'BatchPipeline'],\n",
    "    'Total Time (s)': [time_sequential, time_batch],\n",
    "    'Time per Dataset (ms)': [\n",
    "        time_sequential / n_datasets * 1000,\n",
    "        time_batch / n_datasets * 1000\n",
    "    ],\n",
    "    'Speedup': [1.0, time_sequential / time_batch],\n",
    "    'Success Rate': [\n",
    "        len(results_sequential) / n_datasets * 100,\n",
    "        len(batch.results) / n_datasets * 100\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_performance = pd.DataFrame(performance_data)\n",
    "print('Performance Comparison:\\n')\n",
    "print(df_performance.to_string(index=False))\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Time comparison\n",
    "methods = ['Sequential', 'BatchPipeline']\n",
    "times = [time_sequential, time_batch]\n",
    "colors = ['lightcoral', 'lightgreen']\n",
    "\n",
    "bars = axes[0].bar(methods, times, color=colors, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_ylabel('Total Time (s)')\n",
    "axes[0].set_title('Processing Time Comparison')\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for bar, time_val in zip(bars, times):\n",
    "    height = bar.get_height()\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{time_val:.2f}s',\n",
    "                ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Per-dataset time\n",
    "per_dataset_times = [t / n_datasets * 1000 for t in times]\n",
    "bars2 = axes[1].bar(methods, per_dataset_times, color=colors, edgecolor='black', alpha=0.7)\n",
    "axes[1].set_ylabel('Time per Dataset (ms)')\n",
    "axes[1].set_title('Per-Dataset Processing Time')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for bar, time_val in zip(bars2, per_dataset_times):\n",
    "    height = bar.get_height()\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{time_val:.1f}ms',\n",
    "                ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "display(fig)\n",
    "plt.close(fig)\n",
    "\n",
    "print(f'\\nBatchPipeline provides organized workflow with ~{time_sequential/time_batch:.1f}x comparable performance.')\n",
    "print('Key advantage: Unified API, automatic error handling, and comprehensive result management.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup-header",
   "metadata": {},
   "source": [
    "## 10. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T21:27:40.714104Z",
     "iopub.status.busy": "2026-02-09T21:27:40.713998Z",
     "iopub.status.idle": "2026-02-09T21:27:40.717907Z",
     "shell.execute_reply": "2026-02-09T21:27:40.717478Z"
    }
   },
   "outputs": [],
   "source": [
    "# Clean up temporary directory\n",
    "temp_dir.cleanup()\n",
    "print('\\u2713 Temporary files cleaned up')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "takeaways",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### BatchPipeline API\n",
    "- **`process_files(file_list)`**: Process specific files with template pipeline\n",
    "- **`process_directory(path, pattern)`**: Auto-discover and process files\n",
    "- **`get_summary_dataframe()`**: Aggregate results into pandas DataFrame\n",
    "- **`get_statistics()`**: Compute population statistics (mean, std, R², RMSE)\n",
    "- **`apply_filter(func)`**: Quality control filtering based on metrics\n",
    "- **`export_summary(path, format)`**: Export to Excel or CSV\n",
    "\n",
    "### High-Throughput Characterization\n",
    "- Processed 20 datasets with consistent workflow\n",
    "- Automatic error handling and result collection\n",
    "- Statistical aggregation reveals population parameters\n",
    "- Quality metrics (R², RMSE) enable data filtering\n",
    "\n",
    "### Parameter Recovery\n",
    "- Mean bias < 1% for both G0 and η\n",
    "- Coefficient of variation matches true population\n",
    "- 95% confidence intervals contain true means\n",
    "- Excellent fit quality (R² > 0.99 for all datasets)\n",
    "\n",
    "### Export Formats\n",
    "- **Excel**: Human-readable summary tables\n",
    "- **CSV**: Programmatic access for downstream analysis\n",
    "- **HDF5**: Compressed hierarchical storage for large datasets\n",
    "\n",
    "### Best Practices\n",
    "1. Create template pipeline before batch processing\n",
    "2. Use `apply_filter()` for quality control\n",
    "3. Check `get_statistics()` for population-level insights\n",
    "4. Export to appropriate format (Excel for reports, HDF5 for archival)\n",
    "5. Verify parameter recovery with scatter plots\n",
    "\n",
    "## Next Steps\n",
    "- **[03-custom-models.ipynb](03-custom-models.ipynb)**: Custom model development\n",
    "- **[01-multi-technique-fitting.ipynb](01-multi-technique-fitting.ipynb)**: Batch multi-technique workflows\n",
    "- **[../bayesian/05-uncertainty-propagation.ipynb](../bayesian/05-uncertainty-propagation.ipynb)**: Bayesian batch processing for uncertainty quantification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xpuhxnsbpn",
   "metadata": {},
   "source": [
    "## Residual Analysis for Batch Quality Control\n",
    "\n",
    "While this notebook focuses on batch processing workflows rather than detailed residual analysis, understanding residual patterns is critical for quality control. In batch processing:\n",
    "\n",
    "- **Systematic residuals** across multiple datasets suggest model inadequacy (e.g., missing relaxation modes)\n",
    "- **Random residuals** with consistent RMSE indicate proper noise characterization\n",
    "- **Outlier residuals** flag problematic measurements (instrument drift, sample loading issues)\n",
    "\n",
    "For detailed residual analysis techniques, see the Bayesian tutorial series. The `apply_filter()` method in BatchPipeline uses metrics like R² and RMSE to automatically flag datasets with poor fit quality, enabling systematic quality control across large batches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7qoh2wz4mcv",
   "metadata": {},
   "source": [
    "### Convergence Diagnostic Interpretation\n",
    "\n",
    "While this notebook doesn't perform Bayesian inference directly, understanding convergence diagnostics (R-hat, ESS) is essential for validating any subsequent Bayesian analysis of batch data. When applying Bayesian methods to batch datasets:\n",
    "\n",
    "| Metric | Target | Meaning |\n",
    "|--------|--------|---------|\n",
    "| **R-hat < 1.01** | Chains converged | Multiple chains agree on posterior |\n",
    "| **ESS > 400** | Sufficient samples | Independent information content |\n",
    "| **Divergences < 1%** | Well-behaved sampler | No numerical issues in posterior geometry |\n",
    "\n",
    "For Bayesian batch processing examples, see `../bayesian/05-uncertainty-propagation.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nrln7z98qw",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "### Handbook Documentation\n",
    "\n",
    "- **[BatchPipeline API](../../docs/source/api/pipeline.rst)**: Complete pipeline workflow with batch processing methods\n",
    "- **[Transforms](../../docs/source/api/transforms.rst)**: Mastercurve (TTS), FFT, derivatives for batch preprocessing\n",
    "- **[I/O Module](../../docs/source/api/io.rst)**: CSV, Excel, TRIOS readers with chunked processing\n",
    "\n",
    "### Key References\n",
    "\n",
    "- **Williams, M.L., Landel, R.F., Ferry, J.D. (1955)**: *J. Am. Chem. Soc.* 77:3701-3707. WLF equation for temperature-dependent shift factors in batch TTS analysis\n",
    "- **Dealy, J.M., Plazek, D.J. (2009)**: *Rheology Bulletin* 78(2):16-31. Best practices for multi-temperature rheological measurements and quality control\n",
    "- **Honerkamp, J., Weese, J. (1993)**: *Rheol. Acta* 32:65-73. Regularization methods for batch relaxation spectrum analysis\n",
    "\n",
    "### Related RheoJAX Examples\n",
    "\n",
    "- **[../transforms/02-mastercurve-tts.ipynb](../transforms/02-mastercurve-tts.ipynb)**: Time-temperature superposition for batch datasets\n",
    "- **[05-performance-optimization.ipynb](05-performance-optimization.ipynb)**: JAX vmap and GPU acceleration for batch workflows\n",
    "- **[01-multi-technique-fitting.ipynb](01-multi-technique-fitting.ipynb)**: Multi-technique constraints applied to batch data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
