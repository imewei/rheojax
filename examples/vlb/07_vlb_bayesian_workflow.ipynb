{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VLB Bayesian Workflow\n",
    "\n",
    "**VLB transient network — Full NLSQ → NUTS inference pipeline**\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Complete NLSQ → NUTS Bayesian workflow for VLBLocal\n",
    "- Evaluate convergence diagnostics (R-hat, ESS, divergences)\n",
    "- Construct posterior predictive checks\n",
    "- Compare posteriors from different protocols\n",
    "\n",
    "## Estimated Runtime\n",
    "\n",
    "- Fast demo (1 chain): ~2 min\n",
    "- Full run (4 chains): ~5-10 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "if IN_COLAB:\n",
    "    %pip install -q rheojax\n",
    "    import os\n",
    "    os.environ[\"JAX_ENABLE_X64\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "\n",
    "from rheojax.core.jax_config import safe_import_jax, verify_float64\n",
    "from rheojax.models import VLBLocal\n",
    "\n",
    "jax, jnp = safe_import_jax()\n",
    "verify_float64()\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)  # JAX/equinox upstream deprecation churn — not actionable in user notebooks\n",
    "print(f\"JAX version: {jax.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Synthetic SAOS Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground truth\n",
    "G0_true = 5000.0   # Pa\n",
    "kd_true = 2.0      # 1/s\n",
    "tR = 1.0 / kd_true\n",
    "\n",
    "np.random.seed(42)\n",
    "omega = np.logspace(-2, 3, 40)\n",
    "\n",
    "G_prime_true = G0_true * (omega * tR)**2 / (1 + (omega * tR)**2)\n",
    "G_double_prime_true = G0_true * (omega * tR) / (1 + (omega * tR)**2)\n",
    "\n",
    "# 3% noise\n",
    "noise = 0.03\n",
    "G_prime = G_prime_true * (1 + noise * np.random.randn(len(omega)))\n",
    "G_double_prime = G_double_prime_true * (1 + noise * np.random.randn(len(omega)))\n",
    "G_star = G_prime + 1j * G_double_prime\n",
    "\n",
    "print(f\"True: G₀ = {G0_true} Pa, k_d = {kd_true} 1/s\")\n",
    "print(f\"Data: {len(omega)} frequencies, ω = [{omega.min():.3f}, {omega.max():.0f}] rad/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Step 1: NLSQ Point Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "model = VLBLocal()\n\nt0 = time.time()\nmodel.fit(omega, G_star, test_mode=\"oscillation\")\nt_nlsq = time.time() - t0\n\nprint(f\"NLSQ fit time: {t_nlsq:.2f} s\")\nprint(f\"G₀  = {model.G0:.1f} Pa (true: {G0_true})\")\nprint(f\"k_d = {model.k_d:.4f} 1/s (true: {kd_true})\")\n\n# Fit quality — use predict_saos for G', G'' and compare with |G*|\nG_prime_pred, G_double_prime_pred = model.predict_saos(omega)\nG_star_pred_abs = np.sqrt(np.array(G_prime_pred)**2 + np.array(G_double_prime_pred)**2)\nG_star_data_abs = np.abs(G_star)\nss_res = np.sum((G_star_data_abs - G_star_pred_abs)**2)\nss_tot = np.sum((G_star_data_abs - np.mean(G_star_data_abs))**2)\nr2 = 1 - ss_res / ss_tot\nprint(f\"R² = {r2:.6f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Step 2: Bayesian Inference with NUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Fast demo config ---\n",
    "NUM_WARMUP = 200\n",
    "NUM_SAMPLES = 500\n",
    "NUM_CHAINS = 1\n",
    "# --- Production config (uncomment for full run) ---\n",
    "# NUM_WARMUP = 1000\n",
    "# NUM_SAMPLES = 2000\n",
    "# NUM_CHAINS = 4\n",
    "\n",
    "# Warm-start from NLSQ\n",
    "initial_values = {\"G0\": float(model.G0), \"k_d\": float(model.k_d)}\n",
    "print(f\"Warm-start: {initial_values}\")\n",
    "\n",
    "t0 = time.time()\n",
    "result = model.fit_bayesian(\n",
    "    omega, G_star, test_mode=\"oscillation\",\n",
    "    num_warmup=NUM_WARMUP,\n",
    "    num_samples=NUM_SAMPLES,\n",
    "    num_chains=NUM_CHAINS,\n",
    "    initial_values=initial_values,\n",
    "    seed=42,\n",
    ")\n",
    "t_bayes = time.time() - t0\n",
    "print(f\"\\nBayesian inference time: {t_bayes:.1f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Convergence Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag = result.diagnostics\n",
    "param_names = [\"G0\", \"k_d\"]\n",
    "\n",
    "print(\"Convergence Diagnostics\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"{'Parameter':>10s}  {'R-hat':>8s}  {'ESS':>8s}\")\n",
    "print(\"-\" * 50)\n",
    "for p in param_names:\n",
    "    r_hat = diag.get(\"r_hat\", {}).get(p, float(\"nan\"))\n",
    "    ess = diag.get(\"ess\", {}).get(p, float(\"nan\"))\n",
    "    print(f\"{p:>10s}  {r_hat:8.4f}  {ess:8.0f}\")\n",
    "\n",
    "n_div = diag.get(\"divergences\", diag.get(\"num_divergences\", 0))\n",
    "print(f\"\\nDivergences: {n_div}\")\n",
    "\n",
    "r_hat_ok = all(diag.get(\"r_hat\", {}).get(p, 2.0) < 1.05 for p in param_names)\n",
    "ess_ok = all(diag.get(\"ess\", {}).get(p, 0) > 100 for p in param_names)\n",
    "print(f\"\\nConvergence: {'PASSED' if r_hat_ok and ess_ok else 'CHECK REQUIRED'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ArviZ trace plots\n",
    "idata = result.to_inference_data()\n",
    "\n",
    "axes = az.plot_trace(idata, var_names=param_names, figsize=(12, 5))\n",
    "fig = axes.ravel()[0].figure\n",
    "fig.suptitle(\"Trace Plots\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "display(fig)\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forest plot\n",
    "axes = az.plot_forest(\n",
    "    idata, var_names=param_names,\n",
    "    combined=True, hdi_prob=0.95, figsize=(10, 3),\n",
    ")\n",
    "fig = axes.ravel()[0].figure\n",
    "plt.tight_layout()\n",
    "display(fig)\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Posterior Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior = result.posterior_samples\n",
    "\n",
    "print(\"Parameter Comparison\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Param':>8s}  {'True':>8s}  {'NLSQ':>10s}  {'Bayes (median)':>14s}  {'95% CI':>20s}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "true_vals = {\"G0\": G0_true, \"k_d\": kd_true}\n",
    "nlsq_vals = {\"G0\": model.G0, \"k_d\": model.k_d}\n",
    "\n",
    "for p in param_names:\n",
    "    samples = np.array(posterior[p])\n",
    "    med = np.median(samples)\n",
    "    lo, hi = np.percentile(samples, [2.5, 97.5])\n",
    "    print(f\"{p:>8s}  {true_vals[p]:8.1f}  {nlsq_vals[p]:10.1f}  {med:14.1f}  [{lo:.1f}, {hi:.1f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Posterior Predictive Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "omega_fine = np.logspace(-2.5, 3.5, 200)\nn_draws = min(200, len(posterior[\"G0\"]))\n\nGp_samples = []\nGdp_samples = []\n\nfor i in range(n_draws):\n    model.parameters.set_value(\"G0\", float(posterior[\"G0\"][i]))\n    model.parameters.set_value(\"k_d\", float(posterior[\"k_d\"][i]))\n    Gp_i, Gdp_i = model.predict_saos(omega_fine)\n    Gp_samples.append(np.array(Gp_i))\n    Gdp_samples.append(np.array(Gdp_i))\n\nGp_arr = np.array(Gp_samples)\nGdp_arr = np.array(Gdp_samples)\n\nfig, ax = plt.subplots(figsize=(9, 6))\n\n# 95% CI bands\nax.fill_between(omega_fine, np.percentile(Gp_arr, 2.5, axis=0),\n                np.percentile(Gp_arr, 97.5, axis=0), alpha=0.2, color=\"C0\")\nax.fill_between(omega_fine, np.percentile(Gdp_arr, 2.5, axis=0),\n                np.percentile(Gdp_arr, 97.5, axis=0), alpha=0.2, color=\"C1\")\n\n# Medians\nax.loglog(omega_fine, np.median(Gp_arr, axis=0), \"C0-\", lw=2, label=\"G' (posterior median)\")\nax.loglog(omega_fine, np.median(Gdp_arr, axis=0), \"C1-\", lw=2, label=\"G'' (posterior median)\")\n\n# Data\nax.loglog(omega, np.real(G_star), \"ko\", markersize=4, label=\"G' data\")\nax.loglog(omega, np.imag(G_star), \"ks\", markersize=4, label=\"G'' data\")\n\nax.set_xlabel(\"ω [rad/s]\")\nax.set_ylabel(\"G', G'' [Pa]\")\nax.set_title(\"Posterior Predictive Check\")\nax.legend(fontsize=9)\nax.grid(True, alpha=0.3, which=\"both\")\nplt.tight_layout()\ndisplay(fig)\nplt.close(fig)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Derived Quantities with Uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G0_samples = np.array(posterior[\"G0\"])\n",
    "kd_samples = np.array(posterior[\"k_d\"])\n",
    "\n",
    "tR_samples = 1.0 / kd_samples\n",
    "eta_samples = G0_samples / kd_samples\n",
    "Psi1_samples = 2.0 * G0_samples / kd_samples**2\n",
    "\n",
    "print(\"Derived Quantities (posterior)\")\n",
    "print(\"=\" * 55)\n",
    "for name, samples, true_val in [\n",
    "    (\"t_R [s]\", tR_samples, 1/kd_true),\n",
    "    (\"η₀ [Pa·s]\", eta_samples, G0_true/kd_true),\n",
    "    (\"Ψ₁ [Pa·s²]\", Psi1_samples, 2*G0_true/kd_true**2),\n",
    "]:\n",
    "    med = np.median(samples)\n",
    "    lo, hi = np.percentile(samples, [2.5, 97.5])\n",
    "    print(f\"  {name:12s}: {med:.1f} [{lo:.1f}, {hi:.1f}] (true: {true_val:.1f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "\n",
    "output_dir = os.path.join(\"..\", \"outputs\", \"vlb\", \"bayesian\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "posterior_dict = {k: np.array(v).tolist() for k, v in posterior.items()}\n",
    "with open(os.path.join(output_dir, \"posterior_samples.json\"), \"w\") as f:\n",
    "    json.dump(posterior_dict, f)\n",
    "\n",
    "print(f\"Saved to {output_dir}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **NLSQ warm-start is critical** for fast NUTS convergence\n",
    "2. **VLBLocal has only 2 parameters** → well-identified posteriors\n",
    "3. **R-hat < 1.01 and ESS > 400** are the convergence targets\n",
    "4. **Derived quantities** (t_R, η₀, Ψ₁) inherit uncertainty from the posterior\n",
    "5. **Posterior predictive checks** verify the model captures the data\n",
    "\n",
    "## Production Tips\n",
    "\n",
    "- Use `num_chains=4` for proper convergence diagnostics\n",
    "- Use `num_warmup=1000, num_samples=2000` for publication-quality results\n",
    "- Check `az.plot_energy()` for sampling efficiency\n",
    "- If divergences appear, increase `num_warmup` or check parameter bounds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
