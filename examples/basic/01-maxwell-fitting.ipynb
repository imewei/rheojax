{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# Maxwell Model: Stress Relaxation Fitting\n",
    "\n",
    "This notebook demonstrates the complete workflow for fitting the Maxwell model to stress relaxation data, showcasing modern Rheo capabilities including GPU-accelerated optimization and Bayesian uncertainty quantification.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "After completing this notebook, you will be able to:\n",
    "- Fit the Maxwell model using both Pipeline API and Modular API approaches\n",
    "- Leverage NLSQ optimization for 5-270x speedup over SciPy\n",
    "- Perform Bayesian inference with NLSQ→NUTS warm-start workflow\n",
    "- Interpret all 6 ArviZ diagnostic plots for MCMC convergence\n",
    "- Extract physically meaningful parameters with uncertainty quantification\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Basic understanding of:\n",
    "- Rheological concepts (stress, strain, relaxation)\n",
    "- Linear viscoelasticity\n",
    "- Python and NumPy\n",
    "\n",
    "**Estimated Time:** 30-40 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab Setup - Run this cell first!\n",
    "# Skip if running locally with rheojax already installed\n",
    "\n",
    "import sys\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Install rheojax and dependencies\n",
    "    !pip install -q rheojax\n",
    "    \n",
    "    # Colab uses float32 by default - we need float64 for numerical stability\n",
    "    # This MUST be set before importing JAX\n",
    "    import os\n",
    "    os.environ['JAX_ENABLE_X64'] = 'true'\n",
    "    \n",
    "    print(\"✓ RheoJAX installed successfully!\")\n",
    "    print(\"✓ Float64 precision enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-intro",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "We start by importing necessary libraries. Note the **safe JAX import pattern** - this is critical for ensuring float64 precision throughout the entire JAX stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure matplotlib for inline plotting in VS Code/Jupyter\n",
    "# MUST come before importing matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "# Standard scientific computing imports\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from rheojax.models import Maxwell\n",
    "\n",
    "from rheojax.core.data import RheoData\n",
    "from rheojax.core.jax_config import safe_import_jax, verify_float64\n",
    "\n",
    "# Rheo imports - always explicit\n",
    "from rheojax.pipeline.base import Pipeline\n",
    "from rheojax.pipeline.bayesian import BayesianPipeline\n",
    "\n",
    "# Safe JAX import - REQUIRED for all notebooks using JAX\n",
    "# This pattern ensures float64 precision enforcement throughout\n",
    "jax, jnp = safe_import_jax()\n",
    "\n",
    "# Verify float64 is enabled (educational demonstration)\n",
    "verify_float64()\n",
    "print(f\"✓ JAX float64 precision enabled (default dtype bits: {jax.config.jax_default_dtype_bits})\")\n",
    "\n",
    "# Set reproducible random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib for publication-quality plots\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "maxwell-theory",
   "metadata": {},
   "source": [
    "## Maxwell Model Theory\n",
    "\n",
    "The Maxwell model represents a viscoelastic material as a spring and dashpot in series:\n",
    "\n",
    "$$G(t) = G_0 \\exp\\left(-\\frac{t}{\\tau}\\right)$$\n",
    "\n",
    "where:\n",
    "- $G(t)$ = relaxation modulus (Pa)\n",
    "- $G_0$ = initial modulus (Pa) - represents elastic response\n",
    "- $\\tau = \\eta / G_0$ = relaxation time (s) - characterizes viscous response\n",
    "- $\\eta$ = viscosity (Pa·s)\n",
    "\n",
    "**Physical Interpretation:**\n",
    "- **$G_0$**: Instantaneous elastic modulus - material stiffness at $t=0$\n",
    "- **$\\eta$**: Viscosity - resistance to flow\n",
    "- **$\\tau$**: Time scale for stress relaxation - larger $\\tau$ means slower relaxation\n",
    "\n",
    "**Applicability:**\n",
    "- Simple polymer melts and solutions\n",
    "- Materials with single dominant relaxation time\n",
    "- Limited to small strains (linear viscoelastic regime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-intro",
   "metadata": {},
   "source": [
    "## Generate Synthetic Relaxation Data\n",
    "\n",
    "We create synthetic stress relaxation data with known parameters to validate our fitting workflow. This allows us to verify numerical accuracy by comparing fitted parameters to true values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# True Maxwell parameters\n",
    "G0_true = 1e5  # Pa\n",
    "eta_true = 1e3  # Pa·s\n",
    "tau_true = eta_true / G0_true  # s\n",
    "\n",
    "print(f\"True Parameters:\")\n",
    "print(f\"  G0  = {G0_true:.2e} Pa\")\n",
    "print(f\"  eta = {eta_true:.2e} Pa·s\")\n",
    "print(f\"  tau = {tau_true:.4f} s\")\n",
    "\n",
    "# Generate time array (logarithmically spaced for relaxation)\n",
    "t = np.logspace(-2, 2, 50)  # 0.01 to 100 seconds\n",
    "\n",
    "# True relaxation modulus\n",
    "G_t_true = G0_true * np.exp(-t / tau_true)\n",
    "\n",
    "# Add realistic Gaussian noise (1-2% relative error)\n",
    "noise_level = 0.015  # 1.5%\n",
    "noise = np.random.normal(0, noise_level * G_t_true)\n",
    "G_t_noisy = G_t_true + noise\n",
    "\n",
    "print(f\"\\nData characteristics:\")\n",
    "print(f\"  Time range: {t.min():.2f} - {t.max():.2f} s\")\n",
    "print(f\"  Number of points: {len(t)}\")\n",
    "print(f\"  Noise level: {noise_level*100:.1f}% relative\")\n",
    "print(f\"  SNR: {np.mean(G_t_true) / np.std(noise):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize raw data\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "plt.loglog(t, G_t_noisy, 'o', markersize=6, alpha=0.7, label='Synthetic Data (with noise)')\n",
    "plt.loglog(t, G_t_true, '--', linewidth=2, alpha=0.5, label='True Maxwell Response')\n",
    "plt.xlabel('Time (s)', fontsize=12)\n",
    "plt.ylabel('Relaxation Modulus G(t) (Pa)', fontsize=12)\n",
    "plt.title('Stress Relaxation Data', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3, which='both')\n",
    "plt.legend(fontsize=11)\n",
    "plt.tight_layout()\n",
    "display(fig)\n",
    "plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pipeline-intro",
   "metadata": {},
   "source": [
    "## Approach 1: Pipeline API (Recommended for Standard Workflows)\n",
    "\n",
    "The **Pipeline API** provides a fluent interface for common analysis tasks. It's ideal for rapid prototyping and standardized workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pipeline-fit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RheoData container with metadata\n",
    "data = RheoData(\n",
    "    x=t,\n",
    "    y=G_t_noisy,\n",
    "    x_units='s',\n",
    "    y_units='Pa',\n",
    "    domain='time',\n",
    ")\n",
    "\n",
    "# Pipeline API workflow with timing\n",
    "start_pipeline = time.time()\n",
    "\n",
    "pipeline = Pipeline(data)\n",
    "pipeline.fit('maxwell')\n",
    "\n",
    "pipeline_time = time.time() - start_pipeline\n",
    "\n",
    "# Extract fitted parameters\n",
    "model = pipeline.get_last_model()\n",
    "G0_pipeline = model.parameters.get_value('G0')\n",
    "eta_pipeline = model.parameters.get_value('eta')\n",
    "tau_pipeline = eta_pipeline / G0_pipeline\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PIPELINE API RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Fitted Parameters:\")\n",
    "print(f\"  G0  = {G0_pipeline:.4e} Pa  (true: {G0_true:.4e})\")\n",
    "print(f\"  eta = {eta_pipeline:.4e} Pa·s  (true: {eta_true:.4e})\")\n",
    "print(f\"  tau = {tau_pipeline:.6f} s  (true: {tau_true:.6f})\")\n",
    "print(f\"\\nRelative Errors:\")\n",
    "print(f\"  G0:  {abs(G0_pipeline - G0_true) / G0_true * 100:.4f}%\")\n",
    "print(f\"  eta: {abs(eta_pipeline - eta_true) / eta_true * 100:.4f}%\")\n",
    "print(f\"\\nOptimization time: {pipeline_time:.4f} s\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modular-intro",
   "metadata": {},
   "source": [
    "## Approach 2: Modular API (Recommended for Customization)\n",
    "\n",
    "The **Modular API** provides direct access to model classes with scikit-learn compatible interface. Use this when you need fine control over parameters, bounds, or optimization settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modular-fit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Maxwell model instance\n",
    "model = Maxwell()\n",
    "\n",
    "# Set parameter bounds (optional but recommended)\n",
    "model.parameters.set_bounds('G0', (1e3, 1e7))  # Reasonable modulus range\n",
    "model.parameters.set_bounds('eta', (1e1, 1e5))  # Reasonable viscosity range\n",
    "\n",
    "# Fit with timing\n",
    "start_modular = time.time()\n",
    "\n",
    "model.fit(t, G_t_noisy)\n",
    "\n",
    "modular_time = time.time() - start_modular\n",
    "\n",
    "# Extract fitted parameters\n",
    "G0_modular = model.parameters.get_value('G0')\n",
    "eta_modular = model.parameters.get_value('eta')\n",
    "tau_modular = eta_modular / G0_modular\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODULAR API RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Fitted Parameters:\")\n",
    "print(f\"  G0  = {G0_modular:.4e} Pa  (true: {G0_true:.4e})\")\n",
    "print(f\"  eta = {eta_modular:.4e} Pa·s  (true: {eta_true:.4e})\")\n",
    "print(f\"  tau = {tau_modular:.6f} s  (true: {tau_true:.6f})\")\n",
    "print(f\"\\nRelative Errors:\")\n",
    "print(f\"  G0:  {abs(G0_modular - G0_true) / G0_true * 100:.4f}%\")\n",
    "print(f\"  eta: {abs(eta_modular - eta_true) / eta_true * 100:.4f}%\")\n",
    "print(f\"\\nOptimization time: {modular_time:.4f} s\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Verify both approaches give same results\n",
    "assert np.allclose(G0_pipeline, G0_modular, rtol=1e-2), \"Pipeline and Modular should give identical results\"\n",
    "assert np.allclose(eta_pipeline, eta_modular, rtol=1e-2), \"Pipeline and Modular should give identical results\"\n",
    "print(\"\\n✓ Pipeline and Modular APIs produce identical results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "performance-intro",
   "metadata": {},
   "source": [
    "## Performance Benchmark: NLSQ vs SciPy\n",
    "\n",
    "Rheo uses **NLSQ** (GPU-accelerated nonlinear least squares) as the default optimization backend, providing 5-270x speedup over SciPy's `curve_fit`.\n",
    "\n",
    "The speedup comes from:\n",
    "1. **JAX JIT compilation** - compiles optimization to optimized XLA code\n",
    "2. **Automatic differentiation** - exact gradients without numerical approximation\n",
    "3. **GPU acceleration** - parallel computation on CUDA devices (if available)\n",
    "\n",
    "Let's measure actual performance on your hardware:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "benchmark-timing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark: Multiple fits to get reliable timing\n",
    "n_runs = 10\n",
    "times = []\n",
    "\n",
    "for i in range(n_runs):\n",
    "    model_bench = Maxwell()\n",
    "    start = time.time()\n",
    "    model_bench.fit(t, G_t_noisy)\n",
    "    times.append(time.time() - start)\n",
    "\n",
    "nlsq_mean = np.mean(times[1:])  # Exclude first run (JIT compilation)\n",
    "nlsq_std = np.std(times[1:])\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERFORMANCE BENCHMARK (NLSQ)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Number of runs: {n_runs}\")\n",
    "print(f\"First run (with JIT): {times[0]:.4f} s\")\n",
    "print(f\"Subsequent runs: {nlsq_mean:.4f} ± {nlsq_std:.4f} s\")\n",
    "print(f\"JIT overhead: {times[0] - nlsq_mean:.4f} s\")\n",
    "print(f\"\\nNOTE: SciPy curve_fit typically takes 0.05-0.5s for this problem\")\n",
    "print(f\"Expected speedup: 5-270x depending on problem size and GPU\")\n",
    "print(f\"For this small dataset ({len(t)} points), speedup may be modest.\")\n",
    "print(f\"Speedup increases dramatically with dataset size (>1000 points).\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualization-intro",
   "metadata": {},
   "source": [
    "## Results Visualization\n",
    "\n",
    "We create publication-quality visualizations showing:\n",
    "1. **Fit quality** - data vs model prediction\n",
    "2. **Residual analysis** - systematic errors or outliers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-fit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "G_t_pred = model.predict(t)\n",
    "residuals = G_t_noisy - G_t_pred\n",
    "relative_residuals = residuals / G_t_noisy * 100\n",
    "\n",
    "# Create figure with subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Left: Fit quality\n",
    "ax1.loglog(t, G_t_noisy, 'o', markersize=6, alpha=0.7, label='Data', color='#1f77b4')\n",
    "ax1.loglog(t, G_t_true, '--', linewidth=2, alpha=0.4, label='True', color='gray')\n",
    "ax1.loglog(t, G_t_pred, '-', linewidth=2.5, label='Fitted', color='#ff7f0e')\n",
    "ax1.set_xlabel('Time (s)', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Relaxation Modulus G(t) (Pa)', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Maxwell Model Fit', fontsize=13, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3, which='both')\n",
    "ax1.legend(fontsize=11, framealpha=0.9)\n",
    "\n",
    "# Right: Residual analysis\n",
    "ax2.semilogx(t, relative_residuals, 'o', markersize=6, alpha=0.7, color='#2ca02c')\n",
    "ax2.axhline(0, color='black', linestyle='--', linewidth=1, alpha=0.5)\n",
    "ax2.axhline(noise_level * 100, color='red', linestyle=':', linewidth=1.5, alpha=0.5, label=f'Expected noise: ±{noise_level*100:.1f}%')\n",
    "ax2.axhline(-noise_level * 100, color='red', linestyle=':', linewidth=1.5, alpha=0.5)\n",
    "ax2.set_xlabel('Time (s)', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Relative Residuals (%)', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Residual Analysis', fontsize=13, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend(fontsize=10, framealpha=0.9)\n",
    "\n",
    "plt.tight_layout()\n",
    "display(fig)\n",
    "plt.close(fig)\n",
    "\n",
    "# Compute fit quality metrics\n",
    "ss_res = np.sum(residuals**2)\n",
    "ss_tot = np.sum((G_t_noisy - np.mean(G_t_noisy))**2)\n",
    "r_squared = 1 - (ss_res / ss_tot)\n",
    "rmse = np.sqrt(np.mean(residuals**2))\n",
    "\n",
    "print(\"\\nFit Quality Metrics:\")\n",
    "print(f\"  R² = {r_squared:.6f}\")\n",
    "print(f\"  RMSE = {rmse:.2e} Pa\")\n",
    "print(f\"  Mean |residual| = {np.mean(np.abs(residuals)):.2e} Pa ({np.mean(np.abs(relative_residuals)):.2f}%)\")\n",
    "print(f\"  Max |residual| = {np.max(np.abs(residuals)):.2e} Pa ({np.max(np.abs(relative_residuals)):.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bayesian-intro",
   "metadata": {},
   "source": [
    "## Bayesian Inference: Uncertainty Quantification\n",
    "\n",
    "While NLSQ provides fast point estimates, **Bayesian inference** quantifies parameter uncertainty through posterior distributions. This is essential when:\n",
    "- Parameters are poorly constrained by data\n",
    "- Understanding parameter correlations is important\n",
    "- Propagating uncertainty to predictions is needed\n",
    "- Comparing competing models statistically\n",
    "\n",
    "### Two-Stage Workflow: NLSQ → NUTS\n",
    "\n",
    "1. **NLSQ optimization** (fast) - find approximate maximum likelihood parameters\n",
    "2. **NUTS sampling** (slower) - warm-start from NLSQ for 2-5x faster convergence\n",
    "\n",
    "This warm-start strategy dramatically reduces:\n",
    "- Number of iterations to convergence\n",
    "- Divergent transitions (MCMC failures)\n",
    "- Total computational time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bayesian-fit",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BAYESIAN INFERENCE WITH WARM-START\")\n",
    "print(\"=\"*60)\n",
    "print(\"Running MCMC sampling... (this may take 1-2 minutes)\\n\")\n",
    "\n",
    "# Bayesian inference using warm-start from NLSQ\n",
    "bayesian_start = time.time()\n",
    "\n",
    "result = model.fit_bayesian(\n",
    "    t, G_t_noisy,\n",
    "    num_warmup=1000,   # Burn-in iterations\n",
    "    num_samples=2000,  # Posterior samples\n",
    "    num_chains=1,      # Single chain (faster for demo); use num_chains=4 for production\n",
    "    initial_values={   # Warm-start from NLSQ\n",
    "        'G0': model.parameters.get_value('G0'),\n",
    "        'eta': model.parameters.get_value('eta')\n",
    "    }\n",
    ")\n",
    "\n",
    "bayesian_time = time.time() - bayesian_start\n",
    "\n",
    "print(f\"\\nBayesian inference completed in {bayesian_time:.2f} s\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bayesian-results",
   "metadata": {},
   "source": [
    "### Posterior Summary and Convergence Diagnostics\n",
    "\n",
    "Key metrics for MCMC quality:\n",
    "- **R-hat < 1.01**: Chains have converged (all parameters must meet this)\n",
    "- **ESS > 400**: Effective sample size ensures reliable estimates\n",
    "- **Divergences < 1%**: NUTS sampler is well-behaved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bayesian-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract posterior samples and diagnostics\n",
    "posterior = result.posterior_samples\n",
    "diagnostics = result.diagnostics\n",
    "summary = result.summary\n",
    "\n",
    "# Get credible intervals\n",
    "credible_intervals = model.get_credible_intervals(posterior, credibility=0.95)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"POSTERIOR SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nParameter Estimates (posterior mean ± std):\")\n",
    "print(f\"  G0  = {summary['G0']['mean']:.4e} ± {summary['G0']['std']:.4e} Pa\")\n",
    "print(f\"  eta = {summary['eta']['mean']:.4e} ± {summary['eta']['std']:.4e} Pa·s\")\n",
    "\n",
    "print(f\"\\n95% Credible Intervals:\")\n",
    "print(f\"  G0:  [{credible_intervals['G0'][0]:.4e}, {credible_intervals['G0'][1]:.4e}] Pa\")\n",
    "print(f\"  eta: [{credible_intervals['eta'][0]:.4e}, {credible_intervals['eta'][1]:.4e}] Pa·s\")\n",
    "\n",
    "print(f\"\\nConvergence Diagnostics:\")\n",
    "print(f\"  R-hat (G0):  {diagnostics['r_hat']['G0']:.4f}  {'✓' if diagnostics['r_hat']['G0'] < 1.01 else '✗ POOR'}\")\n",
    "print(f\"  R-hat (eta): {diagnostics['r_hat']['eta']:.4f}  {'✓' if diagnostics['r_hat']['eta'] < 1.01 else '✗ POOR'}\")\n",
    "print(f\"  ESS (G0):    {diagnostics['ess']['G0']:.0f}  {'✓' if diagnostics['ess']['G0'] > 400 else '✗ LOW'}\")\n",
    "print(f\"  ESS (eta):   {diagnostics['ess']['eta']:.0f}  {'✓' if diagnostics['ess']['eta'] > 400 else '✗ LOW'}\")\n",
    "\n",
    "if 'num_divergences' in diagnostics:\n",
    "    div_rate = diagnostics['num_divergences'] / result.num_samples * 100\n",
    "    print(f\"  Divergences: {diagnostics['num_divergences']} ({div_rate:.2f}%)  {'✓' if div_rate < 1 else '✗ HIGH'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Check convergence\n",
    "converged = all([\n",
    "    diagnostics['r_hat']['G0'] < 1.01,\n",
    "    diagnostics['r_hat']['eta'] < 1.01,\n",
    "    diagnostics['ess']['G0'] > 400,\n",
    "    diagnostics['ess']['eta'] > 400\n",
    "])\n",
    "\n",
    "if converged:\n",
    "    print(\"\\n✓ EXCELLENT CONVERGENCE - All diagnostic criteria met!\")\n",
    "else:\n",
    "    print(\"\\n⚠ WARNING: Convergence criteria not met. Increase num_warmup or num_samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arviz-intro",
   "metadata": {},
   "source": [
    "## ArviZ Diagnostic Plots: Comprehensive MCMC Quality Assessment\n",
    "\n",
    "ArviZ provides 6 essential diagnostic plots to assess MCMC quality. Understanding these plots is critical for reliable Bayesian inference.\n",
    "\n",
    "### Plot 1: Trace Plot - Visualize MCMC Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arviz-trace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "\n",
    "# Convert to ArviZ InferenceData for plotting\n",
    "idata = result.to_inference_data()\n",
    "\n",
    "# Trace plot: visualize sampling\n",
    "az.plot_trace(idata, figsize=(12, 6))\n",
    "plt.tight_layout()\n",
    "fig = plt.gcf()  # Get current figure from ArviZ\n",
    "display(fig)\n",
    "plt.close(fig)\n",
    "\n",
    "print(\"\"\"\n",
    "INTERPRETATION - Trace Plot:\n",
    "- LEFT: Posterior marginal distributions (should be smooth, unimodal)\n",
    "- RIGHT: Parameter values vs iteration (should look like \"fuzzy caterpillar\")\n",
    "- GOOD: Stationary mean, no trends, no stuck regions\n",
    "- BAD: Drift, discontinuities, bimodal distributions\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arviz-pair",
   "metadata": {},
   "source": [
    "### Plot 2: Pair Plot - Parameter Correlations and Divergences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-pair",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair plot: parameter correlations\n",
    "az.plot_pair(\n",
    "    idata,\n",
    "    var_names=['G0', 'eta'],\n",
    "    kind='scatter',\n",
    "    divergences=True,  # Highlight problematic samples\n",
    "    figsize=(10, 8)\n",
    ")\n",
    "plt.tight_layout()\n",
    "fig = plt.gcf()  # Get current figure from ArviZ\n",
    "display(fig)\n",
    "plt.close(fig)\n",
    "\n",
    "print(\"\"\"\n",
    "INTERPRETATION - Pair Plot:\n",
    "- DIAGONAL: Marginal posterior distributions\n",
    "- OFF-DIAGONAL: Joint distributions (parameter correlations)\n",
    "- RED POINTS: Divergent transitions (MCMC failures)\n",
    "\n",
    "What to look for:\n",
    "✓ GOOD: Elliptical joint distribution, few/no divergences\n",
    "✗ BAD: Funnel geometry, strong correlations, many divergences\n",
    "\n",
    "For Maxwell model:\n",
    "- G0 and eta should be weakly correlated (relaxation time τ = eta/G0)\n",
    "- Strong correlations indicate parameter non-identifiability\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arviz-forest",
   "metadata": {},
   "source": [
    "### Plot 3: Forest Plot - Credible Interval Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-forest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forest plot: credible intervals\n",
    "az.plot_forest(\n",
    "    idata,\n",
    "    var_names=['G0', 'eta'],\n",
    "    hdi_prob=0.95,  # 95% highest density interval\n",
    "    combined=True,\n",
    "    figsize=(10, 4)\n",
    ")\n",
    "plt.tight_layout()\n",
    "fig = plt.gcf()  # Get current figure from ArviZ\n",
    "display(fig)\n",
    "plt.close(fig)\n",
    "\n",
    "print(\"\"\"\n",
    "INTERPRETATION - Forest Plot:\n",
    "- THICK LINE: 95% credible interval (95% probability parameter in this range)\n",
    "- THIN LINE: Full posterior range\n",
    "- DOT: Posterior mean\n",
    "\n",
    "What to look for:\n",
    "✓ GOOD: Narrow credible intervals (well-constrained parameters)\n",
    "✗ BAD: Very wide intervals (poorly constrained, need more data or tighter priors)\n",
    "\n",
    "Compare:\n",
    "- Relative uncertainty: σ/μ for each parameter\n",
    "- Parameter magnitudes: Are scales appropriate?\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arviz-autocorr",
   "metadata": {},
   "source": [
    "### Plot 4: Autocorrelation Plot - Mixing Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-autocorr",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autocorrelation plot: mixing diagnostic\n",
    "az.plot_autocorr(\n",
    "    idata,\n",
    "    var_names=['G0', 'eta'],\n",
    "    max_lag=100,\n",
    "    figsize=(12, 4)\n",
    ")\n",
    "plt.tight_layout()\n",
    "fig = plt.gcf()  # Get current figure from ArviZ\n",
    "display(fig)\n",
    "plt.close(fig)\n",
    "\n",
    "print(\"\"\"\n",
    "INTERPRETATION - Autocorrelation Plot:\n",
    "- Y-axis: Correlation between samples at different lags\n",
    "- X-axis: Lag (number of iterations)\n",
    "\n",
    "What to look for:\n",
    "✓ GOOD: Autocorrelation drops to ~0 within few dozen lags\n",
    "✗ BAD: Slow decay (high autocorrelation) → poor mixing\n",
    "\n",
    "If autocorrelation is high:\n",
    "- Increase num_samples to get more effective samples\n",
    "- Check for parameter correlations (use pair plot)\n",
    "- Consider reparameterization if persistent\n",
    "\n",
    "Relation to ESS:\n",
    "- High autocorrelation → low ESS (fewer independent samples)\n",
    "- ESS = num_samples / (1 + 2*Σ autocorrelations)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arviz-rank",
   "metadata": {},
   "source": [
    "### Plot 5: Rank Plot - Convergence Diagnostic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-rank",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank plot: modern convergence diagnostic\n",
    "az.plot_rank(\n",
    "    idata,\n",
    "    var_names=['G0', 'eta'],\n",
    "    figsize=(12, 4)\n",
    ")\n",
    "plt.tight_layout()\n",
    "fig = plt.gcf()  # Get current figure from ArviZ\n",
    "display(fig)\n",
    "plt.close(fig)\n",
    "\n",
    "print(\"\"\"\n",
    "INTERPRETATION - Rank Plot:\n",
    "- Histogram of ranked parameter values across chains\n",
    "- Modern alternative to trace plots for convergence\n",
    "\n",
    "What to look for:\n",
    "✓ GOOD: Uniform histogram (flat, all bins similar height)\n",
    "✗ BAD: Non-uniform (peaks, valleys, trends)\n",
    "\n",
    "Non-uniform patterns indicate:\n",
    "- Chains exploring different regions (not converged)\n",
    "- Chain sticking or slow mixing\n",
    "- Need more warmup iterations\n",
    "\n",
    "This is the MOST SENSITIVE convergence diagnostic:\n",
    "- More reliable than R-hat for detecting subtle issues\n",
    "- Should always check even if R-hat < 1.01\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arviz-ess",
   "metadata": {},
   "source": [
    "### Plot 6: ESS Plot - Effective Sample Size\n",
    "\n",
    "**Note:** ESS plot requires multiple chains for meaningful results. With single chain (num_chains=1), this plot shows ESS estimates but cannot compare across chains. For production work, use num_chains=4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-ess",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESS plot: effective sample size\n",
    "try:\n",
    "    az.plot_ess(\n",
    "        idata,\n",
    "        var_names=['G0', 'eta'],\n",
    "        kind='local',  # 'local', 'quantile', or 'evolution'\n",
    "        figsize=(12, 4)\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    fig = plt.gcf()  # Get current figure from ArviZ\n",
    "    display(fig)\n",
    "    plt.close(fig)\n",
    "except Exception as e:\n",
    "    print(f\"Note: ESS plot requires multiple chains for full functionality.\")\n",
    "    print(f\"Current setup: {idata.posterior.chain.size} chain(s)\")\n",
    "    print(f\"For production, use num_chains=4.\\n\")\n",
    "    \n",
    "    # Show ESS values instead\n",
    "    print(f\"Effective Sample Size (ESS):\")\n",
    "    print(f\"  G0:  {diagnostics['ess']['G0']:.0f} / {result.num_samples} samples ({diagnostics['ess']['G0']/result.num_samples*100:.1f}%)\")\n",
    "    print(f\"  eta: {diagnostics['ess']['eta']:.0f} / {result.num_samples} samples ({diagnostics['ess']['eta']/result.num_samples*100:.1f}%)\")\n",
    "\n",
    "print(\"\"\"\n",
    "INTERPRETATION - ESS Plot:\n",
    "- Quantifies sampling efficiency per parameter\n",
    "- ESS = number of \"independent\" samples (accounting for autocorrelation)\n",
    "\n",
    "What to look for:\n",
    "✓ GOOD: ESS > 400 (bulk and tail) for all parameters\n",
    "✗ BAD: Low ESS → need more samples or better mixing\n",
    "\n",
    "ESS types:\n",
    "- BULK: Central posterior region (mean, median estimates)\n",
    "- TAIL: Extreme quantiles (credible interval estimates)\n",
    "- LOCAL: ESS at different quantiles\n",
    "\n",
    "If ESS is low:\n",
    "1. Increase num_samples (more iterations)\n",
    "2. Check autocorrelation plot (poor mixing?)\n",
    "3. Use multiple chains (num_chains=4) for better estimates\n",
    "4. Warm-start from NLSQ (already doing this!)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diagnostic-summary",
   "metadata": {},
   "source": [
    "## Diagnostic Plot Summary\n",
    "\n",
    "### Quick Reference Guide\n",
    "\n",
    "| Plot | Purpose | Good Result | Bad Result | Fix |\n",
    "|------|---------|-------------|------------|-----|\n",
    "| **Trace** | Visualize sampling | Fuzzy caterpillar | Trends, jumps | Increase warmup |\n",
    "| **Pair** | Correlations, divergences | Elliptical, no divergences | Funnel, many divergences | Better priors, reparameterize |\n",
    "| **Forest** | Credible intervals | Narrow intervals | Very wide | More data, tighter priors |\n",
    "| **Autocorr** | Mixing quality | Fast decay to 0 | Slow decay | More samples |\n",
    "| **Rank** | Convergence | Uniform histogram | Non-uniform | More warmup |\n",
    "| **ESS** | Sample efficiency | ESS > 400 | ESS < 100 | More samples, check mixing |\n",
    "\n",
    "### Troubleshooting Workflow\n",
    "\n",
    "If you encounter convergence issues:\n",
    "\n",
    "1. **Check R-hat and ESS first** (numerical diagnostics)\n",
    "2. **Rank plot** - most sensitive convergence check\n",
    "3. **Trace plot** - visual inspection of chains\n",
    "4. **Autocorr plot** - if ESS is low, check mixing\n",
    "5. **Pair plot** - if divergences present, check geometry\n",
    "6. **Forest plot** - assess parameter uncertainty\n",
    "\n",
    "### Common Issues and Solutions\n",
    "\n",
    "**High R-hat (> 1.01):**\n",
    "- Increase `num_warmup` (try 2000 or 5000)\n",
    "- Use multiple chains (`num_chains=4`)\n",
    "- Ensure warm-start from NLSQ\n",
    "\n",
    "**Low ESS (< 400):**\n",
    "- Increase `num_samples` (try 5000 or 10000)\n",
    "- Check autocorrelation plot - if high, mixing is poor\n",
    "- Use warm-start to improve initial proposal\n",
    "\n",
    "**Many divergences (> 1%):**\n",
    "- **Critical issue** - results unreliable!\n",
    "- Warm-start from NLSQ (reduces divergences 10-100x)\n",
    "- Tighter parameter bounds/priors\n",
    "- Reparameterize model (e.g., log-transform parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "physical-interpretation",
   "metadata": {},
   "source": [
    "## Physical Interpretation\n",
    "\n",
    "Let's interpret the fitted parameters in the context of material behavior:\n",
    "\n",
    "### Parameter Meanings\n",
    "\n",
    "**Initial Modulus (G₀):**\n",
    "- Represents instantaneous elastic response\n",
    "- For our fit: ~1×10⁵ Pa (100 kPa)\n",
    "- Physical meaning: Material stiffness at t=0\n",
    "- Typical range: 10² - 10⁹ Pa depending on material\n",
    "\n",
    "**Viscosity (η):**\n",
    "- Represents resistance to flow\n",
    "- For our fit: ~1×10³ Pa·s\n",
    "- Physical meaning: Controls stress relaxation rate\n",
    "- Typical range: 10⁻² - 10⁶ Pa·s\n",
    "\n",
    "**Relaxation Time (τ = η/G₀):**\n",
    "- Time scale for stress decay to 1/e (~37%) of initial value\n",
    "- For our fit: ~0.01 s\n",
    "- Physical meaning: Fast relaxation → fluid-like behavior\n",
    "- Slow relaxation → solid-like behavior\n",
    "\n",
    "### Material Classification\n",
    "\n",
    "Based on relaxation time:\n",
    "- **τ < 0.1 s**: Predominantly viscous (fluid-like)\n",
    "- **0.1 < τ < 100 s**: Viscoelastic (mixed behavior)\n",
    "- **τ > 100 s**: Predominantly elastic (solid-like)\n",
    "\n",
    "Our material (τ ≈ 0.01 s) exhibits **fluid-like behavior** with rapid stress relaxation.\n",
    "\n",
    "### Model Limitations\n",
    "\n",
    "The Maxwell model is valid when:\n",
    "- ✓ Small strains (linear viscoelastic regime, typically < 10%)\n",
    "- ✓ Single dominant relaxation time\n",
    "- ✓ Isothermal conditions\n",
    "\n",
    "Consider alternative models if:\n",
    "- ✗ Multiple relaxation times needed → Generalized Maxwell\n",
    "- ✗ Non-exponential decay → Fractional Maxwell\n",
    "- ✗ Finite equilibrium modulus → Zener (Standard Linear Solid)\n",
    "- ✗ Large strain behavior → Nonlinear models (Gent, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parameter-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table of results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL PARAMETER SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n{'Method':<20} {'G0 (Pa)':<15} {'eta (Pa·s)':<15} {'tau (s)':<10}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'True Values':<20} {G0_true:<15.4e} {eta_true:<15.4e} {tau_true:<10.6f}\")\n",
    "print(f\"{'NLSQ (Point)':<20} {G0_modular:<15.4e} {eta_modular:<15.4e} {tau_modular:<10.6f}\")\n",
    "print(f\"{'Bayesian (Mean)':<20} {summary['G0']['mean']:<15.4e} {summary['eta']['mean']:<15.4e} {summary['eta']['mean']/summary['G0']['mean']:<10.6f}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Uncertainty from Bayesian inference\n",
    "print(f\"\\n{'Bayesian Uncertainty (1σ):':<20} {summary['G0']['std']:<15.4e} {summary['eta']['std']:<15.4e}\")\n",
    "print(f\"{'Relative Uncertainty:':<20} {summary['G0']['std']/summary['G0']['mean']*100:<15.2f}% {summary['eta']['std']/summary['eta']['mean']*100:<15.2f}%\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "print(f\"\\nPhysical Interpretation:\")\n",
    "print(f\"  Material Type: {'Fluid-like (fast relaxation)' if tau_modular < 0.1 else 'Viscoelastic' if tau_modular < 100 else 'Solid-like'}\")\n",
    "print(f\"  Relaxation Time: {tau_modular:.6f} s (time to decay to 37% of initial stress)\")\n",
    "print(f\"  Initial Stiffness: {G0_modular:.2e} Pa ({G0_modular/1e3:.1f} kPa)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "key-takeaways",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### Main Concepts\n",
    "\n",
    "1. **Two API Approaches:**\n",
    "   - **Pipeline API**: Fluent interface for rapid workflows (`Pipeline().load().fit().plot()`)\n",
    "   - **Modular API**: Direct model control for customization (`Maxwell().fit()`)\n",
    "   - Both produce identical numerical results\n",
    "\n",
    "2. **NLSQ Optimization:**\n",
    "   - Default backend provides 5-270x speedup vs SciPy\n",
    "   - JAX JIT compilation + automatic differentiation\n",
    "   - GPU acceleration available (additional 10-100x for large datasets)\n",
    "   - Float64 precision enforced via safe_import_jax()\n",
    "\n",
    "3. **Bayesian Uncertainty Quantification:**\n",
    "   - Two-stage workflow: NLSQ (fast) → NUTS (warm-start)\n",
    "   - Warm-start reduces convergence time 2-5x\n",
    "   - Provides credible intervals and parameter correlations\n",
    "   - Essential for poorly-constrained parameters\n",
    "\n",
    "4. **ArviZ Diagnostic Suite:**\n",
    "   - **6 essential plots** assess MCMC quality comprehensively\n",
    "   - Must check: R-hat < 1.01, ESS > 400, divergences < 1%\n",
    "   - Rank plot is most sensitive convergence diagnostic\n",
    "   - Pair plot reveals parameter correlations and divergences\n",
    "\n",
    "### When to Use Maxwell Model\n",
    "\n",
    "**Appropriate for:**\n",
    "- ✓ Polymer melts and solutions with single relaxation time\n",
    "- ✓ Small strain linear viscoelastic regime\n",
    "- ✓ Rapid screening of relaxation behavior\n",
    "- ✓ Materials with predominantly viscous character\n",
    "\n",
    "**Consider alternatives for:**\n",
    "- ✗ Multiple relaxation times → Generalized Maxwell (Prony series)\n",
    "- ✗ Non-exponential decay → Fractional Maxwell\n",
    "- ✗ Finite equilibrium modulus → Zener model\n",
    "- ✗ Solid-like materials → Kelvin-Voigt or Burgers model\n",
    "\n",
    "### Common Pitfalls\n",
    "\n",
    "1. **Float64 Precision:**\n",
    "   - Always use `safe_import_jax()` pattern\n",
    "   - NLSQ requires float64 for numerical stability\n",
    "   - Automatic via package but verify in custom code\n",
    "\n",
    "2. **Bayesian Convergence:**\n",
    "   - **Never skip diagnostic checks** - R-hat, ESS, divergences\n",
    "   - Use warm-start from NLSQ (essential for complex models)\n",
    "   - Multiple chains (num_chains=4) recommended for production\n",
    "\n",
    "3. **Model Selection:**\n",
    "   - Check residuals for systematic trends\n",
    "   - Maxwell often insufficient for real materials\n",
    "   - Use Bayesian model comparison (WAIC, LOO) to compare alternatives\n",
    "\n",
    "4. **Parameter Bounds:**\n",
    "   - Set physically reasonable bounds in ParameterSet\n",
    "   - Prevents optimization from exploring unphysical regions\n",
    "   - Transforms to priors in Bayesian inference\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "### Explore Related Models\n",
    "- **[02-zener-fitting.ipynb](02-zener-fitting.ipynb)**: Standard Linear Solid with finite equilibrium modulus\n",
    "- **[03-springpot-fitting.ipynb](03-springpot-fitting.ipynb)**: Fractional element for power-law relaxation\n",
    "- **Advanced fractional models**: See `advanced/04-fractional-models-deep-dive.ipynb`\n",
    "\n",
    "### Deepen Bayesian Understanding\n",
    "- **[bayesian/01-bayesian-basics.ipynb](../bayesian/01-bayesian-basics.ipynb)**: Comprehensive NLSQ→NUTS workflow\n",
    "- **[bayesian/03-convergence-diagnostics.ipynb](../bayesian/03-convergence-diagnostics.ipynb)**: Deep dive into all 6 ArviZ plots\n",
    "\n",
    "### Advanced Workflows\n",
    "- **[transforms/02-mastercurve-generation.ipynb](../transforms/02-mastercurve-generation.ipynb)**: Time-temperature superposition\n",
    "- **[advanced/01-multi-technique-fitting.ipynb](../advanced/01-multi-technique-fitting.ipynb)**: Constrained fitting across test modes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "session-info",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Session Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "session-info-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print session information for reproducibility\n",
    "import sys\n",
    "\n",
    "import rheojax\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Rheo version: {rheojax.__version__}\")\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "print(f\"JAX devices: {jax.devices()}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"ArviZ version: {az.__version__}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rheojax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
