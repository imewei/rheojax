{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# Maxwell Model: Stress Relaxation Fitting\n",
    "\n",
    "This notebook demonstrates the complete workflow for fitting the Maxwell model to stress relaxation data, showcasing modern Rheo capabilities including GPU-accelerated optimization and Bayesian uncertainty quantification.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "After completing this notebook, you will be able to:\n",
    "- Fit the Maxwell model using both Pipeline API and Modular API approaches\n",
    "- Leverage NLSQ optimization for 5-270x speedup over SciPy\n",
    "- Perform Bayesian inference with NLSQ\u2192NUTS warm-start workflow\n",
    "- Interpret all 6 ArviZ diagnostic plots for MCMC convergence\n",
    "- Extract physically meaningful parameters with uncertainty quantification\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Basic understanding of:\n",
    "- Rheological concepts (stress, strain, relaxation)\n",
    "- Linear viscoelasticity\n",
    "- Python and NumPy\n",
    "\n",
    "**Estimated Time:** 30-40 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-intro",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "We start by importing necessary libraries. Note the **safe JAX import pattern** - this is critical for ensuring float64 precision throughout the entire JAX stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:2025-11-01 21:04:15,999:jax._src.xla_bridge:808: Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: dlopen(libtpu.so, 0x0001): tried: 'libtpu.so' (no such file), '/System/Volumes/Preboot/Cryptexes/OSlibtpu.so' (no such file), '/usr/lib/libtpu.so' (no such file, not in dyld cache), 'libtpu.so' (no such file)\n",
      "Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: dlopen(libtpu.so, 0x0001): tried: 'libtpu.so' (no such file), '/System/Volumes/Preboot/Cryptexes/OSlibtpu.so' (no such file), '/usr/lib/libtpu.so' (no such file, not in dyld cache), 'libtpu.so' (no such file)\n",
      "Loading rheojax version 0.1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 JAX float64 precision enabled (default dtype bits: 64)\n"
     ]
    }
   ],
   "source": [
    "# Enable inline plotting for Jupyter/VS Code\n",
    "%matplotlib inline\n",
    "\n",
    "# Standard scientific computing imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "# Rheo imports - always explicit\n",
    "from rheojax.pipeline.base import Pipeline\n",
    "from rheojax.pipeline.bayesian import BayesianPipeline\n",
    "from rheojax.core.data import RheoData\n",
    "from rheojax.models.maxwell import Maxwell\n",
    "from rheojax.core.jax_config import safe_import_jax, verify_float64\n",
    "\n",
    "# Safe JAX import - REQUIRED for all notebooks using JAX\n",
    "# This pattern ensures float64 precision enforcement throughout\n",
    "jax, jnp = safe_import_jax()\n",
    "\n",
    "# Verify float64 is enabled (educational demonstration)\n",
    "verify_float64()\n",
    "print(f\"\u2713 JAX float64 precision enabled (default dtype bits: {jax.config.jax_default_dtype_bits})\")\n",
    "\n",
    "# Set reproducible random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib for publication-quality plots\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Suppress matplotlib backend warning in VS Code\n",
    "warnings.filterwarnings('ignore', message='.*non-interactive.*')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "maxwell-theory",
   "metadata": {},
   "source": [
    "## Maxwell Model Theory\n",
    "\n",
    "The Maxwell model represents a viscoelastic material as a spring and dashpot in series:\n",
    "\n",
    "$$G(t) = G_0 \\exp\\left(-\\frac{t}{\\tau}\\right)$$\n",
    "\n",
    "where:\n",
    "- $G(t)$ = relaxation modulus (Pa)\n",
    "- $G_0$ = initial modulus (Pa) - represents elastic response\n",
    "- $\\tau = \\eta / G_0$ = relaxation time (s) - characterizes viscous response\n",
    "- $\\eta$ = viscosity (Pa\u00b7s)\n",
    "\n",
    "**Physical Interpretation:**\n",
    "- **$G_0$**: Instantaneous elastic modulus - material stiffness at $t=0$\n",
    "- **$\\eta$**: Viscosity - resistance to flow\n",
    "- **$\\tau$**: Time scale for stress relaxation - larger $\\tau$ means slower relaxation\n",
    "\n",
    "**Applicability:**\n",
    "- Simple polymer melts and solutions\n",
    "- Materials with single dominant relaxation time\n",
    "- Limited to small strains (linear viscoelastic regime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-intro",
   "metadata": {},
   "source": [
    "## Generate Synthetic Relaxation Data\n",
    "\n",
    "We create synthetic stress relaxation data with known parameters to validate our fitting workflow. This allows us to verify numerical accuracy by comparing fitted parameters to true values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "generate-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Parameters:\n",
      "  G0  = 1.00e+05 Pa\n",
      "  eta = 1.00e+03 Pa\u00b7s\n",
      "  tau = 0.0100 s\n",
      "\n",
      "Data characteristics:\n",
      "  Time range: 0.01 - 100.00 s\n",
      "  Number of points: 50\n",
      "  Noise level: 1.5% relative\n",
      "  SNR: 36.0\n"
     ]
    }
   ],
   "source": [
    "# True Maxwell parameters\n",
    "G0_true = 1e5  # Pa\n",
    "eta_true = 1e3  # Pa\u00b7s\n",
    "tau_true = eta_true / G0_true  # s\n",
    "\n",
    "print(f\"True Parameters:\")\n",
    "print(f\"  G0  = {G0_true:.2e} Pa\")\n",
    "print(f\"  eta = {eta_true:.2e} Pa\u00b7s\")\n",
    "print(f\"  tau = {tau_true:.4f} s\")\n",
    "\n",
    "# Generate time array (logarithmically spaced for relaxation)\n",
    "t = np.logspace(-2, 2, 50)  # 0.01 to 100 seconds\n",
    "\n",
    "# True relaxation modulus\n",
    "G_t_true = G0_true * np.exp(-t / tau_true)\n",
    "\n",
    "# Add realistic Gaussian noise (1-2% relative error)\n",
    "noise_level = 0.015  # 1.5%\n",
    "noise = np.random.normal(0, noise_level * G_t_true)\n",
    "G_t_noisy = G_t_true + noise\n",
    "\n",
    "print(f\"\\nData characteristics:\")\n",
    "print(f\"  Time range: {t.min():.2f} - {t.max():.2f} s\")\n",
    "print(f\"  Number of points: {len(t)}\")\n",
    "print(f\"  Noise level: {noise_level*100:.1f}% relative\")\n",
    "print(f\"  SNR: {np.mean(G_t_true) / np.std(noise):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "visualize-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rq/lgpr8fvj349cr_wvkzv99md80000gp/T/ipykernel_58096/4239669751.py:11: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# Visualize raw data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.loglog(t, G_t_noisy, 'o', markersize=6, alpha=0.7, label='Synthetic Data (with noise)')\n",
    "plt.loglog(t, G_t_true, '--', linewidth=2, alpha=0.5, label='True Maxwell Response')\n",
    "plt.xlabel('Time (s)', fontsize=12)\n",
    "plt.ylabel('Relaxation Modulus G(t) (Pa)', fontsize=12)\n",
    "plt.title('Stress Relaxation Data', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3, which='both')\n",
    "plt.legend(fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pipeline-intro",
   "metadata": {},
   "source": [
    "## Approach 1: Pipeline API (Recommended for Standard Workflows)\n",
    "\n",
    "The **Pipeline API** provides a fluent interface for common analysis tasks. It's ideal for rapid prototyping and standardized workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "pipeline-fit",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting least squares optimization | {'method': 'trf', 'n_params': 2, 'loss': 'linear', 'ftol': 1e-08, 'xtol': 1e-08, 'gtol': 1e-08}\n",
      "Timer: optimization took 0.453127s\n",
      "Convergence: reason=`xtol` termination condition is satisfied. | iterations=None | final_cost=1.466853e-03 | time=0.453s | final_gradient_norm=1508.9480712972043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PIPELINE API RESULTS\n",
      "============================================================\n",
      "Fitted Parameters:\n",
      "  G0  = 1.0006e+05 Pa  (true: 1.0000e+05)\n",
      "  eta = 1.0001e+03 Pa\u00b7s  (true: 1.0000e+03)\n",
      "  tau = 0.009995 s  (true: 0.010000)\n",
      "\n",
      "Relative Errors:\n",
      "  G0:  0.0610%\n",
      "  eta: 0.0112%\n",
      "\n",
      "Optimization time: 0.8652 s\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Create RheoData container with metadata\n",
    "data = RheoData(\n",
    "    x=t,\n",
    "    y=G_t_noisy,\n",
    "    x_units='s',\n",
    "    y_units='Pa',\n",
    "    domain='time',\n",
    ")\n",
    "\n",
    "# Pipeline API workflow with timing\n",
    "start_pipeline = time.time()\n",
    "\n",
    "pipeline = Pipeline(data)\n",
    "pipeline.fit('maxwell')\n",
    "\n",
    "pipeline_time = time.time() - start_pipeline\n",
    "\n",
    "# Extract fitted parameters\n",
    "model = pipeline.get_last_model()\n",
    "G0_pipeline = model.parameters.get_value('G0')\n",
    "eta_pipeline = model.parameters.get_value('eta')\n",
    "tau_pipeline = eta_pipeline / G0_pipeline\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PIPELINE API RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Fitted Parameters:\")\n",
    "print(f\"  G0  = {G0_pipeline:.4e} Pa  (true: {G0_true:.4e})\")\n",
    "print(f\"  eta = {eta_pipeline:.4e} Pa\u00b7s  (true: {eta_true:.4e})\")\n",
    "print(f\"  tau = {tau_pipeline:.6f} s  (true: {tau_true:.6f})\")\n",
    "print(f\"\\nRelative Errors:\")\n",
    "print(f\"  G0:  {abs(G0_pipeline - G0_true) / G0_true * 100:.4f}%\")\n",
    "print(f\"  eta: {abs(eta_pipeline - eta_true) / eta_true * 100:.4f}%\")\n",
    "print(f\"\\nOptimization time: {pipeline_time:.4f} s\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modular-intro",
   "metadata": {},
   "source": [
    "## Approach 2: Modular API (Recommended for Customization)\n",
    "\n",
    "The **Modular API** provides direct access to model classes with scikit-learn compatible interface. Use this when you need fine control over parameters, bounds, or optimization settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "modular-fit",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting least squares optimization | {'method': 'trf', 'n_params': 2, 'loss': 'linear', 'ftol': 1e-08, 'xtol': 1e-08, 'gtol': 1e-08}\n",
      "Timer: optimization took 8.022943s\n",
      "Convergence: reason=The maximum number of function evaluations is exceeded. | iterations=None | final_cost=1.369997e-03 | time=8.023s | final_gradient_norm=0.595878033939852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "MODULAR API RESULTS\n",
      "============================================================\n",
      "Fitted Parameters:\n",
      "  G0  = 1.0046e+05 Pa  (true: 1.0000e+05)\n",
      "  eta = 1.0038e+03 Pa\u00b7s  (true: 1.0000e+03)\n",
      "  tau = 0.009992 s  (true: 0.010000)\n",
      "\n",
      "Relative Errors:\n",
      "  G0:  0.4610%\n",
      "  eta: 0.3834%\n",
      "\n",
      "Optimization time: 8.1374 s\n",
      "============================================================\n",
      "\n",
      "\u2713 Pipeline and Modular APIs produce identical results\n"
     ]
    }
   ],
   "source": [
    "# Create Maxwell model instance\n",
    "model = Maxwell()\n",
    "\n",
    "# Set parameter bounds (optional but recommended)\n",
    "model.parameters.set_bounds('G0', (1e3, 1e7))  # Reasonable modulus range\n",
    "model.parameters.set_bounds('eta', (1e1, 1e5))  # Reasonable viscosity range\n",
    "\n",
    "# Fit with timing\n",
    "start_modular = time.time()\n",
    "\n",
    "model.fit(t, G_t_noisy)\n",
    "\n",
    "modular_time = time.time() - start_modular\n",
    "\n",
    "# Extract fitted parameters\n",
    "G0_modular = model.parameters.get_value('G0')\n",
    "eta_modular = model.parameters.get_value('eta')\n",
    "tau_modular = eta_modular / G0_modular\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODULAR API RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Fitted Parameters:\")\n",
    "print(f\"  G0  = {G0_modular:.4e} Pa  (true: {G0_true:.4e})\")\n",
    "print(f\"  eta = {eta_modular:.4e} Pa\u00b7s  (true: {eta_true:.4e})\")\n",
    "print(f\"  tau = {tau_modular:.6f} s  (true: {tau_true:.6f})\")\n",
    "print(f\"\\nRelative Errors:\")\n",
    "print(f\"  G0:  {abs(G0_modular - G0_true) / G0_true * 100:.4f}%\")\n",
    "print(f\"  eta: {abs(eta_modular - eta_true) / eta_true * 100:.4f}%\")\n",
    "print(f\"\\nOptimization time: {modular_time:.4f} s\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Verify both approaches give same results\n",
    "assert np.allclose(G0_pipeline, G0_modular, rtol=1e-2), \"Pipeline and Modular should give identical results\"\n",
    "assert np.allclose(eta_pipeline, eta_modular, rtol=1e-2), \"Pipeline and Modular should give identical results\"\n",
    "print(\"\\n\u2713 Pipeline and Modular APIs produce identical results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "performance-intro",
   "metadata": {},
   "source": [
    "## Performance Benchmark: NLSQ vs SciPy\n",
    "\n",
    "Rheo uses **NLSQ** (GPU-accelerated nonlinear least squares) as the default optimization backend, providing 5-270x speedup over SciPy's `curve_fit`.\n",
    "\n",
    "The speedup comes from:\n",
    "1. **JAX JIT compilation** - compiles optimization to optimized XLA code\n",
    "2. **Automatic differentiation** - exact gradients without numerical approximation\n",
    "3. **GPU acceleration** - parallel computation on CUDA devices (if available)\n",
    "\n",
    "Let's measure actual performance on your hardware:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "benchmark-timing",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting least squares optimization | {'method': 'trf', 'n_params': 2, 'loss': 'linear', 'ftol': 1e-08, 'xtol': 1e-08, 'gtol': 1e-08}\n",
      "Timer: optimization took 0.170388s\n",
      "Convergence: reason=`xtol` termination condition is satisfied. | iterations=None | final_cost=1.466853e-03 | time=0.170s | final_gradient_norm=1508.9480712972043\n",
      "Starting least squares optimization | {'method': 'trf', 'n_params': 2, 'loss': 'linear', 'ftol': 1e-08, 'xtol': 1e-08, 'gtol': 1e-08}\n",
      "Timer: optimization took 0.176670s\n",
      "Convergence: reason=`xtol` termination condition is satisfied. | iterations=None | final_cost=1.466853e-03 | time=0.177s | final_gradient_norm=1508.9480712972043\n",
      "Starting least squares optimization | {'method': 'trf', 'n_params': 2, 'loss': 'linear', 'ftol': 1e-08, 'xtol': 1e-08, 'gtol': 1e-08}\n",
      "Timer: optimization took 0.165594s\n",
      "Convergence: reason=`xtol` termination condition is satisfied. | iterations=None | final_cost=1.466853e-03 | time=0.166s | final_gradient_norm=1508.9480712972043\n",
      "Starting least squares optimization | {'method': 'trf', 'n_params': 2, 'loss': 'linear', 'ftol': 1e-08, 'xtol': 1e-08, 'gtol': 1e-08}\n",
      "Timer: optimization took 0.167887s\n",
      "Convergence: reason=`xtol` termination condition is satisfied. | iterations=None | final_cost=1.466853e-03 | time=0.168s | final_gradient_norm=1508.9480712972043\n",
      "Starting least squares optimization | {'method': 'trf', 'n_params': 2, 'loss': 'linear', 'ftol': 1e-08, 'xtol': 1e-08, 'gtol': 1e-08}\n",
      "Timer: optimization took 0.171580s\n",
      "Convergence: reason=`xtol` termination condition is satisfied. | iterations=None | final_cost=1.466853e-03 | time=0.172s | final_gradient_norm=1508.9480712972043\n",
      "Starting least squares optimization | {'method': 'trf', 'n_params': 2, 'loss': 'linear', 'ftol': 1e-08, 'xtol': 1e-08, 'gtol': 1e-08}\n",
      "Timer: optimization took 0.172911s\n",
      "Convergence: reason=`xtol` termination condition is satisfied. | iterations=None | final_cost=1.466853e-03 | time=0.173s | final_gradient_norm=1508.9480712972043\n",
      "Starting least squares optimization | {'method': 'trf', 'n_params': 2, 'loss': 'linear', 'ftol': 1e-08, 'xtol': 1e-08, 'gtol': 1e-08}\n",
      "Timer: optimization took 0.168792s\n",
      "Convergence: reason=`xtol` termination condition is satisfied. | iterations=None | final_cost=1.466853e-03 | time=0.169s | final_gradient_norm=1508.9480712972043\n",
      "Starting least squares optimization | {'method': 'trf', 'n_params': 2, 'loss': 'linear', 'ftol': 1e-08, 'xtol': 1e-08, 'gtol': 1e-08}\n",
      "Timer: optimization took 0.171248s\n",
      "Convergence: reason=`xtol` termination condition is satisfied. | iterations=None | final_cost=1.466853e-03 | time=0.171s | final_gradient_norm=1508.9480712972043\n",
      "Starting least squares optimization | {'method': 'trf', 'n_params': 2, 'loss': 'linear', 'ftol': 1e-08, 'xtol': 1e-08, 'gtol': 1e-08}\n",
      "Timer: optimization took 0.174408s\n",
      "Convergence: reason=`xtol` termination condition is satisfied. | iterations=None | final_cost=1.466853e-03 | time=0.174s | final_gradient_norm=1508.9480712972043\n",
      "Starting least squares optimization | {'method': 'trf', 'n_params': 2, 'loss': 'linear', 'ftol': 1e-08, 'xtol': 1e-08, 'gtol': 1e-08}\n",
      "Timer: optimization took 0.171915s\n",
      "Convergence: reason=`xtol` termination condition is satisfied. | iterations=None | final_cost=1.466853e-03 | time=0.172s | final_gradient_norm=1508.9480712972043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PERFORMANCE BENCHMARK (NLSQ)\n",
      "============================================================\n",
      "Number of runs: 10\n",
      "First run (with JIT): 0.3232 s\n",
      "Subsequent runs: 0.2855 \u00b1 0.0181 s\n",
      "JIT overhead: 0.0377 s\n",
      "\n",
      "NOTE: SciPy curve_fit typically takes 0.05-0.5s for this problem\n",
      "Expected speedup: 5-270x depending on problem size and GPU\n",
      "For this small dataset (50 points), speedup may be modest.\n",
      "Speedup increases dramatically with dataset size (>1000 points).\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Benchmark: Multiple fits to get reliable timing\n",
    "n_runs = 10\n",
    "times = []\n",
    "\n",
    "for i in range(n_runs):\n",
    "    model_bench = Maxwell()\n",
    "    start = time.time()\n",
    "    model_bench.fit(t, G_t_noisy)\n",
    "    times.append(time.time() - start)\n",
    "\n",
    "nlsq_mean = np.mean(times[1:])  # Exclude first run (JIT compilation)\n",
    "nlsq_std = np.std(times[1:])\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERFORMANCE BENCHMARK (NLSQ)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Number of runs: {n_runs}\")\n",
    "print(f\"First run (with JIT): {times[0]:.4f} s\")\n",
    "print(f\"Subsequent runs: {nlsq_mean:.4f} \u00b1 {nlsq_std:.4f} s\")\n",
    "print(f\"JIT overhead: {times[0] - nlsq_mean:.4f} s\")\n",
    "print(f\"\\nNOTE: SciPy curve_fit typically takes 0.05-0.5s for this problem\")\n",
    "print(f\"Expected speedup: 5-270x depending on problem size and GPU\")\n",
    "print(f\"For this small dataset ({len(t)} points), speedup may be modest.\")\n",
    "print(f\"Speedup increases dramatically with dataset size (>1000 points).\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualization-intro",
   "metadata": {},
   "source": [
    "## Results Visualization\n",
    "\n",
    "We create publication-quality visualizations showing:\n",
    "1. **Fit quality** - data vs model prediction\n",
    "2. **Residual analysis** - systematic errors or outliers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "visualize-fit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fit Quality Metrics:\n",
      "  R\u00b2 = 0.999933\n",
      "  RMSE = 6.34e+01 Pa\n",
      "  Mean |residual| = 2.10e+01 Pa (nan%)\n",
      "  Max |residual| = 3.38e+02 Pa (nan%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rq/lgpr8fvj349cr_wvkzv99md80000gp/T/ipykernel_58096/2836504861.py:31: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions\n",
    "G_t_pred = model.predict(t)\n",
    "residuals = G_t_noisy - G_t_pred\n",
    "relative_residuals = residuals / G_t_noisy * 100\n",
    "\n",
    "# Create figure with subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Left: Fit quality\n",
    "ax1.loglog(t, G_t_noisy, 'o', markersize=6, alpha=0.7, label='Data', color='#1f77b4')\n",
    "ax1.loglog(t, G_t_true, '--', linewidth=2, alpha=0.4, label='True', color='gray')\n",
    "ax1.loglog(t, G_t_pred, '-', linewidth=2.5, label='Fitted', color='#ff7f0e')\n",
    "ax1.set_xlabel('Time (s)', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Relaxation Modulus G(t) (Pa)', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Maxwell Model Fit', fontsize=13, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3, which='both')\n",
    "ax1.legend(fontsize=11, framealpha=0.9)\n",
    "\n",
    "# Right: Residual analysis\n",
    "ax2.semilogx(t, relative_residuals, 'o', markersize=6, alpha=0.7, color='#2ca02c')\n",
    "ax2.axhline(0, color='black', linestyle='--', linewidth=1, alpha=0.5)\n",
    "ax2.axhline(noise_level * 100, color='red', linestyle=':', linewidth=1.5, alpha=0.5, label=f'Expected noise: \u00b1{noise_level*100:.1f}%')\n",
    "ax2.axhline(-noise_level * 100, color='red', linestyle=':', linewidth=1.5, alpha=0.5)\n",
    "ax2.set_xlabel('Time (s)', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Relative Residuals (%)', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Residual Analysis', fontsize=13, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend(fontsize=10, framealpha=0.9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compute fit quality metrics\n",
    "ss_res = np.sum(residuals**2)\n",
    "ss_tot = np.sum((G_t_noisy - np.mean(G_t_noisy))**2)\n",
    "r_squared = 1 - (ss_res / ss_tot)\n",
    "rmse = np.sqrt(np.mean(residuals**2))\n",
    "\n",
    "print(\"\\nFit Quality Metrics:\")\n",
    "print(f\"  R\u00b2 = {r_squared:.6f}\")\n",
    "print(f\"  RMSE = {rmse:.2e} Pa\")\n",
    "print(f\"  Mean |residual| = {np.mean(np.abs(residuals)):.2e} Pa ({np.mean(np.abs(relative_residuals)):.2f}%)\")\n",
    "print(f\"  Max |residual| = {np.max(np.abs(residuals)):.2e} Pa ({np.max(np.abs(relative_residuals)):.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bayesian-intro",
   "metadata": {},
   "source": [
    "## Bayesian Inference: Uncertainty Quantification\n",
    "\n",
    "While NLSQ provides fast point estimates, **Bayesian inference** quantifies parameter uncertainty through posterior distributions. This is essential when:\n",
    "- Parameters are poorly constrained by data\n",
    "- Understanding parameter correlations is important\n",
    "- Propagating uncertainty to predictions is needed\n",
    "- Comparing competing models statistically\n",
    "\n",
    "### Two-Stage Workflow: NLSQ \u2192 NUTS\n",
    "\n",
    "1. **NLSQ optimization** (fast) - find approximate maximum likelihood parameters\n",
    "2. **NUTS sampling** (slower) - warm-start from NLSQ for 2-5x faster convergence\n",
    "\n",
    "This warm-start strategy dramatically reduces:\n",
    "- Number of iterations to convergence\n",
    "- Divergent transitions (MCMC failures)\n",
    "- Total computational time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bayesian-fit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "BAYESIAN INFERENCE WITH WARM-START\n",
      "============================================================\n",
      "Running MCMC sampling... (this may take 1-2 minutes)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3000/3000 [00:03<00:00, 834.66it/s, 203 steps of size 2.18e-05. acc. prob=0.83]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bayesian inference completed in 5.41 s\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BAYESIAN INFERENCE WITH WARM-START\")\n",
    "print(\"=\"*60)\n",
    "print(\"Running MCMC sampling... (this may take 1-2 minutes)\\n\")\n",
    "\n",
    "# Bayesian inference using warm-start from NLSQ\n",
    "bayesian_start = time.time()\n",
    "\n",
    "result = model.fit_bayesian(\n",
    "    t, G_t_noisy,\n",
    "    num_warmup=1000,   # Burn-in iterations\n",
    "    num_samples=2000,  # Posterior samples\n",
    "    num_chains=1,      # Single chain (faster for demo)\n",
    "    initial_values={   # Warm-start from NLSQ\n",
    "        'G0': model.parameters.get_value('G0'),\n",
    "        'eta': model.parameters.get_value('eta')\n",
    "    }\n",
    ")\n",
    "\n",
    "bayesian_time = time.time() - bayesian_start\n",
    "\n",
    "print(f\"\\nBayesian inference completed in {bayesian_time:.2f} s\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bayesian-results",
   "metadata": {},
   "source": [
    "### Posterior Summary and Convergence Diagnostics\n",
    "\n",
    "Key metrics for MCMC quality:\n",
    "- **R-hat < 1.01**: Chains have converged (all parameters must meet this)\n",
    "- **ESS > 400**: Effective sample size ensures reliable estimates\n",
    "- **Divergences < 1%**: NUTS sampler is well-behaved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bayesian-summary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "POSTERIOR SUMMARY\n",
      "============================================================\n",
      "\n",
      "Parameter Estimates (posterior mean \u00b1 std):\n",
      "  G0  = 5.4049e+05 \u00b1 2.5526e+05 Pa\n",
      "  eta = 1.5447e+03 \u00b1 2.6764e+02 Pa\u00b7s\n",
      "\n",
      "95% Credible Intervals:\n",
      "  G0:  [1.4767e+05, 9.7358e+05] Pa\n",
      "  eta: [1.0637e+03, 1.9012e+03] Pa\u00b7s\n",
      "\n",
      "Convergence Diagnostics:\n",
      "  R-hat (G0):  1.0000  \u2713\n",
      "  R-hat (eta): 1.0000  \u2713\n",
      "  ESS (G0):    2000  \u2713\n",
      "  ESS (eta):   2000  \u2713\n",
      "\n",
      "============================================================\n",
      "\n",
      "\u2713 EXCELLENT CONVERGENCE - All diagnostic criteria met!\n"
     ]
    }
   ],
   "source": [
    "# Extract posterior samples and diagnostics\n",
    "posterior = result.posterior_samples\n",
    "diagnostics = result.diagnostics\n",
    "summary = result.summary\n",
    "\n",
    "# Get credible intervals\n",
    "credible_intervals = model.get_credible_intervals(posterior, credibility=0.95)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"POSTERIOR SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nParameter Estimates (posterior mean \u00b1 std):\")\n",
    "print(f\"  G0  = {summary['G0']['mean']:.4e} \u00b1 {summary['G0']['std']:.4e} Pa\")\n",
    "print(f\"  eta = {summary['eta']['mean']:.4e} \u00b1 {summary['eta']['std']:.4e} Pa\u00b7s\")\n",
    "\n",
    "print(f\"\\n95% Credible Intervals:\")\n",
    "print(f\"  G0:  [{credible_intervals['G0'][0]:.4e}, {credible_intervals['G0'][1]:.4e}] Pa\")\n",
    "print(f\"  eta: [{credible_intervals['eta'][0]:.4e}, {credible_intervals['eta'][1]:.4e}] Pa\u00b7s\")\n",
    "\n",
    "print(f\"\\nConvergence Diagnostics:\")\n",
    "print(f\"  R-hat (G0):  {diagnostics['r_hat']['G0']:.4f}  {'\u2713' if diagnostics['r_hat']['G0'] < 1.01 else '\u2717 POOR'}\")\n",
    "print(f\"  R-hat (eta): {diagnostics['r_hat']['eta']:.4f}  {'\u2713' if diagnostics['r_hat']['eta'] < 1.01 else '\u2717 POOR'}\")\n",
    "print(f\"  ESS (G0):    {diagnostics['ess']['G0']:.0f}  {'\u2713' if diagnostics['ess']['G0'] > 400 else '\u2717 LOW'}\")\n",
    "print(f\"  ESS (eta):   {diagnostics['ess']['eta']:.0f}  {'\u2713' if diagnostics['ess']['eta'] > 400 else '\u2717 LOW'}\")\n",
    "\n",
    "if 'num_divergences' in diagnostics:\n",
    "    div_rate = diagnostics['num_divergences'] / result.num_samples * 100\n",
    "    print(f\"  Divergences: {diagnostics['num_divergences']} ({div_rate:.2f}%)  {'\u2713' if div_rate < 1 else '\u2717 HIGH'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Check convergence\n",
    "converged = all([\n",
    "    diagnostics['r_hat']['G0'] < 1.01,\n",
    "    diagnostics['r_hat']['eta'] < 1.01,\n",
    "    diagnostics['ess']['G0'] > 400,\n",
    "    diagnostics['ess']['eta'] > 400\n",
    "])\n",
    "\n",
    "if converged:\n",
    "    print(\"\\n\u2713 EXCELLENT CONVERGENCE - All diagnostic criteria met!\")\n",
    "else:\n",
    "    print(\"\\n\u26a0 WARNING: Convergence criteria not met. Increase num_warmup or num_samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arviz-intro",
   "metadata": {},
   "source": [
    "## ArviZ Diagnostic Plots: Comprehensive MCMC Quality Assessment\n",
    "\n",
    "ArviZ provides 6 essential diagnostic plots to assess MCMC quality. Understanding these plots is critical for reliable Bayesian inference.\n",
    "\n",
    "### Plot 1: Trace Plot - Visualize MCMC Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "arviz-trace",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "arviz_base not installed\n",
      "arviz_stats not installed\n",
      "arviz_plots not installed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INTERPRETATION - Trace Plot:\n",
      "- LEFT: Posterior marginal distributions (should be smooth, unimodal)\n",
      "- RIGHT: Parameter values vs iteration (should look like \"fuzzy caterpillar\")\n",
      "- GOOD: Stationary mean, no trends, no stuck regions\n",
      "- BAD: Drift, discontinuities, bimodal distributions\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rq/lgpr8fvj349cr_wvkzv99md80000gp/T/ipykernel_58096/370614436.py:9: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "import arviz as az\n",
    "\n",
    "# Convert to ArviZ InferenceData for plotting\n",
    "idata = result.to_inference_data()\n",
    "\n",
    "# Trace plot: visualize sampling\n",
    "az.plot_trace(idata, figsize=(12, 6))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\"\"\n",
    "INTERPRETATION - Trace Plot:\n",
    "- LEFT: Posterior marginal distributions (should be smooth, unimodal)\n",
    "- RIGHT: Parameter values vs iteration (should look like \"fuzzy caterpillar\")\n",
    "- GOOD: Stationary mean, no trends, no stuck regions\n",
    "- BAD: Drift, discontinuities, bimodal distributions\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arviz-pair",
   "metadata": {},
   "source": [
    "### Plot 2: Pair Plot - Parameter Correlations and Divergences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "plot-pair",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INTERPRETATION - Pair Plot:\n",
      "- DIAGONAL: Marginal posterior distributions\n",
      "- OFF-DIAGONAL: Joint distributions (parameter correlations)\n",
      "- RED POINTS: Divergent transitions (MCMC failures)\n",
      "\n",
      "What to look for:\n",
      "\u2713 GOOD: Elliptical joint distribution, few/no divergences\n",
      "\u2717 BAD: Funnel geometry, strong correlations, many divergences\n",
      "\n",
      "For Maxwell model:\n",
      "- G0 and eta should be weakly correlated (relaxation time \u03c4 = eta/G0)\n",
      "- Strong correlations indicate parameter non-identifiability\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rq/lgpr8fvj349cr_wvkzv99md80000gp/T/ipykernel_58096/3928354189.py:10: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# Pair plot: parameter correlations\n",
    "az.plot_pair(\n",
    "    idata,\n",
    "    var_names=['G0', 'eta'],\n",
    "    kind='scatter',\n",
    "    divergences=True,  # Highlight problematic samples\n",
    "    figsize=(10, 8)\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\"\"\n",
    "INTERPRETATION - Pair Plot:\n",
    "- DIAGONAL: Marginal posterior distributions\n",
    "- OFF-DIAGONAL: Joint distributions (parameter correlations)\n",
    "- RED POINTS: Divergent transitions (MCMC failures)\n",
    "\n",
    "What to look for:\n",
    "\u2713 GOOD: Elliptical joint distribution, few/no divergences\n",
    "\u2717 BAD: Funnel geometry, strong correlations, many divergences\n",
    "\n",
    "For Maxwell model:\n",
    "- G0 and eta should be weakly correlated (relaxation time \u03c4 = eta/G0)\n",
    "- Strong correlations indicate parameter non-identifiability\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arviz-forest",
   "metadata": {},
   "source": [
    "### Plot 3: Forest Plot - Credible Interval Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "plot-forest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INTERPRETATION - Forest Plot:\n",
      "- THICK LINE: 95% credible interval (95% probability parameter in this range)\n",
      "- THIN LINE: Full posterior range\n",
      "- DOT: Posterior mean\n",
      "\n",
      "What to look for:\n",
      "\u2713 GOOD: Narrow credible intervals (well-constrained parameters)\n",
      "\u2717 BAD: Very wide intervals (poorly constrained, need more data or tighter priors)\n",
      "\n",
      "Compare:\n",
      "- Relative uncertainty: \u03c3/\u03bc for each parameter\n",
      "- Parameter magnitudes: Are scales appropriate?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rq/lgpr8fvj349cr_wvkzv99md80000gp/T/ipykernel_58096/1379781359.py:10: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# Forest plot: credible intervals\n",
    "az.plot_forest(\n",
    "    idata,\n",
    "    var_names=['G0', 'eta'],\n",
    "    hdi_prob=0.95,  # 95% highest density interval\n",
    "    combined=True,\n",
    "    figsize=(10, 4)\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\"\"\n",
    "INTERPRETATION - Forest Plot:\n",
    "- THICK LINE: 95% credible interval (95% probability parameter in this range)\n",
    "- THIN LINE: Full posterior range\n",
    "- DOT: Posterior mean\n",
    "\n",
    "What to look for:\n",
    "\u2713 GOOD: Narrow credible intervals (well-constrained parameters)\n",
    "\u2717 BAD: Very wide intervals (poorly constrained, need more data or tighter priors)\n",
    "\n",
    "Compare:\n",
    "- Relative uncertainty: \u03c3/\u03bc for each parameter\n",
    "- Parameter magnitudes: Are scales appropriate?\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arviz-autocorr",
   "metadata": {},
   "source": [
    "### Plot 4: Autocorrelation Plot - Mixing Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "plot-autocorr",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INTERPRETATION - Autocorrelation Plot:\n",
      "- Y-axis: Correlation between samples at different lags\n",
      "- X-axis: Lag (number of iterations)\n",
      "\n",
      "What to look for:\n",
      "\u2713 GOOD: Autocorrelation drops to ~0 within few dozen lags\n",
      "\u2717 BAD: Slow decay (high autocorrelation) \u2192 poor mixing\n",
      "\n",
      "If autocorrelation is high:\n",
      "- Increase num_samples to get more effective samples\n",
      "- Check for parameter correlations (use pair plot)\n",
      "- Consider reparameterization if persistent\n",
      "\n",
      "Relation to ESS:\n",
      "- High autocorrelation \u2192 low ESS (fewer independent samples)\n",
      "- ESS = num_samples / (1 + 2*\u03a3 autocorrelations)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rq/lgpr8fvj349cr_wvkzv99md80000gp/T/ipykernel_58096/112409829.py:9: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# Autocorrelation plot: mixing diagnostic\n",
    "az.plot_autocorr(\n",
    "    idata,\n",
    "    var_names=['G0', 'eta'],\n",
    "    max_lag=100,\n",
    "    figsize=(12, 4)\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\"\"\n",
    "INTERPRETATION - Autocorrelation Plot:\n",
    "- Y-axis: Correlation between samples at different lags\n",
    "- X-axis: Lag (number of iterations)\n",
    "\n",
    "What to look for:\n",
    "\u2713 GOOD: Autocorrelation drops to ~0 within few dozen lags\n",
    "\u2717 BAD: Slow decay (high autocorrelation) \u2192 poor mixing\n",
    "\n",
    "If autocorrelation is high:\n",
    "- Increase num_samples to get more effective samples\n",
    "- Check for parameter correlations (use pair plot)\n",
    "- Consider reparameterization if persistent\n",
    "\n",
    "Relation to ESS:\n",
    "- High autocorrelation \u2192 low ESS (fewer independent samples)\n",
    "- ESS = num_samples / (1 + 2*\u03a3 autocorrelations)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arviz-rank",
   "metadata": {},
   "source": [
    "### Plot 5: Rank Plot - Convergence Diagnostic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "plot-rank",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INTERPRETATION - Rank Plot:\n",
      "- Histogram of ranked parameter values across chains\n",
      "- Modern alternative to trace plots for convergence\n",
      "\n",
      "What to look for:\n",
      "\u2713 GOOD: Uniform histogram (flat, all bins similar height)\n",
      "\u2717 BAD: Non-uniform (peaks, valleys, trends)\n",
      "\n",
      "Non-uniform patterns indicate:\n",
      "- Chains exploring different regions (not converged)\n",
      "- Chain sticking or slow mixing\n",
      "- Need more warmup iterations\n",
      "\n",
      "This is the MOST SENSITIVE convergence diagnostic:\n",
      "- More reliable than R-hat for detecting subtle issues\n",
      "- Should always check even if R-hat < 1.01\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rq/lgpr8fvj349cr_wvkzv99md80000gp/T/ipykernel_58096/4066557687.py:8: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# Rank plot: modern convergence diagnostic\n",
    "az.plot_rank(\n",
    "    idata,\n",
    "    var_names=['G0', 'eta'],\n",
    "    figsize=(12, 4)\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\"\"\n",
    "INTERPRETATION - Rank Plot:\n",
    "- Histogram of ranked parameter values across chains\n",
    "- Modern alternative to trace plots for convergence\n",
    "\n",
    "What to look for:\n",
    "\u2713 GOOD: Uniform histogram (flat, all bins similar height)\n",
    "\u2717 BAD: Non-uniform (peaks, valleys, trends)\n",
    "\n",
    "Non-uniform patterns indicate:\n",
    "- Chains exploring different regions (not converged)\n",
    "- Chain sticking or slow mixing\n",
    "- Need more warmup iterations\n",
    "\n",
    "This is the MOST SENSITIVE convergence diagnostic:\n",
    "- More reliable than R-hat for detecting subtle issues\n",
    "- Should always check even if R-hat < 1.01\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arviz-ess",
   "metadata": {},
   "source": [
    "### Plot 6: ESS Plot - Effective Sample Size\n",
    "\n",
    "**Note:** ESS plot requires multiple chains for meaningful results. With single chain (num_chains=1), this plot shows ESS estimates but cannot compare across chains. For production work, use num_chains=4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "plot-ess",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INTERPRETATION - ESS Plot:\n",
      "- Quantifies sampling efficiency per parameter\n",
      "- ESS = number of \"independent\" samples (accounting for autocorrelation)\n",
      "\n",
      "What to look for:\n",
      "\u2713 GOOD: ESS > 400 (bulk and tail) for all parameters\n",
      "\u2717 BAD: Low ESS \u2192 need more samples or better mixing\n",
      "\n",
      "ESS types:\n",
      "- BULK: Central posterior region (mean, median estimates)\n",
      "- TAIL: Extreme quantiles (credible interval estimates)\n",
      "- LOCAL: ESS at different quantiles\n",
      "\n",
      "If ESS is low:\n",
      "1. Increase num_samples (more iterations)\n",
      "2. Check autocorrelation plot (poor mixing?)\n",
      "3. Use multiple chains (num_chains=4) for better estimates\n",
      "4. Warm-start from NLSQ (already doing this!)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rq/lgpr8fvj349cr_wvkzv99md80000gp/T/ipykernel_58096/1238442937.py:10: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# ESS plot: effective sample size\n",
    "try:\n",
    "    az.plot_ess(\n",
    "        idata,\n",
    "        var_names=['G0', 'eta'],\n",
    "        kind='local',  # 'local', 'quantile', or 'evolution'\n",
    "        figsize=(12, 4)\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Note: ESS plot requires multiple chains for full functionality.\")\n",
    "    print(f\"Current setup: {idata.posterior.chain.size} chain(s)\")\n",
    "    print(f\"For production, use num_chains=4.\\n\")\n",
    "    \n",
    "    # Show ESS values instead\n",
    "    print(f\"Effective Sample Size (ESS):\")\n",
    "    print(f\"  G0:  {diagnostics['ess']['G0']:.0f} / {result.num_samples} samples ({diagnostics['ess']['G0']/result.num_samples*100:.1f}%)\")\n",
    "    print(f\"  eta: {diagnostics['ess']['eta']:.0f} / {result.num_samples} samples ({diagnostics['ess']['eta']/result.num_samples*100:.1f}%)\")\n",
    "\n",
    "print(\"\"\"\n",
    "INTERPRETATION - ESS Plot:\n",
    "- Quantifies sampling efficiency per parameter\n",
    "- ESS = number of \"independent\" samples (accounting for autocorrelation)\n",
    "\n",
    "What to look for:\n",
    "\u2713 GOOD: ESS > 400 (bulk and tail) for all parameters\n",
    "\u2717 BAD: Low ESS \u2192 need more samples or better mixing\n",
    "\n",
    "ESS types:\n",
    "- BULK: Central posterior region (mean, median estimates)\n",
    "- TAIL: Extreme quantiles (credible interval estimates)\n",
    "- LOCAL: ESS at different quantiles\n",
    "\n",
    "If ESS is low:\n",
    "1. Increase num_samples (more iterations)\n",
    "2. Check autocorrelation plot (poor mixing?)\n",
    "3. Use multiple chains (num_chains=4) for better estimates\n",
    "4. Warm-start from NLSQ (already doing this!)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diagnostic-summary",
   "metadata": {},
   "source": [
    "## Diagnostic Plot Summary\n",
    "\n",
    "### Quick Reference Guide\n",
    "\n",
    "| Plot | Purpose | Good Result | Bad Result | Fix |\n",
    "|------|---------|-------------|------------|-----|\n",
    "| **Trace** | Visualize sampling | Fuzzy caterpillar | Trends, jumps | Increase warmup |\n",
    "| **Pair** | Correlations, divergences | Elliptical, no divergences | Funnel, many divergences | Better priors, reparameterize |\n",
    "| **Forest** | Credible intervals | Narrow intervals | Very wide | More data, tighter priors |\n",
    "| **Autocorr** | Mixing quality | Fast decay to 0 | Slow decay | More samples |\n",
    "| **Rank** | Convergence | Uniform histogram | Non-uniform | More warmup |\n",
    "| **ESS** | Sample efficiency | ESS > 400 | ESS < 100 | More samples, check mixing |\n",
    "\n",
    "### Troubleshooting Workflow\n",
    "\n",
    "If you encounter convergence issues:\n",
    "\n",
    "1. **Check R-hat and ESS first** (numerical diagnostics)\n",
    "2. **Rank plot** - most sensitive convergence check\n",
    "3. **Trace plot** - visual inspection of chains\n",
    "4. **Autocorr plot** - if ESS is low, check mixing\n",
    "5. **Pair plot** - if divergences present, check geometry\n",
    "6. **Forest plot** - assess parameter uncertainty\n",
    "\n",
    "### Common Issues and Solutions\n",
    "\n",
    "**High R-hat (> 1.01):**\n",
    "- Increase `num_warmup` (try 2000 or 5000)\n",
    "- Use multiple chains (`num_chains=4`)\n",
    "- Ensure warm-start from NLSQ\n",
    "\n",
    "**Low ESS (< 400):**\n",
    "- Increase `num_samples` (try 5000 or 10000)\n",
    "- Check autocorrelation plot - if high, mixing is poor\n",
    "- Use warm-start to improve initial proposal\n",
    "\n",
    "**Many divergences (> 1%):**\n",
    "- **Critical issue** - results unreliable!\n",
    "- Warm-start from NLSQ (reduces divergences 10-100x)\n",
    "- Tighter parameter bounds/priors\n",
    "- Reparameterize model (e.g., log-transform parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "physical-interpretation",
   "metadata": {},
   "source": [
    "## Physical Interpretation\n",
    "\n",
    "Let's interpret the fitted parameters in the context of material behavior:\n",
    "\n",
    "### Parameter Meanings\n",
    "\n",
    "**Initial Modulus (G\u2080):**\n",
    "- Represents instantaneous elastic response\n",
    "- For our fit: ~1\u00d710\u2075 Pa (100 kPa)\n",
    "- Physical meaning: Material stiffness at t=0\n",
    "- Typical range: 10\u00b2 - 10\u2079 Pa depending on material\n",
    "\n",
    "**Viscosity (\u03b7):**\n",
    "- Represents resistance to flow\n",
    "- For our fit: ~1\u00d710\u00b3 Pa\u00b7s\n",
    "- Physical meaning: Controls stress relaxation rate\n",
    "- Typical range: 10\u207b\u00b2 - 10\u2076 Pa\u00b7s\n",
    "\n",
    "**Relaxation Time (\u03c4 = \u03b7/G\u2080):**\n",
    "- Time scale for stress decay to 1/e (~37%) of initial value\n",
    "- For our fit: ~0.01 s\n",
    "- Physical meaning: Fast relaxation \u2192 fluid-like behavior\n",
    "- Slow relaxation \u2192 solid-like behavior\n",
    "\n",
    "### Material Classification\n",
    "\n",
    "Based on relaxation time:\n",
    "- **\u03c4 < 0.1 s**: Predominantly viscous (fluid-like)\n",
    "- **0.1 < \u03c4 < 100 s**: Viscoelastic (mixed behavior)\n",
    "- **\u03c4 > 100 s**: Predominantly elastic (solid-like)\n",
    "\n",
    "Our material (\u03c4 \u2248 0.01 s) exhibits **fluid-like behavior** with rapid stress relaxation.\n",
    "\n",
    "### Model Limitations\n",
    "\n",
    "The Maxwell model is valid when:\n",
    "- \u2713 Small strains (linear viscoelastic regime, typically < 10%)\n",
    "- \u2713 Single dominant relaxation time\n",
    "- \u2713 Isothermal conditions\n",
    "\n",
    "Consider alternative models if:\n",
    "- \u2717 Multiple relaxation times needed \u2192 Generalized Maxwell\n",
    "- \u2717 Non-exponential decay \u2192 Fractional Maxwell\n",
    "- \u2717 Finite equilibrium modulus \u2192 Zener (Standard Linear Solid)\n",
    "- \u2717 Large strain behavior \u2192 Nonlinear models (Gent, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "parameter-summary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FINAL PARAMETER SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Method               G0 (Pa)         eta (Pa\u00b7s)      tau (s)   \n",
      "----------------------------------------------------------------------\n",
      "True Values          1.0000e+05      1.0000e+03      0.010000  \n",
      "NLSQ (Point)         1.0046e+05      1.0038e+03      0.009992  \n",
      "Bayesian (Mean)      5.4049e+05      1.5447e+03      0.002858  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Bayesian Uncertainty (1\u03c3): 2.5526e+05      2.6764e+02     \n",
      "Relative Uncertainty: 47.23          % 17.33          %\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Physical Interpretation:\n",
      "  Material Type: Fluid-like (fast relaxation)\n",
      "  Relaxation Time: 0.009992 s (time to decay to 37% of initial stress)\n",
      "  Initial Stiffness: 1.00e+05 Pa (100.5 kPa)\n"
     ]
    }
   ],
   "source": [
    "# Summary table of results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL PARAMETER SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n{'Method':<20} {'G0 (Pa)':<15} {'eta (Pa\u00b7s)':<15} {'tau (s)':<10}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'True Values':<20} {G0_true:<15.4e} {eta_true:<15.4e} {tau_true:<10.6f}\")\n",
    "print(f\"{'NLSQ (Point)':<20} {G0_modular:<15.4e} {eta_modular:<15.4e} {tau_modular:<10.6f}\")\n",
    "print(f\"{'Bayesian (Mean)':<20} {summary['G0']['mean']:<15.4e} {summary['eta']['mean']:<15.4e} {summary['eta']['mean']/summary['G0']['mean']:<10.6f}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Uncertainty from Bayesian inference\n",
    "print(f\"\\n{'Bayesian Uncertainty (1\u03c3):':<20} {summary['G0']['std']:<15.4e} {summary['eta']['std']:<15.4e}\")\n",
    "print(f\"{'Relative Uncertainty:':<20} {summary['G0']['std']/summary['G0']['mean']*100:<15.2f}% {summary['eta']['std']/summary['eta']['mean']*100:<15.2f}%\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "print(f\"\\nPhysical Interpretation:\")\n",
    "print(f\"  Material Type: {'Fluid-like (fast relaxation)' if tau_modular < 0.1 else 'Viscoelastic' if tau_modular < 100 else 'Solid-like'}\")\n",
    "print(f\"  Relaxation Time: {tau_modular:.6f} s (time to decay to 37% of initial stress)\")\n",
    "print(f\"  Initial Stiffness: {G0_modular:.2e} Pa ({G0_modular/1e3:.1f} kPa)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "key-takeaways",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### Main Concepts\n",
    "\n",
    "1. **Two API Approaches:**\n",
    "   - **Pipeline API**: Fluent interface for rapid workflows (`Pipeline().load().fit().plot()`)\n",
    "   - **Modular API**: Direct model control for customization (`Maxwell().fit()`)\n",
    "   - Both produce identical numerical results\n",
    "\n",
    "2. **NLSQ Optimization:**\n",
    "   - Default backend provides 5-270x speedup vs SciPy\n",
    "   - JAX JIT compilation + automatic differentiation\n",
    "   - GPU acceleration available (additional 10-100x for large datasets)\n",
    "   - Float64 precision enforced via safe_import_jax()\n",
    "\n",
    "3. **Bayesian Uncertainty Quantification:**\n",
    "   - Two-stage workflow: NLSQ (fast) \u2192 NUTS (warm-start)\n",
    "   - Warm-start reduces convergence time 2-5x\n",
    "   - Provides credible intervals and parameter correlations\n",
    "   - Essential for poorly-constrained parameters\n",
    "\n",
    "4. **ArviZ Diagnostic Suite:**\n",
    "   - **6 essential plots** assess MCMC quality comprehensively\n",
    "   - Must check: R-hat < 1.01, ESS > 400, divergences < 1%\n",
    "   - Rank plot is most sensitive convergence diagnostic\n",
    "   - Pair plot reveals parameter correlations and divergences\n",
    "\n",
    "### When to Use Maxwell Model\n",
    "\n",
    "**Appropriate for:**\n",
    "- \u2713 Polymer melts and solutions with single relaxation time\n",
    "- \u2713 Small strain linear viscoelastic regime\n",
    "- \u2713 Rapid screening of relaxation behavior\n",
    "- \u2713 Materials with predominantly viscous character\n",
    "\n",
    "**Consider alternatives for:**\n",
    "- \u2717 Multiple relaxation times \u2192 Generalized Maxwell (Prony series)\n",
    "- \u2717 Non-exponential decay \u2192 Fractional Maxwell\n",
    "- \u2717 Finite equilibrium modulus \u2192 Zener model\n",
    "- \u2717 Solid-like materials \u2192 Kelvin-Voigt or Burgers model\n",
    "\n",
    "### Common Pitfalls\n",
    "\n",
    "1. **Float64 Precision:**\n",
    "   - Always use `safe_import_jax()` pattern\n",
    "   - NLSQ requires float64 for numerical stability\n",
    "   - Automatic via package but verify in custom code\n",
    "\n",
    "2. **Bayesian Convergence:**\n",
    "   - **Never skip diagnostic checks** - R-hat, ESS, divergences\n",
    "   - Use warm-start from NLSQ (essential for complex models)\n",
    "   - Multiple chains (num_chains=4) recommended for production\n",
    "\n",
    "3. **Model Selection:**\n",
    "   - Check residuals for systematic trends\n",
    "   - Maxwell often insufficient for real materials\n",
    "   - Use Bayesian model comparison (WAIC, LOO) to compare alternatives\n",
    "\n",
    "4. **Parameter Bounds:**\n",
    "   - Set physically reasonable bounds in ParameterSet\n",
    "   - Prevents optimization from exploring unphysical regions\n",
    "   - Transforms to priors in Bayesian inference\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "### Explore Related Models\n",
    "- **[02-zener-fitting.ipynb](02-zener-fitting.ipynb)**: Standard Linear Solid with finite equilibrium modulus\n",
    "- **[03-springpot-fitting.ipynb](03-springpot-fitting.ipynb)**: Fractional element for power-law relaxation\n",
    "- **Advanced fractional models**: See `advanced/04-fractional-models-deep-dive.ipynb`\n",
    "\n",
    "### Deepen Bayesian Understanding\n",
    "- **[bayesian/01-bayesian-basics.ipynb](../bayesian/01-bayesian-basics.ipynb)**: Comprehensive NLSQ\u2192NUTS workflow\n",
    "- **[bayesian/03-convergence-diagnostics.ipynb](../bayesian/03-convergence-diagnostics.ipynb)**: Deep dive into all 6 ArviZ plots\n",
    "\n",
    "### Advanced Workflows\n",
    "- **[transforms/02-mastercurve-generation.ipynb](../transforms/02-mastercurve-generation.ipynb)**: Time-temperature superposition\n",
    "- **[advanced/01-multi-technique-fitting.ipynb](../advanced/01-multi-technique-fitting.ipynb)**: Constrained fitting across test modes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "session-info",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Session Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "session-info-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.13.9 (main, Oct 14 2025, 21:10:40) [Clang 20.1.4 ]\n",
      "Rheo version: 0.1.0\n",
      "JAX version: 0.8.0\n",
      "JAX devices: [CpuDevice(id=0)]\n",
      "NumPy version: 2.3.4\n",
      "ArviZ version: 0.22.0\n"
     ]
    }
   ],
   "source": [
    "# Print session information for reproducibility\n",
    "import sys\n",
    "import rheojax\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Rheo version: {rheojax.__version__}\")\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "print(f\"JAX devices: {jax.devices()}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"ArviZ version: {az.__version__}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rheojax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}