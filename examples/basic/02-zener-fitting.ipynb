{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zener Model: Oscillatory Shear Fitting\n",
    "\n",
    "This notebook demonstrates the complete workflow for fitting the Zener (Standard Linear Solid) model to oscillatory shear data, showcasing modern Rheo capabilities including GPU-accelerated optimization and Bayesian uncertainty quantification.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "After completing this notebook, you will be able to:\n",
    "- Fit the Zener model to oscillatory shear data (G', G\" vs frequency)\n",
    "- Understand the physical meaning of equilibrium and Maxwell moduli\n",
    "- Leverage NLSQ optimization for 5-270x speedup over SciPy\n",
    "- Perform Bayesian inference with NLSQ\u2192NUTS warm-start workflow\n",
    "- Interpret all 6 ArviZ diagnostic plots for MCMC convergence\n",
    "- Extract physically meaningful parameters with uncertainty quantification\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Basic understanding of:\n",
    "- Rheological concepts (storage modulus G', loss modulus G\")\n",
    "- Linear viscoelasticity\n",
    "- Oscillatory shear testing\n",
    "- Python and NumPy\n",
    "\n",
    "**Recommended:** Complete `01-maxwell-fitting.ipynb` first\n",
    "\n",
    "**Estimated Time:** 35-45 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "We start by importing necessary libraries. Note the **safe JAX import pattern** - this is critical for ensuring float64 precision throughout the entire JAX stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard scientific computing imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Rheo imports - always explicit\n",
    "from rheojax.pipeline.base import Pipeline\n",
    "from rheojax.pipeline.bayesian import BayesianPipeline\n",
    "from rheojax.core.data import RheoData\n",
    "from rheojax.models.zener import Zener\n",
    "from rheojax.core.jax_config import safe_import_jax, verify_float64\n",
    "\n",
    "# Safe JAX import - REQUIRED for all notebooks using JAX\n",
    "# This pattern ensures float64 precision enforcement throughout\n",
    "jax, jnp = safe_import_jax()\n",
    "\n",
    "# Verify float64 is enabled (educational demonstration)\n",
    "verify_float64()\n",
    "print(f\"\u2713 JAX float64 precision enabled (default dtype bits: {jax.config.jax_default_dtype_bits})\")\n",
    "\n",
    "# Set reproducible random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib for publication-quality plots\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zener Model Theory\n",
    "\n",
    "The Zener model (also called Standard Linear Solid or SLS) represents a viscoelastic material as a Maxwell element (spring and dashpot in series) in parallel with an equilibrium spring:\n",
    "\n",
    "**Complex Modulus (Oscillatory Shear):**\n",
    "$$G^*(\\omega) = G_e + \\frac{G_m (\\omega\\tau)^2}{1 + (\\omega\\tau)^2} + i\\frac{G_m \\omega\\tau}{1 + (\\omega\\tau)^2}$$\n",
    "\n",
    "where:\n",
    "- $G'(\\omega)$ = storage modulus = $G_e + \\frac{G_m (\\omega\\tau)^2}{1 + (\\omega\\tau)^2}$\n",
    "- $G''(\\omega)$ = loss modulus = $\\frac{G_m \\omega\\tau}{1 + (\\omega\\tau)^2}$\n",
    "- $G_e$ = equilibrium modulus (Pa) - long-time elastic response\n",
    "- $G_m$ = Maxwell modulus (Pa) - transient elastic component\n",
    "- $\\eta$ = viscosity (Pa\u00b7s) - resistance to flow\n",
    "- $\\tau = \\eta / G_m$ = relaxation time (s)\n",
    "\n",
    "**Physical Interpretation:**\n",
    "- **$G_e$**: Equilibrium modulus - elastic response at $t\u2192\\infty$ (solid-like behavior)\n",
    "- **$G_m$**: Maxwell modulus - transient elastic component that relaxes\n",
    "- **$\\eta$**: Viscosity - determines relaxation rate\n",
    "- **$\\tau$**: Relaxation time - characteristic time scale for stress relaxation\n",
    "\n",
    "**Applicability:**\n",
    "- Crosslinked polymers (gels, elastomers)\n",
    "- Materials with finite equilibrium modulus\n",
    "- Limited to small strains (linear viscoelastic regime)\n",
    "- Single dominant relaxation time\n",
    "\n",
    "**Comparison to Maxwell Model:**\n",
    "- Maxwell: $G_e = 0$ (complete stress relaxation)\n",
    "- Zener: $G_e > 0$ (finite equilibrium modulus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Synthetic Oscillation Data\n",
    "\n",
    "We create synthetic oscillatory shear data with known parameters to validate our fitting workflow. This allows us to verify numerical accuracy by comparing fitted parameters to true values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True Zener parameters\n",
    "Ge_true = 1e4  # Pa (equilibrium modulus)\n",
    "Gm_true = 5e4  # Pa (Maxwell modulus)\n",
    "eta_true = 1e3  # Pa\u00b7s (viscosity)\n",
    "tau_true = eta_true / Gm_true  # s (relaxation time)\n",
    "\n",
    "print(f\"True Parameters:\")\n",
    "print(f\"  Ge  = {Ge_true:.2e} Pa\")\n",
    "print(f\"  Gm  = {Gm_true:.2e} Pa\")\n",
    "print(f\"  eta = {eta_true:.2e} Pa\u00b7s\")\n",
    "print(f\"  tau = {tau_true:.4f} s\")\n",
    "\n",
    "# Generate frequency array (logarithmically spaced)\n",
    "omega = np.logspace(-2, 3, 40)  # 0.01 to 1000 rad/s\n",
    "\n",
    "# True complex modulus\n",
    "omega_tau = omega * tau_true\n",
    "omega_tau_sq = omega_tau**2\n",
    "G_prime_true = Ge_true + Gm_true * omega_tau_sq / (1 + omega_tau_sq)\n",
    "G_double_prime_true = Gm_true * omega_tau / (1 + omega_tau_sq)\n",
    "\n",
    "# Add realistic Gaussian noise (1-2% relative error)\n",
    "noise_level = 0.015  # 1.5%\n",
    "noise_Gp = np.random.normal(0, noise_level * G_prime_true)\n",
    "noise_Gpp = np.random.normal(0, noise_level * G_double_prime_true)\n",
    "\n",
    "G_prime_noisy = G_prime_true + noise_Gp\n",
    "G_double_prime_noisy = G_double_prime_true + noise_Gpp\n",
    "\n",
    "# Create complex modulus for fitting\n",
    "G_star_noisy = G_prime_noisy + 1j * G_double_prime_noisy\n",
    "\n",
    "print(f\"\\nData characteristics:\")\n",
    "print(f\"  Frequency range: {omega.min():.2f} - {omega.max():.2f} rad/s\")\n",
    "print(f\"  Number of points: {len(omega)}\")\n",
    "print(f\"  Noise level: {noise_level*100:.1f}% relative\")\n",
    "print(f\"  SNR (G'): {np.mean(G_prime_true) / np.std(noise_Gp):.1f}\")\n",
    "print(f\"  SNR (G''): {np.mean(G_double_prime_true) / np.std(noise_Gpp):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize raw data\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: G' and G\" vs frequency\n",
    "ax1.loglog(omega, G_prime_noisy, 'o', markersize=6, alpha=0.7, label=\"G' (data)\", color='#1f77b4')\n",
    "ax1.loglog(omega, G_double_prime_noisy, 's', markersize=6, alpha=0.7, label='G\" (data)', color='#ff7f0e')\n",
    "ax1.loglog(omega, G_prime_true, '--', linewidth=2, alpha=0.4, label=\"G' (true)\", color='#1f77b4')\n",
    "ax1.loglog(omega, G_double_prime_true, '--', linewidth=2, alpha=0.4, label='G\" (true)', color='#ff7f0e')\n",
    "ax1.set_xlabel('Angular Frequency \u03c9 (rad/s)', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Modulus (Pa)', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Oscillatory Shear Data', fontsize=13, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3, which='both')\n",
    "ax1.legend(fontsize=10, loc='best', ncol=2)\n",
    "\n",
    "# Right: tan(\u03b4) = G\"/G'\n",
    "tan_delta_noisy = G_double_prime_noisy / G_prime_noisy\n",
    "tan_delta_true = G_double_prime_true / G_prime_true\n",
    "ax2.semilogx(omega, tan_delta_noisy, 'o', markersize=6, alpha=0.7, label='Data', color='#2ca02c')\n",
    "ax2.semilogx(omega, tan_delta_true, '--', linewidth=2, alpha=0.4, label='True', color='gray')\n",
    "ax2.set_xlabel('Angular Frequency \u03c9 (rad/s)', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('tan(\u03b4) = G\"/G\\'', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Loss Tangent', fontsize=13, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend(fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPhysical insights from data:\")\n",
    "print(f\"  G' at low \u03c9: {G_prime_noisy[0]:.2e} Pa (approaches Ge)\")\n",
    "print(f\"  G' at high \u03c9: {G_prime_noisy[-1]:.2e} Pa (approaches Ge + Gm)\")\n",
    "print(f\"  tan(\u03b4) peak: {tan_delta_noisy.max():.4f} at \u03c9 \u2248 {omega[np.argmax(tan_delta_noisy)]:.2f} rad/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 1: Pipeline API (Recommended for Standard Workflows)\n",
    "\n",
    "The **Pipeline API** provides a fluent interface for common analysis tasks. It's ideal for rapid prototyping and standardized workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RheoData container with metadata\n",
    "data = RheoData(\n",
    "    x=omega,\n",
    "    y=G_star_noisy,\n",
    "    x_units='rad/s',\n",
    "    y_units='Pa',\n",
    "    domain='frequency',\n",
    ")\n",
    "\n",
    "# Pipeline API workflow with timing\n",
    "start_pipeline = time.time()\n",
    "\n",
    "pipeline = Pipeline(data)\n",
    "pipeline.fit('zener')\n",
    "\n",
    "pipeline_time = time.time() - start_pipeline\n",
    "\n",
    "# Extract fitted parameters\n",
    "model = pipeline.get_last_model()\n",
    "Ge_pipeline = model.parameters.get_value('Ge')\n",
    "Gm_pipeline = model.parameters.get_value('Gm')\n",
    "eta_pipeline = model.parameters.get_value('eta')\n",
    "tau_pipeline = eta_pipeline / Gm_pipeline\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PIPELINE API RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Fitted Parameters:\")\n",
    "print(f\"  Ge  = {Ge_pipeline:.4e} Pa  (true: {Ge_true:.4e})\")\n",
    "print(f\"  Gm  = {Gm_pipeline:.4e} Pa  (true: {Gm_true:.4e})\")\n",
    "print(f\"  eta = {eta_pipeline:.4e} Pa\u00b7s  (true: {eta_true:.4e})\")\n",
    "print(f\"  tau = {tau_pipeline:.6f} s  (true: {tau_true:.6f})\")\n",
    "print(f\"\\nRelative Errors:\")\n",
    "print(f\"  Ge:  {abs(Ge_pipeline - Ge_true) / Ge_true * 100:.4f}%\")\n",
    "print(f\"  Gm:  {abs(Gm_pipeline - Gm_true) / Gm_true * 100:.4f}%\")\n",
    "print(f\"  eta: {abs(eta_pipeline - eta_true) / eta_true * 100:.4f}%\")\n",
    "print(f\"\\nOptimization time: {pipeline_time:.4f} s\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 2: Modular API (Recommended for Customization)\n",
    "\n",
    "The **Modular API** provides direct access to model classes with scikit-learn compatible interface. Use this when you need fine control over parameters, bounds, or optimization settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Zener model instance\n",
    "model = Zener()\n",
    "\n",
    "# Set parameter bounds (optional but recommended)\n",
    "model.parameters.set_bounds('Ge', (1e2, 1e6))  # Reasonable modulus range\n",
    "model.parameters.set_bounds('Gm', (1e3, 1e7))  # Reasonable modulus range\n",
    "model.parameters.set_bounds('eta', (1e1, 1e5))  # Reasonable viscosity range\n",
    "\n",
    "# Fit with timing\n",
    "start_modular = time.time()\n",
    "\n",
    "model.fit(omega, G_star_noisy)\n",
    "\n",
    "modular_time = time.time() - start_modular\n",
    "\n",
    "# Extract fitted parameters\n",
    "Ge_modular = model.parameters.get_value('Ge')\n",
    "Gm_modular = model.parameters.get_value('Gm')\n",
    "eta_modular = model.parameters.get_value('eta')\n",
    "tau_modular = eta_modular / Gm_modular\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODULAR API RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Fitted Parameters:\")\n",
    "print(f\"  Ge  = {Ge_modular:.4e} Pa  (true: {Ge_true:.4e})\")\n",
    "print(f\"  Gm  = {Gm_modular:.4e} Pa  (true: {Gm_true:.4e})\")\n",
    "print(f\"  eta = {eta_modular:.4e} Pa\u00b7s  (true: {eta_true:.4e})\")\n",
    "print(f\"  tau = {tau_modular:.6f} s  (true: {tau_true:.6f})\")\n",
    "print(f\"\\nRelative Errors:\")\n",
    "print(f\"  Ge:  {abs(Ge_modular - Ge_true) / Ge_true * 100:.4f}%\")\n",
    "print(f\"  Gm:  {abs(Gm_modular - Gm_true) / Gm_true * 100:.4f}%\")\n",
    "print(f\"  eta: {abs(eta_modular - eta_true) / eta_true * 100:.4f}%\")\n",
    "print(f\"\\nOptimization time: {modular_time:.4f} s\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Verify both approaches give same results\n",
    "assert np.allclose(Ge_pipeline, Ge_modular, rtol=1e-9), \"Pipeline and Modular should give identical results\"\n",
    "assert np.allclose(Gm_pipeline, Gm_modular, rtol=1e-9), \"Pipeline and Modular should give identical results\"\n",
    "assert np.allclose(eta_pipeline, eta_modular, rtol=1e-9), \"Pipeline and Modular should give identical results\"\n",
    "print(\"\\n\u2713 Pipeline and Modular APIs produce identical results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Benchmark: NLSQ vs SciPy\n",
    "\n",
    "Rheo uses **NLSQ** (GPU-accelerated nonlinear least squares) as the default optimization backend, providing 5-270x speedup over SciPy's `curve_fit`.\n",
    "\n",
    "The speedup comes from:\n",
    "1. **JAX JIT compilation** - compiles optimization to optimized XLA code\n",
    "2. **Automatic differentiation** - exact gradients without numerical approximation\n",
    "3. **GPU acceleration** - parallel computation on CUDA devices (if available)\n",
    "\n",
    "Let's measure actual performance on your hardware:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark: Multiple fits to get reliable timing\n",
    "n_runs = 10\n",
    "times = []\n",
    "\n",
    "for i in range(n_runs):\n",
    "    model_bench = Zener()\n",
    "    start = time.time()\n",
    "    model_bench.fit(omega, G_star_noisy)\n",
    "    times.append(time.time() - start)\n",
    "\n",
    "nlsq_mean = np.mean(times[1:])  # Exclude first run (JIT compilation)\n",
    "nlsq_std = np.std(times[1:])\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERFORMANCE BENCHMARK (NLSQ)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Number of runs: {n_runs}\")\n",
    "print(f\"First run (with JIT): {times[0]:.4f} s\")\n",
    "print(f\"Subsequent runs: {nlsq_mean:.4f} \u00b1 {nlsq_std:.4f} s\")\n",
    "print(f\"JIT overhead: {times[0] - nlsq_mean:.4f} s\")\n",
    "print(f\"\\nNOTE: SciPy curve_fit typically takes 0.05-0.5s for this problem\")\n",
    "print(f\"Expected speedup: 5-270x depending on problem size and GPU\")\n",
    "print(f\"For this small dataset ({len(omega)} points), speedup may be modest.\")\n",
    "print(f\"Speedup increases dramatically with dataset size (>1000 points).\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Visualization\n",
    "\n",
    "We create publication-quality visualizations showing:\n",
    "1. **Fit quality** - data vs model prediction for G' and G\"\n",
    "2. **Residual analysis** - systematic errors or outliers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "G_star_pred = model.predict(omega)\n",
    "G_prime_pred = np.real(G_star_pred)\n",
    "G_double_prime_pred = np.imag(G_star_pred)\n",
    "\n",
    "# Calculate residuals\n",
    "residuals_Gp = G_prime_noisy - G_prime_pred\n",
    "residuals_Gpp = G_double_prime_noisy - G_double_prime_pred\n",
    "relative_residuals_Gp = residuals_Gp / G_prime_noisy * 100\n",
    "relative_residuals_Gpp = residuals_Gpp / G_double_prime_noisy * 100\n",
    "\n",
    "# Create figure with subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Top left: G' fit quality\n",
    "axes[0, 0].loglog(omega, G_prime_noisy, 'o', markersize=6, alpha=0.7, label='Data', color='#1f77b4')\n",
    "axes[0, 0].loglog(omega, G_prime_true, '--', linewidth=2, alpha=0.4, label='True', color='gray')\n",
    "axes[0, 0].loglog(omega, G_prime_pred, '-', linewidth=2.5, label='Fitted', color='#ff7f0e')\n",
    "axes[0, 0].set_xlabel('\u03c9 (rad/s)', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_ylabel(\"Storage Modulus G' (Pa)\", fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_title(\"G' Fit Quality\", fontsize=13, fontweight='bold')\n",
    "axes[0, 0].grid(True, alpha=0.3, which='both')\n",
    "axes[0, 0].legend(fontsize=10, framealpha=0.9)\n",
    "\n",
    "# Top right: G\" fit quality\n",
    "axes[0, 1].loglog(omega, G_double_prime_noisy, 's', markersize=6, alpha=0.7, label='Data', color='#1f77b4')\n",
    "axes[0, 1].loglog(omega, G_double_prime_true, '--', linewidth=2, alpha=0.4, label='True', color='gray')\n",
    "axes[0, 1].loglog(omega, G_double_prime_pred, '-', linewidth=2.5, label='Fitted', color='#ff7f0e')\n",
    "axes[0, 1].set_xlabel('\u03c9 (rad/s)', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Loss Modulus G\" (Pa)', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_title('G\" Fit Quality', fontsize=13, fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3, which='both')\n",
    "axes[0, 1].legend(fontsize=10, framealpha=0.9)\n",
    "\n",
    "# Bottom left: G' residuals\n",
    "axes[1, 0].semilogx(omega, relative_residuals_Gp, 'o', markersize=6, alpha=0.7, color='#2ca02c')\n",
    "axes[1, 0].axhline(0, color='black', linestyle='--', linewidth=1, alpha=0.5)\n",
    "axes[1, 0].axhline(noise_level * 100, color='red', linestyle=':', linewidth=1.5, alpha=0.5, label=f'Expected noise: \u00b1{noise_level*100:.1f}%')\n",
    "axes[1, 0].axhline(-noise_level * 100, color='red', linestyle=':', linewidth=1.5, alpha=0.5)\n",
    "axes[1, 0].set_xlabel('\u03c9 (rad/s)', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_ylabel(\"G' Relative Residuals (%)\", fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_title(\"G' Residual Analysis\", fontsize=13, fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].legend(fontsize=9, framealpha=0.9)\n",
    "\n",
    "# Bottom right: G\" residuals\n",
    "axes[1, 1].semilogx(omega, relative_residuals_Gpp, 's', markersize=6, alpha=0.7, color='#2ca02c')\n",
    "axes[1, 1].axhline(0, color='black', linestyle='--', linewidth=1, alpha=0.5)\n",
    "axes[1, 1].axhline(noise_level * 100, color='red', linestyle=':', linewidth=1.5, alpha=0.5, label=f'Expected noise: \u00b1{noise_level*100:.1f}%')\n",
    "axes[1, 1].axhline(-noise_level * 100, color='red', linestyle=':', linewidth=1.5, alpha=0.5)\n",
    "axes[1, 1].set_xlabel('\u03c9 (rad/s)', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_ylabel('G\" Relative Residuals (%)', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_title('G\" Residual Analysis', fontsize=13, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].legend(fontsize=9, framealpha=0.9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compute fit quality metrics\n",
    "ss_res_Gp = np.sum(residuals_Gp**2)\n",
    "ss_tot_Gp = np.sum((G_prime_noisy - np.mean(G_prime_noisy))**2)\n",
    "r_squared_Gp = 1 - (ss_res_Gp / ss_tot_Gp)\n",
    "\n",
    "ss_res_Gpp = np.sum(residuals_Gpp**2)\n",
    "ss_tot_Gpp = np.sum((G_double_prime_noisy - np.mean(G_double_prime_noisy))**2)\n",
    "r_squared_Gpp = 1 - (ss_res_Gpp / ss_tot_Gpp)\n",
    "\n",
    "print(\"\\nFit Quality Metrics:\")\n",
    "print(f\"  G' R\u00b2 = {r_squared_Gp:.6f}\")\n",
    "print(f\"  G' RMSE = {np.sqrt(np.mean(residuals_Gp**2)):.2e} Pa\")\n",
    "print(f\"  G' Mean |residual| = {np.mean(np.abs(residuals_Gp)):.2e} Pa ({np.mean(np.abs(relative_residuals_Gp)):.2f}%)\")\n",
    "print(f\"\\n  G'' R\u00b2 = {r_squared_Gpp:.6f}\")\n",
    "print(f\"  G'' RMSE = {np.sqrt(np.mean(residuals_Gpp**2)):.2e} Pa\")\n",
    "print(f\"  G'' Mean |residual| = {np.mean(np.abs(residuals_Gpp)):.2e} Pa ({np.mean(np.abs(relative_residuals_Gpp)):.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Inference: Uncertainty Quantification\n",
    "\n",
    "While NLSQ provides fast point estimates, **Bayesian inference** quantifies parameter uncertainty through posterior distributions. This is essential when:\n",
    "- Parameters are poorly constrained by data\n",
    "- Understanding parameter correlations is important\n",
    "- Propagating uncertainty to predictions is needed\n",
    "- Comparing competing models statistically\n",
    "\n",
    "### Two-Stage Workflow: NLSQ \u2192 NUTS\n",
    "\n",
    "1. **NLSQ optimization** (fast) - find approximate maximum likelihood parameters\n",
    "2. **NUTS sampling** (slower) - warm-start from NLSQ for 2-5x faster convergence\n",
    "\n",
    "This warm-start strategy dramatically reduces:\n",
    "- Number of iterations to convergence\n",
    "- Divergent transitions (MCMC failures)\n",
    "- Total computational time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BAYESIAN INFERENCE WITH WARM-START\")\n",
    "print(\"=\"*60)\n",
    "print(\"Running MCMC sampling... (this may take 1-2 minutes)\\n\")\n",
    "\n",
    "# Bayesian inference using warm-start from NLSQ\n",
    "bayesian_start = time.time()\n",
    "\n",
    "result = model.fit_bayesian(\n",
    "    omega, G_star_noisy,\n",
    "    num_warmup=1000,   # Burn-in iterations\n",
    "    num_samples=2000,  # Posterior samples\n",
    "    num_chains=1,      # Single chain (faster for demo)\n",
    "    initial_values={   # Warm-start from NLSQ\n",
    "        'Ge': model.parameters.get_value('Ge'),\n",
    "        'Gm': model.parameters.get_value('Gm'),\n",
    "        'eta': model.parameters.get_value('eta')\n",
    "    }\n",
    ")\n",
    "\n",
    "bayesian_time = time.time() - bayesian_start\n",
    "\n",
    "print(f\"\\nBayesian inference completed in {bayesian_time:.2f} s\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior Summary and Convergence Diagnostics\n",
    "\n",
    "Key metrics for MCMC quality:\n",
    "- **R-hat < 1.01**: Chains have converged (all parameters must meet this)\n",
    "- **ESS > 400**: Effective sample size ensures reliable estimates\n",
    "- **Divergences < 1%**: NUTS sampler is well-behaved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract posterior samples and diagnostics\n",
    "posterior = result.posterior_samples\n",
    "diagnostics = result.diagnostics\n",
    "summary = result.summary\n",
    "\n",
    "# Get credible intervals\n",
    "credible_intervals = model.get_credible_intervals(posterior, credibility=0.95)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"POSTERIOR SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nParameter Estimates (posterior mean \u00b1 std):\")\n",
    "print(f\"  Ge  = {summary['Ge']['mean']:.4e} \u00b1 {summary['Ge']['std']:.4e} Pa\")\n",
    "print(f\"  Gm  = {summary['Gm']['mean']:.4e} \u00b1 {summary['Gm']['std']:.4e} Pa\")\n",
    "print(f\"  eta = {summary['eta']['mean']:.4e} \u00b1 {summary['eta']['std']:.4e} Pa\u00b7s\")\n",
    "\n",
    "print(f\"\\n95% Credible Intervals:\")\n",
    "print(f\"  Ge:  [{credible_intervals['Ge'][0]:.4e}, {credible_intervals['Ge'][1]:.4e}] Pa\")\n",
    "print(f\"  Gm:  [{credible_intervals['Gm'][0]:.4e}, {credible_intervals['Gm'][1]:.4e}] Pa\")\n",
    "print(f\"  eta: [{credible_intervals['eta'][0]:.4e}, {credible_intervals['eta'][1]:.4e}] Pa\u00b7s\")\n",
    "\n",
    "print(f\"\\nConvergence Diagnostics:\")\n",
    "print(f\"  R-hat (Ge):  {diagnostics['r_hat']['Ge']:.4f}  {'\u2713' if diagnostics['r_hat']['Ge'] < 1.01 else '\u2717 POOR'}\")\n",
    "print(f\"  R-hat (Gm):  {diagnostics['r_hat']['Gm']:.4f}  {'\u2713' if diagnostics['r_hat']['Gm'] < 1.01 else '\u2717 POOR'}\")\n",
    "print(f\"  R-hat (eta): {diagnostics['r_hat']['eta']:.4f}  {'\u2713' if diagnostics['r_hat']['eta'] < 1.01 else '\u2717 POOR'}\")\n",
    "print(f\"  ESS (Ge):    {diagnostics['ess']['Ge']:.0f}  {'\u2713' if diagnostics['ess']['Ge'] > 400 else '\u2717 LOW'}\")\n",
    "print(f\"  ESS (Gm):    {diagnostics['ess']['Gm']:.0f}  {'\u2713' if diagnostics['ess']['Gm'] > 400 else '\u2717 LOW'}\")\n",
    "print(f\"  ESS (eta):   {diagnostics['ess']['eta']:.0f}  {'\u2713' if diagnostics['ess']['eta'] > 400 else '\u2717 LOW'}\")\n",
    "\n",
    "if 'num_divergences' in diagnostics:\n",
    "    div_rate = diagnostics['num_divergences'] / result.num_samples * 100\n",
    "    print(f\"  Divergences: {diagnostics['num_divergences']} ({div_rate:.2f}%)  {'\u2713' if div_rate < 1 else '\u2717 HIGH'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Check convergence\n",
    "converged = all([\n",
    "    diagnostics['r_hat']['Ge'] < 1.01,\n",
    "    diagnostics['r_hat']['Gm'] < 1.01,\n",
    "    diagnostics['r_hat']['eta'] < 1.01,\n",
    "    diagnostics['ess']['Ge'] > 400,\n",
    "    diagnostics['ess']['Gm'] > 400,\n",
    "    diagnostics['ess']['eta'] > 400\n",
    "])\n",
    "\n",
    "if converged:\n",
    "    print(\"\\n\u2713 EXCELLENT CONVERGENCE - All diagnostic criteria met!\")\n",
    "else:\n",
    "    print(\"\\n\u26a0 WARNING: Convergence criteria not met. Increase num_warmup or num_samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ArviZ Diagnostic Plots: Comprehensive MCMC Quality Assessment\n",
    "\n",
    "ArviZ provides 6 essential diagnostic plots to assess MCMC quality. Understanding these plots is critical for reliable Bayesian inference.\n",
    "\n",
    "### Plot 1: Trace Plot - Visualize MCMC Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "\n",
    "# Convert to ArviZ InferenceData for plotting\n",
    "idata = result.to_inference_data()\n",
    "\n",
    "# Trace plot: visualize sampling\n",
    "az.plot_trace(idata, figsize=(12, 8))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\"\"\n",
    "INTERPRETATION - Trace Plot:\n",
    "- LEFT: Posterior marginal distributions (should be smooth, unimodal)\n",
    "- RIGHT: Parameter values vs iteration (should look like \"fuzzy caterpillar\")\n",
    "- GOOD: Stationary mean, no trends, no stuck regions\n",
    "- BAD: Drift, discontinuities, bimodal distributions\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot 2: Pair Plot - Parameter Correlations and Divergences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair plot: parameter correlations\n",
    "az.plot_pair(\n",
    "    idata,\n",
    "    var_names=['Ge', 'Gm', 'eta'],\n",
    "    kind='scatter',\n",
    "    divergences=True,  # Highlight problematic samples\n",
    "    figsize=(12, 10)\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\"\"\n",
    "INTERPRETATION - Pair Plot:\n",
    "- DIAGONAL: Marginal posterior distributions\n",
    "- OFF-DIAGONAL: Joint distributions (parameter correlations)\n",
    "- RED POINTS: Divergent transitions (MCMC failures)\n",
    "\n",
    "What to look for:\n",
    "\u2713 GOOD: Elliptical joint distribution, few/no divergences\n",
    "\u2717 BAD: Funnel geometry, strong correlations, many divergences\n",
    "\n",
    "For Zener model:\n",
    "- Ge and Gm may show weak correlation (both contribute to G')\n",
    "- Gm and eta often correlated (both determine relaxation time \u03c4)\n",
    "- Strong correlations indicate parameter non-identifiability\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot 3: Forest Plot - Credible Interval Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forest plot: credible intervals\n",
    "az.plot_forest(\n",
    "    idata,\n",
    "    var_names=['Ge', 'Gm', 'eta'],\n",
    "    hdi_prob=0.95,  # 95% highest density interval\n",
    "    combined=True,\n",
    "    figsize=(10, 4)\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\"\"\n",
    "INTERPRETATION - Forest Plot:\n",
    "- THICK LINE: 95% credible interval (95% probability parameter in this range)\n",
    "- THIN LINE: Full posterior range\n",
    "- DOT: Posterior mean\n",
    "\n",
    "What to look for:\n",
    "\u2713 GOOD: Narrow credible intervals (well-constrained parameters)\n",
    "\u2717 BAD: Very wide intervals (poorly constrained, need more data or tighter priors)\n",
    "\n",
    "Compare:\n",
    "- Relative uncertainty: \u03c3/\u03bc for each parameter\n",
    "- Parameter magnitudes: Are scales appropriate?\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot 4: Autocorrelation Plot - Mixing Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autocorrelation plot: mixing diagnostic\n",
    "az.plot_autocorr(\n",
    "    idata,\n",
    "    var_names=['Ge', 'Gm', 'eta'],\n",
    "    max_lag=100,\n",
    "    figsize=(12, 4)\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\"\"\n",
    "INTERPRETATION - Autocorrelation Plot:\n",
    "- Y-axis: Correlation between samples at different lags\n",
    "- X-axis: Lag (number of iterations)\n",
    "\n",
    "What to look for:\n",
    "\u2713 GOOD: Autocorrelation drops to ~0 within few dozen lags\n",
    "\u2717 BAD: Slow decay (high autocorrelation) \u2192 poor mixing\n",
    "\n",
    "If autocorrelation is high:\n",
    "- Increase num_samples to get more effective samples\n",
    "- Check for parameter correlations (use pair plot)\n",
    "- Consider reparameterization if persistent\n",
    "\n",
    "Relation to ESS:\n",
    "- High autocorrelation \u2192 low ESS (fewer independent samples)\n",
    "- ESS = num_samples / (1 + 2*\u03a3 autocorrelations)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot 5: Rank Plot - Convergence Diagnostic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank plot: modern convergence diagnostic\n",
    "az.plot_rank(\n",
    "    idata,\n",
    "    var_names=['Ge', 'Gm', 'eta'],\n",
    "    figsize=(12, 4)\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\"\"\n",
    "INTERPRETATION - Rank Plot:\n",
    "- Histogram of ranked parameter values across chains\n",
    "- Modern alternative to trace plots for convergence\n",
    "\n",
    "What to look for:\n",
    "\u2713 GOOD: Uniform histogram (flat, all bins similar height)\n",
    "\u2717 BAD: Non-uniform (peaks, valleys, trends)\n",
    "\n",
    "Non-uniform patterns indicate:\n",
    "- Chains exploring different regions (not converged)\n",
    "- Chain sticking or slow mixing\n",
    "- Need more warmup iterations\n",
    "\n",
    "This is the MOST SENSITIVE convergence diagnostic:\n",
    "- More reliable than R-hat for detecting subtle issues\n",
    "- Should always check even if R-hat < 1.01\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot 6: ESS Plot - Effective Sample Size\n",
    "\n",
    "**Note:** ESS plot requires multiple chains for meaningful results. With single chain (num_chains=1), this plot shows ESS estimates but cannot compare across chains. For production work, use num_chains=4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESS plot: effective sample size\n",
    "try:\n",
    "    az.plot_ess(\n",
    "        idata,\n",
    "        var_names=['Ge', 'Gm', 'eta'],\n",
    "        kind='local',  # 'local', 'quantile', or 'evolution'\n",
    "        figsize=(12, 4)\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Note: ESS plot requires multiple chains for full functionality.\")\n",
    "    print(f\"Current setup: {idata.posterior.chain.size} chain(s)\")\n",
    "    print(f\"For production, use num_chains=4.\\n\")\n",
    "    \n",
    "    # Show ESS values instead\n",
    "    print(f\"Effective Sample Size (ESS):\")\n",
    "    print(f\"  Ge:  {diagnostics['ess']['Ge']:.0f} / {result.num_samples} samples ({diagnostics['ess']['Ge']/result.num_samples*100:.1f}%)\")\n",
    "    print(f\"  Gm:  {diagnostics['ess']['Gm']:.0f} / {result.num_samples} samples ({diagnostics['ess']['Gm']/result.num_samples*100:.1f}%)\")\n",
    "    print(f\"  eta: {diagnostics['ess']['eta']:.0f} / {result.num_samples} samples ({diagnostics['ess']['eta']/result.num_samples*100:.1f}%)\")\n",
    "\n",
    "print(\"\"\"\n",
    "INTERPRETATION - ESS Plot:\n",
    "- Quantifies sampling efficiency per parameter\n",
    "- ESS = number of \"independent\" samples (accounting for autocorrelation)\n",
    "\n",
    "What to look for:\n",
    "\u2713 GOOD: ESS > 400 (bulk and tail) for all parameters\n",
    "\u2717 BAD: Low ESS \u2192 need more samples or better mixing\n",
    "\n",
    "ESS types:\n",
    "- BULK: Central posterior region (mean, median estimates)\n",
    "- TAIL: Extreme quantiles (credible interval estimates)\n",
    "- LOCAL: ESS at different quantiles\n",
    "\n",
    "If ESS is low:\n",
    "1. Increase num_samples (more iterations)\n",
    "2. Check autocorrelation plot (poor mixing?)\n",
    "3. Use multiple chains (num_chains=4) for better estimates\n",
    "4. Warm-start from NLSQ (already doing this!)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Physical Interpretation\n",
    "\n",
    "Let's interpret the fitted parameters in the context of material behavior:\n",
    "\n",
    "### Parameter Meanings\n",
    "\n",
    "**Equilibrium Modulus (Ge):**\n",
    "- Represents long-time elastic response (t\u2192\u221e)\n",
    "- For our fit: ~1\u00d710\u2074 Pa (10 kPa)\n",
    "- Physical meaning: Permanent network structure or entanglement contribution\n",
    "- Typical range: 10\u00b2 - 10\u2076 Pa depending on material\n",
    "\n",
    "**Maxwell Modulus (Gm):**\n",
    "- Represents transient elastic component\n",
    "- For our fit: ~5\u00d710\u2074 Pa (50 kPa)\n",
    "- Physical meaning: Temporary elastic storage that relaxes\n",
    "- Typical range: 10\u00b3 - 10\u2077 Pa\n",
    "\n",
    "**Viscosity (\u03b7):**\n",
    "- Represents resistance to flow\n",
    "- For our fit: ~1\u00d710\u00b3 Pa\u00b7s\n",
    "- Physical meaning: Controls rate of stress relaxation\n",
    "- Typical range: 10\u207b\u00b2 - 10\u2076 Pa\u00b7s\n",
    "\n",
    "**Relaxation Time (\u03c4 = \u03b7/Gm):**\n",
    "- Time scale for stress decay to 1/e (~37%) of initial value\n",
    "- For our fit: ~0.02 s\n",
    "- Physical meaning: Fast relaxation \u2192 fluid-like, Slow relaxation \u2192 solid-like\n",
    "\n",
    "### Material Classification\n",
    "\n",
    "Based on Ge/Gm ratio:\n",
    "- **Ge/Gm < 0.1**: Predominantly viscous (weak gel, concentrated solution)\n",
    "- **0.1 < Ge/Gm < 10**: Viscoelastic (soft solids, weak gels)\n",
    "- **Ge/Gm > 10**: Predominantly elastic (strong gels, elastomers)\n",
    "\n",
    "Our material (Ge/Gm \u2248 0.2) exhibits **balanced viscoelastic behavior** with significant equilibrium elasticity.\n",
    "\n",
    "### Model Limitations\n",
    "\n",
    "The Zener model is valid when:\n",
    "- \u2713 Small strains (linear viscoelastic regime, typically < 10%)\n",
    "- \u2713 Single dominant relaxation time\n",
    "- \u2713 Finite equilibrium modulus\n",
    "\n",
    "Consider alternative models if:\n",
    "- \u2717 Multiple relaxation times needed \u2192 Generalized Maxwell (Prony series)\n",
    "- \u2717 No equilibrium modulus \u2192 Maxwell model\n",
    "- \u2717 Power-law relaxation \u2192 Fractional Zener models\n",
    "- \u2717 Large strain behavior \u2192 Nonlinear models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table of results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL PARAMETER SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n{'Method':<20} {'Ge (Pa)':<15} {'Gm (Pa)':<15} {'eta (Pa\u00b7s)':<15} {'tau (s)':<10}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'True Values':<20} {Ge_true:<15.4e} {Gm_true:<15.4e} {eta_true:<15.4e} {tau_true:<10.6f}\")\n",
    "print(f\"{'NLSQ (Point)':<20} {Ge_modular:<15.4e} {Gm_modular:<15.4e} {eta_modular:<15.4e} {tau_modular:<10.6f}\")\n",
    "print(f\"{'Bayesian (Mean)':<20} {summary['Ge']['mean']:<15.4e} {summary['Gm']['mean']:<15.4e} {summary['eta']['mean']:<15.4e} {summary['eta']['mean']/summary['Gm']['mean']:<10.6f}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Uncertainty from Bayesian inference\n",
    "print(f\"\\n{'Bayesian Uncertainty (1\u03c3):':<20} {summary['Ge']['std']:<15.4e} {summary['Gm']['std']:<15.4e} {summary['eta']['std']:<15.4e}\")\n",
    "print(f\"{'Relative Uncertainty:':<20} {summary['Ge']['std']/summary['Ge']['mean']*100:<15.2f}% {summary['Gm']['std']/summary['Gm']['mean']*100:<15.2f}% {summary['eta']['std']/summary['eta']['mean']*100:<15.2f}%\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Material classification\n",
    "Ge_Gm_ratio = Ge_modular / Gm_modular\n",
    "print(f\"\\nPhysical Interpretation:\")\n",
    "print(f\"  Ge/Gm ratio: {Ge_Gm_ratio:.3f}\")\n",
    "if Ge_Gm_ratio < 0.1:\n",
    "    material_type = \"Predominantly viscous (weak gel/solution)\"\n",
    "elif Ge_Gm_ratio < 10:\n",
    "    material_type = \"Balanced viscoelastic (soft solid/gel)\"\n",
    "else:\n",
    "    material_type = \"Predominantly elastic (strong gel/elastomer)\"\n",
    "print(f\"  Material Type: {material_type}\")\n",
    "print(f\"  Relaxation Time: {tau_modular:.6f} s\")\n",
    "print(f\"  Equilibrium Modulus: {Ge_modular:.2e} Pa ({Ge_modular/1e3:.1f} kPa)\")\n",
    "print(f\"  Total Modulus (G0): {(Ge_modular + Gm_modular):.2e} Pa ({(Ge_modular + Gm_modular)/1e3:.1f} kPa)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### Main Concepts\n",
    "\n",
    "1. **Zener Model Characteristics:**\n",
    "   - Three-parameter model: Ge (equilibrium), Gm (Maxwell), eta (viscosity)\n",
    "   - Finite equilibrium modulus distinguishes from Maxwell model\n",
    "   - Single relaxation time describes transient response\n",
    "   - Applicable to crosslinked polymers, gels, soft solids\n",
    "\n",
    "2. **Oscillatory Shear Data:**\n",
    "   - Complex modulus: G* = G' + iG\"\n",
    "   - G' (storage) measures elastic energy storage\n",
    "   - G\" (loss) measures viscous dissipation\n",
    "   - tan(\u03b4) = G\"/G' quantifies viscoelastic character\n",
    "\n",
    "3. **NLSQ Optimization:**\n",
    "   - Default backend provides 5-270x speedup vs SciPy\n",
    "   - JAX JIT compilation + automatic differentiation\n",
    "   - GPU acceleration available (additional 10-100x for large datasets)\n",
    "   - Float64 precision enforced via safe_import_jax()\n",
    "\n",
    "4. **Bayesian Uncertainty Quantification:**\n",
    "   - Two-stage workflow: NLSQ (fast) \u2192 NUTS (warm-start)\n",
    "   - Warm-start reduces convergence time 2-5x\n",
    "   - Provides credible intervals and parameter correlations\n",
    "   - Essential for identifying non-identifiability issues\n",
    "\n",
    "5. **ArviZ Diagnostic Suite:**\n",
    "   - **6 essential plots** assess MCMC quality comprehensively\n",
    "   - Must check: R-hat < 1.01, ESS > 400, divergences < 1%\n",
    "   - Rank plot is most sensitive convergence diagnostic\n",
    "   - Pair plot reveals parameter correlations (Gm-eta often correlated)\n",
    "\n",
    "### When to Use Zener Model\n",
    "\n",
    "**Appropriate for:**\n",
    "- \u2713 Crosslinked polymers with finite equilibrium modulus\n",
    "- \u2713 Gels and soft solids (physical or chemical networks)\n",
    "- \u2713 Materials with single dominant relaxation time\n",
    "- \u2713 Small strain linear viscoelastic regime\n",
    "\n",
    "**Consider alternatives for:**\n",
    "- \u2717 Complete stress relaxation (Ge=0) \u2192 Maxwell model\n",
    "- \u2717 Multiple relaxation times \u2192 Generalized Maxwell\n",
    "- \u2717 Power-law frequency dependence \u2192 Fractional Zener models\n",
    "- \u2717 Solid-like materials with no flow \u2192 Kelvin-Voigt\n",
    "\n",
    "### Common Pitfalls\n",
    "\n",
    "1. **Parameter Correlation:**\n",
    "   - Gm and eta often correlated (both determine \u03c4)\n",
    "   - Check pair plot for non-identifiability\n",
    "   - May need multi-technique fitting to improve constraint\n",
    "\n",
    "2. **Frequency Range:**\n",
    "   - Need data spanning relaxation time: 0.1\u03c4 < 1/\u03c9 < 10\u03c4\n",
    "   - Insufficient range \u2192 poor Ge or Gm estimation\n",
    "   - Use mastercurve generation to extend range\n",
    "\n",
    "3. **Model Selection:**\n",
    "   - Check residuals for systematic trends\n",
    "   - Zener may be insufficient if multiple relaxation times present\n",
    "   - Use Bayesian model comparison (WAIC, LOO) to compare alternatives\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "### Explore Related Models\n",
    "- **[03-springpot-fitting.ipynb](03-springpot-fitting.ipynb)**: Fractional element for power-law behavior\n",
    "- **[01-maxwell-fitting.ipynb](01-maxwell-fitting.ipynb)**: Special case with Ge=0\n",
    "- **Advanced fractional models**: See `advanced/04-fractional-models-deep-dive.ipynb`\n",
    "\n",
    "### Deepen Bayesian Understanding\n",
    "- **[bayesian/01-bayesian-basics.ipynb](../bayesian/01-bayesian-basics.ipynb)**: Comprehensive NLSQ\u2192NUTS workflow\n",
    "- **[bayesian/03-convergence-diagnostics.ipynb](../bayesian/03-convergence-diagnostics.ipynb)**: Deep dive into all 6 ArviZ plots\n",
    "- **[bayesian/04-model-comparison.ipynb](../bayesian/04-model-comparison.ipynb)**: Statistical model selection\n",
    "\n",
    "### Advanced Workflows\n",
    "- **[transforms/02-mastercurve-generation.ipynb](../transforms/02-mastercurve-generation.ipynb)**: Extend frequency range via TTS\n",
    "- **[advanced/01-multi-technique-fitting.ipynb](../advanced/01-multi-technique-fitting.ipynb)**: Constrained fitting across test modes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Session Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print session information for reproducibility\n",
    "import sys\n",
    "import rheojax\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Rheo version: {rheojax.__version__}\")\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "print(f\"JAX devices: {jax.devices()}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"ArviZ version: {az.__version__}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}