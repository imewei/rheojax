{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zener Model: Oscillatory Shear Fitting\n",
    "\n",
    "This notebook demonstrates the complete workflow for fitting the Zener (Standard Linear Solid) model to oscillatory shear data, showcasing modern Rheo capabilities including GPU-accelerated optimization and Bayesian uncertainty quantification.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "After completing this notebook, you will be able to:\n",
    "- Fit the Zener model to oscillatory shear data (G', G\" vs frequency)\n",
    "- Understand the physical meaning of equilibrium and Maxwell moduli\n",
    "- Leverage NLSQ optimization for 5-270x speedup over SciPy\n",
    "- Perform Bayesian inference with NLSQ→NUTS warm-start workflow\n",
    "- Interpret all 6 ArviZ diagnostic plots for MCMC convergence\n",
    "- Extract physically meaningful parameters with uncertainty quantification\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Basic understanding of:\n",
    "- Rheological concepts (storage modulus G', loss modulus G\")\n",
    "- Linear viscoelasticity\n",
    "- Oscillatory shear testing\n",
    "- Python and NumPy\n",
    "\n",
    "**Recommended:** Complete `01-maxwell-fitting.ipynb` first\n",
    "\n",
    "**Estimated Time:** 35-45 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab Setup - Run this cell first!\n",
    "# Skip if running locally with rheojax already installed\n",
    "\n",
    "import sys\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Install rheojax and dependencies\n",
    "    !pip install -q rheojax\n",
    "    \n",
    "    # Colab uses float32 by default - we need float64 for numerical stability\n",
    "    # This MUST be set before importing JAX\n",
    "    import os\n",
    "    os.environ['JAX_ENABLE_X64'] = 'true'\n",
    "    \n",
    "    print(\"✓ RheoJAX installed successfully!\")\n",
    "    print(\"✓ Float64 precision enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "We start by importing necessary libraries. Note the **safe JAX import pattern** - this is critical for ensuring float64 precision throughout the entire JAX stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure matplotlib for inline plotting in VS Code/Jupyter\n",
    "# MUST come before importing matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Standard scientific computing imports\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from rheojax.models.zener import Zener\n",
    "\n",
    "from rheojax.core.data import RheoData\n",
    "from rheojax.core.jax_config import safe_import_jax, verify_float64\n",
    "from rheojax.core.test_modes import TestMode\n",
    "\n",
    "# Rheo imports - always explicit\n",
    "from rheojax.pipeline.base import Pipeline\n",
    "from rheojax.pipeline.bayesian import BayesianPipeline\n",
    "\n",
    "# Safe JAX import - REQUIRED for all notebooks using JAX\n",
    "# This pattern ensures float64 precision enforcement throughout\n",
    "jax, jnp = safe_import_jax()\n",
    "\n",
    "# Verify float64 is enabled (educational demonstration)\n",
    "verify_float64()\n",
    "print(f\"✓ JAX float64 precision enabled (default dtype bits: {jax.config.jax_default_dtype_bits})\")\n",
    "\n",
    "# Set reproducible random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib for publication-quality plots\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Suppress matplotlib backend warning in VS Code\n",
    "warnings.filterwarnings('ignore', message='.*non-interactive.*')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zener Model Theory\n",
    "\n",
    "The Zener model (also called Standard Linear Solid or SLS) represents a viscoelastic material as a Maxwell element (spring and dashpot in series) in parallel with an equilibrium spring:\n",
    "\n",
    "**Complex Modulus (Oscillatory Shear):**\n",
    "$$G^*(\\omega) = G_e + \\frac{G_m (\\omega\\tau)^2}{1 + (\\omega\\tau)^2} + i\\frac{G_m \\omega\\tau}{1 + (\\omega\\tau)^2}$$\n",
    "\n",
    "where:\n",
    "- $G'(\\omega)$ = storage modulus = $G_e + \\frac{G_m (\\omega\\tau)^2}{1 + (\\omega\\tau)^2}$\n",
    "- $G''(\\omega)$ = loss modulus = $\\frac{G_m \\omega\\tau}{1 + (\\omega\\tau)^2}$\n",
    "- $G_e$ = equilibrium modulus (Pa) - long-time elastic response\n",
    "- $G_m$ = Maxwell modulus (Pa) - transient elastic component\n",
    "- $\\eta$ = viscosity (Pa·s) - resistance to flow\n",
    "- $\\tau = \\eta / G_m$ = relaxation time (s)\n",
    "\n",
    "**Physical Interpretation:**\n",
    "- **$G_e$**: Equilibrium modulus - elastic response at $t→\\infty$ (solid-like behavior)\n",
    "- **$G_m$**: Maxwell modulus - transient elastic component that relaxes\n",
    "- **$\\eta$**: Viscosity - determines relaxation rate\n",
    "- **$\\tau$**: Relaxation time - characteristic time scale for stress relaxation\n",
    "\n",
    "**Applicability:**\n",
    "- Crosslinked polymers (gels, elastomers)\n",
    "- Materials with finite equilibrium modulus\n",
    "- Limited to small strains (linear viscoelastic regime)\n",
    "- Single dominant relaxation time\n",
    "\n",
    "**Comparison to Maxwell Model:**\n",
    "- Maxwell: $G_e = 0$ (complete stress relaxation)\n",
    "- Zener: $G_e > 0$ (finite equilibrium modulus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Synthetic Oscillation Data\n",
    "\n",
    "We create synthetic oscillatory shear data with known parameters to validate our fitting workflow. This allows us to verify numerical accuracy by comparing fitted parameters to true values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True Zener parameters\n",
    "Ge_true = 1e4  # Pa (equilibrium modulus)\n",
    "Gm_true = 5e4  # Pa (Maxwell modulus)\n",
    "eta_true = 1e3  # Pa·s (viscosity)\n",
    "tau_true = eta_true / Gm_true  # s (relaxation time)\n",
    "\n",
    "print(f\"True Parameters:\")\n",
    "print(f\"  Ge  = {Ge_true:.2e} Pa\")\n",
    "print(f\"  Gm  = {Gm_true:.2e} Pa\")\n",
    "print(f\"  eta = {eta_true:.2e} Pa·s\")\n",
    "print(f\"  tau = {tau_true:.4f} s\")\n",
    "\n",
    "# Generate frequency array (logarithmically spaced)\n",
    "omega = np.logspace(-2, 3, 40)  # 0.01 to 1000 rad/s\n",
    "\n",
    "# True complex modulus\n",
    "omega_tau = omega * tau_true\n",
    "omega_tau_sq = omega_tau**2\n",
    "G_prime_true = Ge_true + Gm_true * omega_tau_sq / (1 + omega_tau_sq)\n",
    "G_double_prime_true = Gm_true * omega_tau / (1 + omega_tau_sq)\n",
    "\n",
    "# Add realistic Gaussian noise (1-2% relative error)\n",
    "noise_level = 0.015  # 1.5%\n",
    "noise_Gp = np.random.normal(0, noise_level * G_prime_true)\n",
    "noise_Gpp = np.random.normal(0, noise_level * G_double_prime_true)\n",
    "\n",
    "G_prime_noisy = G_prime_true + noise_Gp\n",
    "G_double_prime_noisy = G_double_prime_true + noise_Gpp\n",
    "\n",
    "# Create complex modulus for fitting\n",
    "G_star_noisy = G_prime_noisy + 1j * G_double_prime_noisy\n",
    "\n",
    "print(f\"\\nData characteristics:\")\n",
    "print(f\"  Frequency range: {omega.min():.2f} - {omega.max():.2f} rad/s\")\n",
    "print(f\"  Number of points: {len(omega)}\")\n",
    "print(f\"  Noise level: {noise_level*100:.1f}% relative\")\n",
    "print(f\"  SNR (G'): {np.mean(G_prime_true) / np.std(noise_Gp):.1f}\")\n",
    "print(f\"  SNR (G''): {np.mean(G_double_prime_true) / np.std(noise_Gpp):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize raw data\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: G' and G\" vs frequency\n",
    "ax1.loglog(omega, G_prime_noisy, 'o', markersize=6, alpha=0.7, label=\"G' (data)\", color='#1f77b4')\n",
    "ax1.loglog(omega, G_double_prime_noisy, 's', markersize=6, alpha=0.7, label='G\" (data)', color='#ff7f0e')\n",
    "ax1.loglog(omega, G_prime_true, '--', linewidth=2, alpha=0.4, label=\"G' (true)\", color='#1f77b4')\n",
    "ax1.loglog(omega, G_double_prime_true, '--', linewidth=2, alpha=0.4, label='G\" (true)', color='#ff7f0e')\n",
    "ax1.set_xlabel('Angular Frequency ω (rad/s)', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Modulus (Pa)', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Oscillatory Shear Data', fontsize=13, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3, which='both')\n",
    "ax1.legend(fontsize=10, loc='best', ncol=2)\n",
    "\n",
    "# Right: tan(δ) = G\"/G'\n",
    "tan_delta_noisy = G_double_prime_noisy / G_prime_noisy\n",
    "tan_delta_true = G_double_prime_true / G_prime_true\n",
    "ax2.semilogx(omega, tan_delta_noisy, 'o', markersize=6, alpha=0.7, label='Data', color='#2ca02c')\n",
    "ax2.semilogx(omega, tan_delta_true, '--', linewidth=2, alpha=0.4, label='True', color='gray')\n",
    "ax2.set_xlabel('Angular Frequency ω (rad/s)', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('tan(δ) = G\"/G\\'', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Loss Tangent', fontsize=13, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend(fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "display(fig)\n",
    "plt.close(fig)\n",
    "\n",
    "print(\"\\nPhysical insights from data:\")\n",
    "print(f\"  G' at low ω: {G_prime_noisy[0]:.2e} Pa (approaches Ge)\")\n",
    "print(f\"  G' at high ω: {G_prime_noisy[-1]:.2e} Pa (approaches Ge + Gm)\")\n",
    "print(f\"  tan(δ) peak: {tan_delta_noisy.max():.4f} at ω ≈ {omega[np.argmax(tan_delta_noisy)]:.2f} rad/s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 1: Pipeline API (Recommended for Standard Workflows)\n",
    "\n",
    "The **Pipeline API** provides a fluent interface for common analysis tasks. It's ideal for rapid prototyping and standardized workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RheoData container with metadata\n",
    "# Note: For complex oscillatory data, domain='frequency' should auto-detect test_mode=OSCILLATION\n",
    "# However, if auto-detection doesn't work, use the Modular API with explicit test_mode parameter\n",
    "data = RheoData(\n",
    "    x=omega,\n",
    "    y=G_star_noisy,\n",
    "    x_units='rad/s',\n",
    "    y_units='Pa',\n",
    "    domain='frequency',\n",
    ")\n",
    "\n",
    "# Pipeline API workflow with timing\n",
    "start_pipeline = time.time()\n",
    "\n",
    "pipeline = Pipeline(data)\n",
    "pipeline.fit('zener')\n",
    "\n",
    "pipeline_time = time.time() - start_pipeline\n",
    "\n",
    "# Extract fitted parameters\n",
    "model = pipeline.get_last_model()\n",
    "Ge_pipeline = model.parameters.get_value('Ge')\n",
    "Gm_pipeline = model.parameters.get_value('Gm')\n",
    "eta_pipeline = model.parameters.get_value('eta')\n",
    "tau_pipeline = eta_pipeline / Gm_pipeline\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PIPELINE API RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Fitted Parameters:\")\n",
    "print(f\"  Ge  = {Ge_pipeline:.4e} Pa  (true: {Ge_true:.4e})\")\n",
    "print(f\"  Gm  = {Gm_pipeline:.4e} Pa  (true: {Gm_true:.4e})\")\n",
    "print(f\"  eta = {eta_pipeline:.4e} Pa·s  (true: {eta_true:.4e})\")\n",
    "print(f\"  tau = {tau_pipeline:.6f} s  (true: {tau_true:.6f})\")\n",
    "print(f\"\\nRelative Errors:\")\n",
    "print(f\"  Ge:  {abs(Ge_pipeline - Ge_true) / Ge_true * 100:.4f}%\")\n",
    "print(f\"  Gm:  {abs(Gm_pipeline - Gm_true) / Gm_true * 100:.4f}%\")\n",
    "print(f\"  eta: {abs(eta_pipeline - eta_true) / eta_true * 100:.4f}%\")\n",
    "print(f\"\\nOptimization time: {pipeline_time:.4f} s\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Note: For oscillatory data, Modular API with explicit test_mode=OSCILLATION\n",
    "# gives more accurate results than Pipeline API auto-detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 2: Modular API (Recommended for Customization)\n",
    "\n",
    "The **Modular API** provides direct access to model classes with scikit-learn compatible interface. Use this when you need fine control over parameters, bounds, or optimization settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Zener model instance\n",
    "model = Zener()\n",
    "\n",
    "# Set parameter bounds (optional but recommended)\n",
    "model.parameters.set_bounds('Ge', (1e2, 1e6))  # Reasonable modulus range\n",
    "model.parameters.set_bounds('Gm', (1e3, 1e7))  # Reasonable modulus range\n",
    "model.parameters.set_bounds('eta', (1e1, 1e5))  # Reasonable viscosity range\n",
    "\n",
    "# Fit with timing\n",
    "start_modular = time.time()\n",
    "\n",
    "# IMPORTANT: For oscillatory data (complex modulus), must specify test_mode=OSCILLATION\n",
    "# This ensures the model fits both G' (real) and G\" (imaginary) components correctly\n",
    "model.fit(omega, G_star_noisy, test_mode=TestMode.OSCILLATION)\n",
    "\n",
    "modular_time = time.time() - start_modular\n",
    "\n",
    "# Extract fitted parameters\n",
    "Ge_modular = model.parameters.get_value('Ge')\n",
    "Gm_modular = model.parameters.get_value('Gm')\n",
    "eta_modular = model.parameters.get_value('eta')\n",
    "tau_modular = eta_modular / Gm_modular\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODULAR API RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Fitted Parameters:\")\n",
    "print(f\"  Ge  = {Ge_modular:.4e} Pa  (true: {Ge_true:.4e})\")\n",
    "print(f\"  Gm  = {Gm_modular:.4e} Pa  (true: {Gm_true:.4e})\")\n",
    "print(f\"  eta = {eta_modular:.4e} Pa·s  (true: {eta_true:.4e})\")\n",
    "print(f\"  tau = {tau_modular:.6f} s  (true: {tau_true:.6f})\")\n",
    "print(f\"\\nRelative Errors:\")\n",
    "print(f\"  Ge:  {abs(Ge_modular - Ge_true) / Ge_true * 100:.4f}%\")\n",
    "print(f\"  Gm:  {abs(Gm_modular - Gm_true) / Gm_true * 100:.4f}%\")\n",
    "print(f\"  eta: {abs(eta_modular - eta_true) / eta_true * 100:.4f}%\")\n",
    "print(f\"\\nOptimization time: {modular_time:.4f} s\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Note: Pipeline and Modular APIs should give identical results when using same test_mode\n",
    "# If RheoData auto-detection doesn't work properly, results may differ\n",
    "# Both should achieve <1% error vs true values for this synthetic dataset\n",
    "print(f\"\\nComparison to Pipeline API:\")\n",
    "print(f\"  Ge  difference: {abs(Ge_pipeline - Ge_modular) / Ge_true * 100:.2f}% of true value\")\n",
    "print(f\"  Gm  difference: {abs(Gm_pipeline - Gm_modular) / Gm_true * 100:.2f}% of true value\")\n",
    "print(f\"  eta difference: {abs(eta_pipeline - eta_modular) / eta_true * 100:.2f}% of true value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Benchmark: NLSQ vs SciPy\n",
    "\n",
    "Rheo uses **NLSQ** (GPU-accelerated nonlinear least squares) as the default optimization backend, providing 5-270x speedup over SciPy's `curve_fit`.\n",
    "\n",
    "The speedup comes from:\n",
    "1. **JAX JIT compilation** - compiles optimization to optimized XLA code\n",
    "2. **Automatic differentiation** - exact gradients without numerical approximation\n",
    "3. **GPU acceleration** - parallel computation on CUDA devices (if available)\n",
    "\n",
    "Let's measure actual performance on your hardware:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark: Multiple fits to get reliable timing\n",
    "n_runs = 10\n",
    "times = []\n",
    "\n",
    "for i in range(n_runs):\n",
    "    model_bench = Zener()\n",
    "    start = time.time()\n",
    "    model_bench.fit(omega, G_star_noisy, test_mode=TestMode.OSCILLATION)\n",
    "    times.append(time.time() - start)\n",
    "\n",
    "nlsq_mean = np.mean(times[1:])  # Exclude first run (JIT compilation)\n",
    "nlsq_std = np.std(times[1:])\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERFORMANCE BENCHMARK (NLSQ)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Number of runs: {n_runs}\")\n",
    "print(f\"First run (with JIT): {times[0]:.4f} s\")\n",
    "print(f\"Subsequent runs: {nlsq_mean:.4f} ± {nlsq_std:.4f} s\")\n",
    "print(f\"JIT overhead: {times[0] - nlsq_mean:.4f} s\")\n",
    "print(f\"\\nNOTE: SciPy curve_fit typically takes 0.05-0.5s for this problem\")\n",
    "print(f\"Expected speedup: 5-270x depending on problem size and GPU\")\n",
    "print(f\"For this small dataset ({len(omega)} points), speedup may be modest.\")\n",
    "print(f\"Speedup increases dramatically with dataset size (>1000 points).\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Visualization\n",
    "\n",
    "We create publication-quality visualizations showing:\n",
    "1. **Fit quality** - data vs model prediction for G' and G\"\n",
    "2. **Residual analysis** - systematic errors or outliers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "G_star_pred = model.predict(omega)\n",
    "G_prime_pred = np.real(G_star_pred)\n",
    "G_double_prime_pred = np.imag(G_star_pred)\n",
    "\n",
    "# Calculate residuals\n",
    "residuals_Gp = G_prime_noisy - G_prime_pred\n",
    "residuals_Gpp = G_double_prime_noisy - G_double_prime_pred\n",
    "relative_residuals_Gp = residuals_Gp / G_prime_noisy * 100\n",
    "relative_residuals_Gpp = residuals_Gpp / G_double_prime_noisy * 100\n",
    "\n",
    "# Create figure with subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Top left: G' fit quality\n",
    "axes[0, 0].loglog(omega, G_prime_noisy, 'o', markersize=6, alpha=0.7, label='Data', color='#1f77b4')\n",
    "axes[0, 0].loglog(omega, G_prime_true, '--', linewidth=2, alpha=0.4, label='True', color='gray')\n",
    "axes[0, 0].loglog(omega, G_prime_pred, '-', linewidth=2.5, label='Fitted', color='#ff7f0e')\n",
    "axes[0, 0].set_xlabel('ω (rad/s)', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_ylabel(\"Storage Modulus G' (Pa)\", fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_title(\"G' Fit Quality\", fontsize=13, fontweight='bold')\n",
    "axes[0, 0].grid(True, alpha=0.3, which='both')\n",
    "axes[0, 0].legend(fontsize=10, framealpha=0.9)\n",
    "\n",
    "# Top right: G\" fit quality\n",
    "axes[0, 1].loglog(omega, G_double_prime_noisy, 's', markersize=6, alpha=0.7, label='Data', color='#1f77b4')\n",
    "axes[0, 1].loglog(omega, G_double_prime_true, '--', linewidth=2, alpha=0.4, label='True', color='gray')\n",
    "axes[0, 1].loglog(omega, G_double_prime_pred, '-', linewidth=2.5, label='Fitted', color='#ff7f0e')\n",
    "axes[0, 1].set_xlabel('ω (rad/s)', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Loss Modulus G\" (Pa)', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_title('G\" Fit Quality', fontsize=13, fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3, which='both')\n",
    "axes[0, 1].legend(fontsize=10, framealpha=0.9)\n",
    "\n",
    "# Bottom left: G' residuals\n",
    "axes[1, 0].semilogx(omega, relative_residuals_Gp, 'o', markersize=6, alpha=0.7, color='#2ca02c')\n",
    "axes[1, 0].axhline(0, color='black', linestyle='--', linewidth=1, alpha=0.5)\n",
    "axes[1, 0].axhline(noise_level * 100, color='red', linestyle=':', linewidth=1.5, alpha=0.5, label=f'Expected noise: ±{noise_level*100:.1f}%')\n",
    "axes[1, 0].axhline(-noise_level * 100, color='red', linestyle=':', linewidth=1.5, alpha=0.5)\n",
    "axes[1, 0].set_xlabel('ω (rad/s)', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_ylabel(\"G' Relative Residuals (%)\", fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_title(\"G' Residual Analysis\", fontsize=13, fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].legend(fontsize=9, framealpha=0.9)\n",
    "\n",
    "# Bottom right: G\" residuals\n",
    "axes[1, 1].semilogx(omega, relative_residuals_Gpp, 's', markersize=6, alpha=0.7, color='#2ca02c')\n",
    "axes[1, 1].axhline(0, color='black', linestyle='--', linewidth=1, alpha=0.5)\n",
    "axes[1, 1].axhline(noise_level * 100, color='red', linestyle=':', linewidth=1.5, alpha=0.5, label=f'Expected noise: ±{noise_level*100:.1f}%')\n",
    "axes[1, 1].axhline(-noise_level * 100, color='red', linestyle=':', linewidth=1.5, alpha=0.5)\n",
    "axes[1, 1].set_xlabel('ω (rad/s)', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_ylabel('G\" Relative Residuals (%)', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_title('G\" Residual Analysis', fontsize=13, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].legend(fontsize=9, framealpha=0.9)\n",
    "\n",
    "plt.tight_layout()\n",
    "display(fig)\n",
    "plt.close(fig)\n",
    "\n",
    "# Compute fit quality metrics\n",
    "ss_res_Gp = np.sum(residuals_Gp**2)\n",
    "ss_tot_Gp = np.sum((G_prime_noisy - np.mean(G_prime_noisy))**2)\n",
    "r_squared_Gp = 1 - (ss_res_Gp / ss_tot_Gp)\n",
    "\n",
    "ss_res_Gpp = np.sum(residuals_Gpp**2)\n",
    "ss_tot_Gpp = np.sum((G_double_prime_noisy - np.mean(G_double_prime_noisy))**2)\n",
    "r_squared_Gpp = 1 - (ss_res_Gpp / ss_tot_Gpp)\n",
    "\n",
    "print(\"\\nFit Quality Metrics:\")\n",
    "print(f\"  G' R² = {r_squared_Gp:.6f}\")\n",
    "print(f\"  G' RMSE = {np.sqrt(np.mean(residuals_Gp**2)):.2e} Pa\")\n",
    "print(f\"  G' Mean |residual| = {np.mean(np.abs(residuals_Gp)):.2e} Pa ({np.mean(np.abs(relative_residuals_Gp)):.2f}%)\")\n",
    "print(f\"\\n  G'' R² = {r_squared_Gpp:.6f}\")\n",
    "print(f\"  G'' RMSE = {np.sqrt(np.mean(residuals_Gpp**2)):.2e} Pa\")\n",
    "print(f\"  G'' Mean |residual| = {np.mean(np.abs(residuals_Gpp)):.2e} Pa ({np.mean(np.abs(relative_residuals_Gpp)):.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Inference: Uncertainty Quantification\n",
    "\n",
    "While NLSQ provides fast point estimates, **Bayesian inference** quantifies parameter uncertainty through posterior distributions. This is essential when:\n",
    "- Parameters are poorly constrained by data\n",
    "- Understanding parameter correlations is important\n",
    "- Propagating uncertainty to predictions is needed\n",
    "- Comparing competing models statistically\n",
    "\n",
    "### Two-Stage Workflow: NLSQ → NUTS\n",
    "\n",
    "1. **NLSQ optimization** (fast) - find approximate maximum likelihood parameters\n",
    "2. **NUTS sampling** (slower) - warm-start from NLSQ for 2-5x faster convergence\n",
    "\n",
    "This warm-start strategy dramatically reduces:\n",
    "- Number of iterations to convergence\n",
    "- Divergent transitions (MCMC failures)\n",
    "- Total computational time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BAYESIAN INFERENCE WITH WARM-START\")\n",
    "print(\"=\"*60)\n",
    "print(\"Running MCMC sampling... (this may take 1-2 minutes)\\n\")\n",
    "\n",
    "# Bayesian inference using warm-start from NLSQ\n",
    "bayesian_start = time.time()\n",
    "\n",
    "result = model.fit_bayesian(\n",
    "    omega, G_star_noisy,\n",
    "    num_warmup=1000,   # Burn-in iterations\n",
    "    num_samples=2000,  # Posterior samples\n",
    "    num_chains=1,      # Single chain (faster for demo)\n",
    "    initial_values={   # Warm-start from NLSQ\n",
    "        'Ge': model.parameters.get_value('Ge'),\n",
    "        'Gm': model.parameters.get_value('Gm'),\n",
    "        'eta': model.parameters.get_value('eta')\n",
    "    }\n",
    ")\n",
    "\n",
    "bayesian_time = time.time() - bayesian_start\n",
    "\n",
    "print(f\"\\nBayesian inference completed in {bayesian_time:.2f} s\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior Summary and Convergence Diagnostics\n",
    "\n",
    "Key metrics for MCMC quality:\n",
    "- **R-hat < 1.01**: Chains have converged (all parameters must meet this)\n",
    "- **ESS > 400**: Effective sample size ensures reliable estimates\n",
    "- **Divergences < 1%**: NUTS sampler is well-behaved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract posterior samples and diagnostics\n",
    "posterior = result.posterior_samples\n",
    "diagnostics = result.diagnostics\n",
    "summary = result.summary\n",
    "\n",
    "# Get credible intervals\n",
    "credible_intervals = model.get_credible_intervals(posterior, credibility=0.95)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"POSTERIOR SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nParameter Estimates (posterior mean ± std):\")\n",
    "print(f\"  Ge  = {summary['Ge']['mean']:.4e} ± {summary['Ge']['std']:.4e} Pa\")\n",
    "print(f\"  Gm  = {summary['Gm']['mean']:.4e} ± {summary['Gm']['std']:.4e} Pa\")\n",
    "print(f\"  eta = {summary['eta']['mean']:.4e} ± {summary['eta']['std']:.4e} Pa·s\")\n",
    "\n",
    "print(f\"\\n95% Credible Intervals:\")\n",
    "print(f\"  Ge:  [{credible_intervals['Ge'][0]:.4e}, {credible_intervals['Ge'][1]:.4e}] Pa\")\n",
    "print(f\"  Gm:  [{credible_intervals['Gm'][0]:.4e}, {credible_intervals['Gm'][1]:.4e}] Pa\")\n",
    "print(f\"  eta: [{credible_intervals['eta'][0]:.4e}, {credible_intervals['eta'][1]:.4e}] Pa·s\")\n",
    "\n",
    "print(f\"\\nConvergence Diagnostics:\")\n",
    "print(f\"  R-hat (Ge):  {diagnostics['r_hat']['Ge']:.4f}  {'✓' if diagnostics['r_hat']['Ge'] < 1.01 else '✗ POOR'}\")\n",
    "print(f\"  R-hat (Gm):  {diagnostics['r_hat']['Gm']:.4f}  {'✓' if diagnostics['r_hat']['Gm'] < 1.01 else '✗ POOR'}\")\n",
    "print(f\"  R-hat (eta): {diagnostics['r_hat']['eta']:.4f}  {'✓' if diagnostics['r_hat']['eta'] < 1.01 else '✗ POOR'}\")\n",
    "print(f\"  ESS (Ge):    {diagnostics['ess']['Ge']:.0f}  {'✓' if diagnostics['ess']['Ge'] > 400 else '✗ LOW'}\")\n",
    "print(f\"  ESS (Gm):    {diagnostics['ess']['Gm']:.0f}  {'✓' if diagnostics['ess']['Gm'] > 400 else '✗ LOW'}\")\n",
    "print(f\"  ESS (eta):   {diagnostics['ess']['eta']:.0f}  {'✓' if diagnostics['ess']['eta'] > 400 else '✗ LOW'}\")\n",
    "\n",
    "if 'num_divergences' in diagnostics:\n",
    "    div_rate = diagnostics['num_divergences'] / result.num_samples * 100\n",
    "    print(f\"  Divergences: {diagnostics['num_divergences']} ({div_rate:.2f}%)  {'✓' if div_rate < 1 else '✗ HIGH'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Check convergence\n",
    "converged = all([\n",
    "    diagnostics['r_hat']['Ge'] < 1.01,\n",
    "    diagnostics['r_hat']['Gm'] < 1.01,\n",
    "    diagnostics['r_hat']['eta'] < 1.01,\n",
    "    diagnostics['ess']['Ge'] > 400,\n",
    "    diagnostics['ess']['Gm'] > 400,\n",
    "    diagnostics['ess']['eta'] > 400\n",
    "])\n",
    "\n",
    "if converged:\n",
    "    print(\"\\n✓ EXCELLENT CONVERGENCE - All diagnostic criteria met!\")\n",
    "else:\n",
    "    print(\"\\n⚠ WARNING: Convergence criteria not met. Increase num_warmup or num_samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ArviZ Diagnostic Plots: Comprehensive MCMC Quality Assessment\n",
    "\n",
    "ArviZ provides 6 essential diagnostic plots to assess MCMC quality. Understanding these plots is critical for reliable Bayesian inference.\n",
    "\n",
    "### Plot 1: Trace Plot - Visualize MCMC Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "\n",
    "# Convert to ArviZ InferenceData for plotting\n",
    "idata = result.to_inference_data()\n",
    "\n",
    "# Trace plot: visualize sampling\n",
    "az.plot_trace(idata, figsize=(12, 8))\n",
    "plt.tight_layout()\n",
    "fig = plt.gcf()  # Get current figure from ArviZ\n",
    "display(fig)\n",
    "plt.close(fig)\n",
    "\n",
    "print(\"\"\"\n",
    "INTERPRETATION - Trace Plot:\n",
    "- LEFT: Posterior marginal distributions (should be smooth, unimodal)\n",
    "- RIGHT: Parameter values vs iteration (should look like \"fuzzy caterpillar\")\n",
    "- GOOD: Stationary mean, no trends, no stuck regions\n",
    "- BAD: Drift, discontinuities, bimodal distributions\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot 2: Pair Plot - Parameter Correlations and Divergences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair plot: parameter correlations\n",
    "az.plot_pair(\n",
    "    idata,\n",
    "    var_names=['Ge', 'Gm', 'eta'],\n",
    "    kind='scatter',\n",
    "    divergences=True,  # Highlight problematic samples\n",
    "    figsize=(12, 10)\n",
    ")\n",
    "plt.tight_layout()\n",
    "fig = plt.gcf()  # Get current figure from ArviZ\n",
    "display(fig)\n",
    "plt.close(fig)\n",
    "\n",
    "print(\"\"\"\n",
    "INTERPRETATION - Pair Plot:\n",
    "- DIAGONAL: Marginal posterior distributions\n",
    "- OFF-DIAGONAL: Joint distributions (parameter correlations)\n",
    "- RED POINTS: Divergent transitions (MCMC failures)\n",
    "\n",
    "What to look for:\n",
    "✓ GOOD: Elliptical joint distribution, few/no divergences\n",
    "✗ BAD: Funnel geometry, strong correlations, many divergences\n",
    "\n",
    "For Zener model:\n",
    "- Ge and Gm may show weak correlation (both contribute to G')\n",
    "- Gm and eta often correlated (both determine relaxation time τ)\n",
    "- Strong correlations indicate parameter non-identifiability\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot 3: Forest Plot - Credible Interval Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forest plot: credible intervals\n",
    "az.plot_forest(\n",
    "    idata,\n",
    "    var_names=['Ge', 'Gm', 'eta'],\n",
    "    hdi_prob=0.95,  # 95% highest density interval\n",
    "    combined=True,\n",
    "    figsize=(10, 4)\n",
    ")\n",
    "plt.tight_layout()\n",
    "fig = plt.gcf()  # Get current figure from ArviZ\n",
    "display(fig)\n",
    "plt.close(fig)\n",
    "\n",
    "print(\"\"\"\n",
    "INTERPRETATION - Forest Plot:\n",
    "- THICK LINE: 95% credible interval (95% probability parameter in this range)\n",
    "- THIN LINE: Full posterior range\n",
    "- DOT: Posterior mean\n",
    "\n",
    "What to look for:\n",
    "✓ GOOD: Narrow credible intervals (well-constrained parameters)\n",
    "✗ BAD: Very wide intervals (poorly constrained, need more data or tighter priors)\n",
    "\n",
    "Compare:\n",
    "- Relative uncertainty: σ/μ for each parameter\n",
    "- Parameter magnitudes: Are scales appropriate?\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot 4: Autocorrelation Plot - Mixing Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autocorrelation plot: mixing diagnostic\n",
    "az.plot_autocorr(\n",
    "    idata,\n",
    "    var_names=['Ge', 'Gm', 'eta'],\n",
    "    max_lag=100,\n",
    "    figsize=(12, 4)\n",
    ")\n",
    "plt.tight_layout()\n",
    "fig = plt.gcf()  # Get current figure from ArviZ\n",
    "display(fig)\n",
    "plt.close(fig)\n",
    "\n",
    "print(\"\"\"\n",
    "INTERPRETATION - Autocorrelation Plot:\n",
    "- Y-axis: Correlation between samples at different lags\n",
    "- X-axis: Lag (number of iterations)\n",
    "\n",
    "What to look for:\n",
    "✓ GOOD: Autocorrelation drops to ~0 within few dozen lags\n",
    "✗ BAD: Slow decay (high autocorrelation) → poor mixing\n",
    "\n",
    "If autocorrelation is high:\n",
    "- Increase num_samples to get more effective samples\n",
    "- Check for parameter correlations (use pair plot)\n",
    "- Consider reparameterization if persistent\n",
    "\n",
    "Relation to ESS:\n",
    "- High autocorrelation → low ESS (fewer independent samples)\n",
    "- ESS = num_samples / (1 + 2*Σ autocorrelations)\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot 5: Rank Plot - Convergence Diagnostic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank plot: modern convergence diagnostic\n",
    "az.plot_rank(\n",
    "    idata,\n",
    "    var_names=['Ge', 'Gm', 'eta'],\n",
    "    figsize=(12, 4)\n",
    ")\n",
    "plt.tight_layout()\n",
    "fig = plt.gcf()  # Get current figure from ArviZ\n",
    "display(fig)\n",
    "plt.close(fig)\n",
    "\n",
    "print(\"\"\"\n",
    "INTERPRETATION - Rank Plot:\n",
    "- Histogram of ranked parameter values across chains\n",
    "- Modern alternative to trace plots for convergence\n",
    "\n",
    "What to look for:\n",
    "✓ GOOD: Uniform histogram (flat, all bins similar height)\n",
    "✗ BAD: Non-uniform (peaks, valleys, trends)\n",
    "\n",
    "Non-uniform patterns indicate:\n",
    "- Chains exploring different regions (not converged)\n",
    "- Chain sticking or slow mixing\n",
    "- Need more warmup iterations\n",
    "\n",
    "This is the MOST SENSITIVE convergence diagnostic:\n",
    "- More reliable than R-hat for detecting subtle issues\n",
    "- Should always check even if R-hat < 1.01\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot 6: ESS Plot - Effective Sample Size\n",
    "\n",
    "**Note:** ESS plot requires multiple chains for meaningful results. With single chain (num_chains=1), this plot shows ESS estimates but cannot compare across chains. For production work, use num_chains=4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESS plot: effective sample size\n",
    "try:\n",
    "    az.plot_ess(\n",
    "        idata,\n",
    "        var_names=['Ge', 'Gm', 'eta'],\n",
    "        kind='local',  # 'local', 'quantile', or 'evolution'\n",
    "        figsize=(12, 4)\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    fig = plt.gcf()  # Get current figure from ArviZ\n",
    "    display(fig)\n",
    "    plt.close(fig)\n",
    "except Exception as e:\n",
    "    print(f\"Note: ESS plot requires multiple chains for full functionality.\")\n",
    "    print(f\"Current setup: {idata.posterior.chain.size} chain(s)\")\n",
    "    print(f\"For production, use num_chains=4.\\n\")\n",
    "    \n",
    "    # Show ESS values instead\n",
    "    print(f\"Effective Sample Size (ESS):\")\n",
    "    print(f\"  Ge:  {diagnostics['ess']['Ge']:.0f} / {result.num_samples} samples ({diagnostics['ess']['Ge']/result.num_samples*100:.1f}%)\")\n",
    "    print(f\"  Gm:  {diagnostics['ess']['Gm']:.0f} / {result.num_samples} samples ({diagnostics['ess']['Gm']/result.num_samples*100:.1f}%)\")\n",
    "    print(f\"  eta: {diagnostics['ess']['eta']:.0f} / {result.num_samples} samples ({diagnostics['ess']['eta']/result.num_samples*100:.1f}%)\")\n",
    "\n",
    "print(\"\"\"\n",
    "INTERPRETATION - ESS Plot:\n",
    "- Quantifies sampling efficiency per parameter\n",
    "- ESS = number of \"independent\" samples (accounting for autocorrelation)\n",
    "\n",
    "What to look for:\n",
    "✓ GOOD: ESS > 400 (bulk and tail) for all parameters\n",
    "✗ BAD: Low ESS → need more samples or better mixing\n",
    "\n",
    "ESS types:\n",
    "- BULK: Central posterior region (mean, median estimates)\n",
    "- TAIL: Extreme quantiles (credible interval estimates)\n",
    "- LOCAL: ESS at different quantiles\n",
    "\n",
    "If ESS is low:\n",
    "1. Increase num_samples (more iterations)\n",
    "2. Check autocorrelation plot (poor mixing?)\n",
    "3. Use multiple chains (num_chains=4) for better estimates\n",
    "4. Warm-start from NLSQ (already doing this!)\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Physical Interpretation\n",
    "\n",
    "Let's interpret the fitted parameters in the context of material behavior:\n",
    "\n",
    "### Parameter Meanings\n",
    "\n",
    "**Equilibrium Modulus (Ge):**\n",
    "- Represents long-time elastic response (t→∞)\n",
    "- For our fit: ~1×10⁴ Pa (10 kPa)\n",
    "- Physical meaning: Permanent network structure or entanglement contribution\n",
    "- Typical range: 10² - 10⁶ Pa depending on material\n",
    "\n",
    "**Maxwell Modulus (Gm):**\n",
    "- Represents transient elastic component\n",
    "- For our fit: ~5×10⁴ Pa (50 kPa)\n",
    "- Physical meaning: Temporary elastic storage that relaxes\n",
    "- Typical range: 10³ - 10⁷ Pa\n",
    "\n",
    "**Viscosity (η):**\n",
    "- Represents resistance to flow\n",
    "- For our fit: ~1×10³ Pa·s\n",
    "- Physical meaning: Controls rate of stress relaxation\n",
    "- Typical range: 10⁻² - 10⁶ Pa·s\n",
    "\n",
    "**Relaxation Time (τ = η/Gm):**\n",
    "- Time scale for stress decay to 1/e (~37%) of initial value\n",
    "- For our fit: ~0.02 s\n",
    "- Physical meaning: Fast relaxation → fluid-like, Slow relaxation → solid-like\n",
    "\n",
    "### Material Classification\n",
    "\n",
    "Based on Ge/Gm ratio:\n",
    "- **Ge/Gm < 0.1**: Predominantly viscous (weak gel, concentrated solution)\n",
    "- **0.1 < Ge/Gm < 10**: Viscoelastic (soft solids, weak gels)\n",
    "- **Ge/Gm > 10**: Predominantly elastic (strong gels, elastomers)\n",
    "\n",
    "Our material (Ge/Gm ≈ 0.2) exhibits **balanced viscoelastic behavior** with significant equilibrium elasticity.\n",
    "\n",
    "### Model Limitations\n",
    "\n",
    "The Zener model is valid when:\n",
    "- ✓ Small strains (linear viscoelastic regime, typically < 10%)\n",
    "- ✓ Single dominant relaxation time\n",
    "- ✓ Finite equilibrium modulus\n",
    "\n",
    "Consider alternative models if:\n",
    "- ✗ Multiple relaxation times needed → Generalized Maxwell (Prony series)\n",
    "- ✗ No equilibrium modulus → Maxwell model\n",
    "- ✗ Power-law relaxation → Fractional Zener models\n",
    "- ✗ Large strain behavior → Nonlinear models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table of results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL PARAMETER SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n{'Method':<20} {'Ge (Pa)':<15} {'Gm (Pa)':<15} {'eta (Pa·s)':<15} {'tau (s)':<10}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'True Values':<20} {Ge_true:<15.4e} {Gm_true:<15.4e} {eta_true:<15.4e} {tau_true:<10.6f}\")\n",
    "print(f\"{'NLSQ (Point)':<20} {Ge_modular:<15.4e} {Gm_modular:<15.4e} {eta_modular:<15.4e} {tau_modular:<10.6f}\")\n",
    "print(f\"{'Bayesian (Mean)':<20} {summary['Ge']['mean']:<15.4e} {summary['Gm']['mean']:<15.4e} {summary['eta']['mean']:<15.4e} {summary['eta']['mean']/summary['Gm']['mean']:<10.6f}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Uncertainty from Bayesian inference\n",
    "print(f\"\\n{'Bayesian Uncertainty (1σ):':<20} {summary['Ge']['std']:<15.4e} {summary['Gm']['std']:<15.4e} {summary['eta']['std']:<15.4e}\")\n",
    "print(f\"{'Relative Uncertainty:':<20} {summary['Ge']['std']/summary['Ge']['mean']*100:<15.2f}% {summary['Gm']['std']/summary['Gm']['mean']*100:<15.2f}% {summary['eta']['std']/summary['eta']['mean']*100:<15.2f}%\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Material classification\n",
    "Ge_Gm_ratio = Ge_modular / Gm_modular\n",
    "print(f\"\\nPhysical Interpretation:\")\n",
    "print(f\"  Ge/Gm ratio: {Ge_Gm_ratio:.3f}\")\n",
    "if Ge_Gm_ratio < 0.1:\n",
    "    material_type = \"Predominantly viscous (weak gel/solution)\"\n",
    "elif Ge_Gm_ratio < 10:\n",
    "    material_type = \"Balanced viscoelastic (soft solid/gel)\"\n",
    "else:\n",
    "    material_type = \"Predominantly elastic (strong gel/elastomer)\"\n",
    "print(f\"  Material Type: {material_type}\")\n",
    "print(f\"  Relaxation Time: {tau_modular:.6f} s\")\n",
    "print(f\"  Equilibrium Modulus: {Ge_modular:.2e} Pa ({Ge_modular/1e3:.1f} kPa)\")\n",
    "print(f\"  Total Modulus (G0): {(Ge_modular + Gm_modular):.2e} Pa ({(Ge_modular + Gm_modular)/1e3:.1f} kPa)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### Main Concepts\n",
    "\n",
    "1. **Zener Model Characteristics:**\n",
    "   - Three-parameter model: Ge (equilibrium), Gm (Maxwell), eta (viscosity)\n",
    "   - Finite equilibrium modulus distinguishes from Maxwell model\n",
    "   - Single relaxation time describes transient response\n",
    "   - Applicable to crosslinked polymers, gels, soft solids\n",
    "\n",
    "2. **Oscillatory Shear Data:**\n",
    "   - Complex modulus: G* = G' + iG\"\n",
    "   - G' (storage) measures elastic energy storage\n",
    "   - G\" (loss) measures viscous dissipation\n",
    "   - tan(δ) = G\"/G' quantifies viscoelastic character\n",
    "\n",
    "3. **NLSQ Optimization:**\n",
    "   - Default backend provides 5-270x speedup vs SciPy\n",
    "   - JAX JIT compilation + automatic differentiation\n",
    "   - GPU acceleration available (additional 10-100x for large datasets)\n",
    "   - Float64 precision enforced via safe_import_jax()\n",
    "\n",
    "4. **Bayesian Uncertainty Quantification:**\n",
    "   - Two-stage workflow: NLSQ (fast) → NUTS (warm-start)\n",
    "   - Warm-start reduces convergence time 2-5x\n",
    "   - Provides credible intervals and parameter correlations\n",
    "   - Essential for identifying non-identifiability issues\n",
    "\n",
    "5. **ArviZ Diagnostic Suite:**\n",
    "   - **6 essential plots** assess MCMC quality comprehensively\n",
    "   - Must check: R-hat < 1.01, ESS > 400, divergences < 1%\n",
    "   - Rank plot is most sensitive convergence diagnostic\n",
    "   - Pair plot reveals parameter correlations (Gm-eta often correlated)\n",
    "\n",
    "### When to Use Zener Model\n",
    "\n",
    "**Appropriate for:**\n",
    "- ✓ Crosslinked polymers with finite equilibrium modulus\n",
    "- ✓ Gels and soft solids (physical or chemical networks)\n",
    "- ✓ Materials with single dominant relaxation time\n",
    "- ✓ Small strain linear viscoelastic regime\n",
    "\n",
    "**Consider alternatives for:**\n",
    "- ✗ Complete stress relaxation (Ge=0) → Maxwell model\n",
    "- ✗ Multiple relaxation times → Generalized Maxwell\n",
    "- ✗ Power-law frequency dependence → Fractional Zener models\n",
    "- ✗ Solid-like materials with no flow → Kelvin-Voigt\n",
    "\n",
    "### Common Pitfalls\n",
    "\n",
    "1. **Parameter Correlation:**\n",
    "   - Gm and eta often correlated (both determine τ)\n",
    "   - Check pair plot for non-identifiability\n",
    "   - May need multi-technique fitting to improve constraint\n",
    "\n",
    "2. **Frequency Range:**\n",
    "   - Need data spanning relaxation time: 0.1τ < 1/ω < 10τ\n",
    "   - Insufficient range → poor Ge or Gm estimation\n",
    "   - Use mastercurve generation to extend range\n",
    "\n",
    "3. **Model Selection:**\n",
    "   - Check residuals for systematic trends\n",
    "   - Zener may be insufficient if multiple relaxation times present\n",
    "   - Use Bayesian model comparison (WAIC, LOO) to compare alternatives\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "### Explore Related Models\n",
    "- **[03-springpot-fitting.ipynb](03-springpot-fitting.ipynb)**: Fractional element for power-law behavior\n",
    "- **[01-maxwell-fitting.ipynb](01-maxwell-fitting.ipynb)**: Special case with Ge=0\n",
    "- **Advanced fractional models**: See `advanced/04-fractional-models-deep-dive.ipynb`\n",
    "\n",
    "### Deepen Bayesian Understanding\n",
    "- **[bayesian/01-bayesian-basics.ipynb](../bayesian/01-bayesian-basics.ipynb)**: Comprehensive NLSQ→NUTS workflow\n",
    "- **[bayesian/03-convergence-diagnostics.ipynb](../bayesian/03-convergence-diagnostics.ipynb)**: Deep dive into all 6 ArviZ plots\n",
    "- **[bayesian/04-model-comparison.ipynb](../bayesian/04-model-comparison.ipynb)**: Statistical model selection\n",
    "\n",
    "### Advanced Workflows\n",
    "- **[transforms/02-mastercurve-generation.ipynb](../transforms/02-mastercurve-generation.ipynb)**: Extend frequency range via TTS\n",
    "- **[advanced/01-multi-technique-fitting.ipynb](../advanced/01-multi-technique-fitting.ipynb)**: Constrained fitting across test modes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Session Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print session information for reproducibility\n",
    "import sys\n",
    "\n",
    "import rheojax\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Rheo version: {rheojax.__version__}\")\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "print(f\"JAX devices: {jax.devices()}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"ArviZ version: {az.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10q9cseas4zk",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Simulate the issue\n",
    "y_complex = np.array([1+2j, 3+4j, 5+6j])\n",
    "predictions_complex = np.array([1.1+2.1j, 2.9+3.9j, 5.1+6.1j])\n",
    "\n",
    "# Old method (would trigger warning)\n",
    "print(\"Testing old method (should warn):\")\n",
    "with warnings.catch_warnings(record=True) as w:\n",
    "    warnings.simplefilter(\"always\")\n",
    "    ss_res_old = np.sum((y_complex - predictions_complex) ** 2)\n",
    "    ss_tot_old = np.sum((y_complex - np.mean(y_complex)) ** 2)\n",
    "    r2_old = 1 - (ss_res_old / ss_tot_old)\n",
    "    try:\n",
    "        result_old = float(r2_old)\n",
    "        print(f\"  Result: {result_old}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")\n",
    "    \n",
    "    if w:\n",
    "        print(f\"  Warnings: {[str(warn.message) for warn in w]}\")\n",
    "\n",
    "# New method (should not warn)\n",
    "print(\"\\nTesting new method (should not warn):\")\n",
    "with warnings.catch_warnings(record=True) as w:\n",
    "    warnings.simplefilter(\"always\")\n",
    "    ss_res_new = np.sum(np.abs(y_complex - predictions_complex) ** 2)\n",
    "    ss_tot_new = np.sum(np.abs(y_complex - np.mean(y_complex)) ** 2)\n",
    "    r2_new = 1 - (ss_res_new / ss_tot_new)\n",
    "    result_new = float(np.real(r2_new))\n",
    "    print(f\"  Result: {result_new}\")\n",
    "    \n",
    "    if w:\n",
    "        print(f\"  Warnings: {[str(warn.message) for warn in w]}\")\n",
    "    else:\n",
    "        print(\"  No warnings!\")\n",
    "\n",
    "print(f\"\\nR² value is real: {np.isreal(r2_new)}\")\n",
    "print(f\"R² value: {r2_new}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rheojax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
