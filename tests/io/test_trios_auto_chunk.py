"""Unit tests for TRIOS auto-chunking detection and behavior.

This test module validates the auto-detection logic that determines when to
automatically use chunked loading for large TRIOS files.

Key Tests:
- File size detection triggers chunked loading at 5 MB threshold
- auto_chunk=False parameter disables auto-detection
- Logging messages inform users when auto-chunking is activated
- Progress callback integration works correctly
"""

from __future__ import annotations

import logging
import os
import tempfile
from pathlib import Path

import numpy as np
import pytest

from rheojax.io.readers.trios import load_trios


# Test fixture: Create synthetic TRIOS files of various sizes
@pytest.fixture
def trios_file_factory():
    """Factory to create synthetic TRIOS files of specified sizes."""

    def _create_file(target_size_mb: float, data_points: int = None) -> Path:
        """Create a synthetic TRIOS file with target size in MB.

        Args:
            target_size_mb: Target file size in MB
            data_points: Number of data points (auto-calculated if None)

        Returns:
            Path to temporary TRIOS file
        """
        # Estimate points needed: ~80 bytes per point + ~2 KB header
        if data_points is None:
            header_size = 2048  # bytes
            bytes_per_point = 80
            target_bytes = target_size_mb * 1024 * 1024
            data_points = max(100, int((target_bytes - header_size) / bytes_per_point))

        # Create temporary file
        temp_file = tempfile.NamedTemporaryFile(
            mode="w", suffix=".txt", delete=False, encoding="utf-8"
        )

        try:
            # Write TRIOS header
            temp_file.write("Filename\ttest_file.txt\n")
            temp_file.write("Instrument serial number\t12345\n")
            temp_file.write("Instrument name\tARES-G2\n")
            temp_file.write("operator\tTest User\n")
            temp_file.write("rundate\t2024-01-01 12:00:00\n")
            temp_file.write("Sample name\tTest Sample\n")
            temp_file.write("Geometry name\t25mm Parallel Plate\n")
            temp_file.write("Geometry type\tParallel Plate\n")
            temp_file.write("\n")

            # Write segment header
            temp_file.write("[step]\n")
            temp_file.write("Step name\tFrequency sweep (150.0 Â°C)\n")
            temp_file.write(f"Number of points\t{data_points}\n")

            # Write column headers
            temp_file.write(
                "Variables\tAngular frequency\tStorage modulus\tLoss modulus\tOscillation strain\n"
            )
            temp_file.write("Units\trad/s\tPa\tPa\t%\n")

            # Generate realistic frequency sweep data
            frequencies = np.logspace(-2, 2, data_points)  # 0.01 to 100 rad/s

            for i, omega in enumerate(frequencies):
                # Simple Maxwell model response for realistic data
                tau = 1.0  # relaxation time
                G_infinity = 1e3  # Pa
                G_0 = 1e6  # Pa

                G_prime = G_infinity + (G_0 - G_infinity) * (omega * tau) ** 2 / (
                    1 + (omega * tau) ** 2
                )
                G_double_prime = (
                    (G_0 - G_infinity) * (omega * tau) / (1 + (omega * tau) ** 2)
                )

                strain = 1.0  # 1% strain

                temp_file.write(
                    f"Data point {i+1}\t{omega:.6e}\t{G_prime:.6e}\t{G_double_prime:.6e}\t{strain:.6e}\n"
                )

            temp_file.close()

            # Verify file size
            actual_size_mb = os.path.getsize(temp_file.name) / (1024 * 1024)
            print(
                f"Created TRIOS file: {data_points} points, {actual_size_mb:.2f} MB (target: {target_size_mb:.2f} MB)"
            )

            return Path(temp_file.name)

        except Exception:
            temp_file.close()
            os.unlink(temp_file.name)
            raise

    yield _create_file

    # Cleanup is handled by test teardown


@pytest.fixture(autouse=True)
def cleanup_temp_files(request):
    """Cleanup temporary files after each test."""
    temp_files = []

    def track_file(filepath: Path):
        temp_files.append(filepath)

    request.node._temp_files = temp_files

    yield track_file

    # Cleanup
    for filepath in temp_files:
        if filepath.exists():
            try:
                os.unlink(filepath)
            except Exception:
                pass


# Test 1: Small file (< 5 MB) should NOT trigger auto-chunking
@pytest.mark.unit
@pytest.mark.smoke
def test_small_file_no_auto_chunk(trios_file_factory, cleanup_temp_files):
    """Test that files < 5 MB do NOT trigger auto-chunking."""
    # Create 1 MB file (well below threshold)
    filepath = trios_file_factory(target_size_mb=1.0)
    cleanup_temp_files(filepath)

    # Load file - should use full loading
    data = load_trios(filepath)

    # Verify data loaded successfully
    assert data is not None
    assert len(data.x) > 0
    assert len(data.y) > 0
    assert data.x.shape == data.y.shape


# Test 2: Large file (> 5 MB) should trigger auto-chunking
@pytest.mark.unit
def test_large_file_auto_chunk(trios_file_factory, cleanup_temp_files, caplog):
    """Test that files > 5 MB automatically trigger chunked loading."""
    # Create 10 MB file (well above threshold)
    filepath = trios_file_factory(target_size_mb=10.0)
    cleanup_temp_files(filepath)

    # Enable logging to capture auto-chunk message
    with caplog.at_level(logging.INFO):
        data = load_trios(filepath)

    # Verify data loaded successfully
    assert data is not None
    assert len(data.x) > 0
    assert len(data.y) > 0

    # Check that auto-chunking was logged
    assert any("auto-chunk" in record.message.lower() for record in caplog.records)


# Test 3: File exactly at threshold (5 MB) should trigger auto-chunking
@pytest.mark.unit
def test_threshold_file_auto_chunk(trios_file_factory, cleanup_temp_files):
    """Test that files at exactly 5 MB threshold trigger chunked loading."""
    # Create file as close to 5 MB as possible
    filepath = trios_file_factory(target_size_mb=5.0)
    cleanup_temp_files(filepath)

    file_size_mb = os.path.getsize(filepath) / (1024 * 1024)

    # Load file
    data = load_trios(filepath)

    # Verify data loaded successfully
    assert data is not None
    assert len(data.x) > 0

    # If file is >= 5 MB, chunking should be used
    # (exact threshold behavior depends on implementation)
    print(f"File size: {file_size_mb:.2f} MB")


# Test 4: auto_chunk=False disables auto-detection
@pytest.mark.unit
def test_auto_chunk_false_disables_auto_detection(
    trios_file_factory, cleanup_temp_files, caplog
):
    """Test that auto_chunk=False disables auto-detection for large files."""
    # Create 10 MB file
    filepath = trios_file_factory(target_size_mb=10.0)
    cleanup_temp_files(filepath)

    # Load with auto_chunk=False
    with caplog.at_level(logging.INFO):
        data = load_trios(filepath, auto_chunk=False)

    # Verify data loaded successfully
    assert data is not None
    assert len(data.x) > 0

    # Should NOT have auto-chunk logging (disabled)
    # Note: This checks that auto-detection was bypassed
    # (In practice, user explicitly disabled it, so no automatic decision made)


# Test 5: Progress callback integration (when auto-chunking is enabled)
@pytest.mark.unit
def test_progress_callback_with_auto_chunk(trios_file_factory, cleanup_temp_files):
    """Test that progress callback works when auto-chunking is triggered."""
    # Create 10 MB file
    filepath = trios_file_factory(target_size_mb=10.0)
    cleanup_temp_files(filepath)

    # Track progress updates
    progress_updates = []

    def progress_callback(current_points: int, total_points: int):
        """Track progress updates."""
        progress_updates.append((current_points, total_points))

    # Load with progress callback
    data = load_trios(filepath, progress_callback=progress_callback)

    # Verify data loaded
    assert data is not None
    assert len(data.x) > 0

    # Progress callback should have been called if chunking was used
    # (callback only applies to chunked loading)
    # Note: Callback behavior depends on implementation - may be called or not
    # depending on whether auto-chunking delegates to load_trios_chunked
    print(f"Progress updates: {len(progress_updates)}")


# Test 6: Logging message format validation
@pytest.mark.unit
def test_auto_chunk_logging_message_format(
    trios_file_factory, cleanup_temp_files, caplog
):
    """Test that auto-chunk logging message has correct format."""
    # Create 10 MB file
    filepath = trios_file_factory(target_size_mb=10.0)
    cleanup_temp_files(filepath)

    file_size_mb = os.path.getsize(filepath) / (1024 * 1024)

    with caplog.at_level(logging.INFO):
        data = load_trios(filepath)

    assert data is not None

    # Check logging message contains file size
    auto_chunk_logs = [
        record.message
        for record in caplog.records
        if "auto-chunk" in record.message.lower()
    ]

    if auto_chunk_logs:
        # Verify message format includes file size
        assert any(
            f"{file_size_mb:.1f}" in msg or "MB" in msg for msg in auto_chunk_logs
        )


# Test 7: Data integrity - chunked vs full loading produces identical results
@pytest.mark.unit
def test_auto_chunk_data_integrity(trios_file_factory, cleanup_temp_files):
    """Test that auto-chunked loading produces identical data to full loading."""
    # Create moderate-sized file (3 MB - below threshold)
    filepath = trios_file_factory(target_size_mb=3.0)
    cleanup_temp_files(filepath)

    # Load with auto-chunking enabled (but won't trigger for 3 MB)
    data_full = load_trios(filepath)

    # Force chunked loading
    data_chunked = load_trios(filepath, chunk_size=5000)

    # Verify identical results
    assert data_full is not None
    assert data_chunked is not None

    # Compare data arrays
    np.testing.assert_allclose(data_full.x, data_chunked.x, rtol=1e-10)
    np.testing.assert_allclose(data_full.y, data_chunked.y, rtol=1e-10)

    # Compare metadata
    assert data_full.x_units == data_chunked.x_units
    assert data_full.y_units == data_chunked.y_units
    assert data_full.domain == data_chunked.domain
